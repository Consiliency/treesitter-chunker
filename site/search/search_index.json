{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tree-sitter Chunker Documentation","text":"<p>Welcome to the Tree-sitter Chunker documentation! Tree-sitter Chunker is a powerful Python library for semantically chunking source code using Tree-sitter parsers. It intelligently splits code into meaningful units like functions, classes, and methods, making it perfect for code analysis, embeddings, and documentation generation.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started - Installation and your first chunking project</li> <li>API Reference - Complete API documentation with all 107+ exported APIs</li> <li>User Guide - Comprehensive usage guide with plugins and performance</li> <li>Plugin Development - Create custom language plugins</li> <li>Configuration - Configuration files and options</li> <li>Performance Guide - Optimization strategies and benchmarking</li> <li>Export Formats - JSON, JSONL, and Parquet export options</li> <li>Cookbook - Practical recipes and examples</li> <li>Architecture - System design and internals</li> </ul>"},{"location":"#text-processing","title":"Text Processing","text":"<ul> <li>Intelligent Fallback - Automatic chunking method selection</li> <li>Token Limits - LLM-aware token limit handling</li> <li>Markdown Processor - Markdown document chunking</li> <li>Config Processor - Configuration file chunking</li> <li>Log Processor - Log file analysis and chunking</li> </ul>"},{"location":"#graph-database-export","title":"Graph &amp; Database Export","text":"<ul> <li>GraphML Export - Detailed GraphML export walkthrough</li> <li>Export Formats - See Export Formats for JSON/JSONL/Parquet and Neo4j integration</li> </ul>"},{"location":"#language-support","title":"Language Support","text":"<ul> <li>Grammar Discovery - Automatic grammar discovery from GitHub</li> <li>Zero-Config API - Simple API that requires no setup</li> </ul>"},{"location":"#what-is-tree-sitter-chunker","title":"What is Tree-sitter Chunker?","text":"<p>Tree-sitter Chunker leverages the power of Tree-sitter parsers to understand code structure and extract semantic chunks. Unlike simple line-based splitting, it:</p> <ul> <li>\u2728 Understands Code Structure - Extracts functions, classes, methods based on AST</li> <li>\ud83d\ude80 High Performance - Efficient parser caching and pooling</li> <li>\ud83d\udd27 Language Agnostic - Supports Python, JavaScript, Rust, C, C++</li> <li>\ud83e\udde9 Extensible - Easy to add new languages and chunk types</li> <li>\ud83d\udd12 Thread Safe - Designed for concurrent processing</li> <li>\ud83d\udce6 Zero Config - Works out of the box with sensible defaults</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from chunker.chunker import chunk_file\n\n# Chunk a Python file\nchunks = chunk_file(\"example.py\", \"python\")\n\n# Process results\nfor chunk in chunks:\n    print(f\"{chunk.node_type} at lines {chunk.start_line}-{chunk.end_line}\")\n    print(f\"  {chunk.content.split(chr(10))[0]}...\")\n</code></pre>"},{"location":"#features","title":"Features","text":""},{"location":"#semantic-chunking","title":"\ud83c\udfaf Semantic Chunking","text":"<p>Extract meaningful code units: - Functions and methods - Classes and structures - Nested contexts preserved - Accurate line and byte positions - Support for 5 languages with plugin architecture</p>"},{"location":"#performance-optimized","title":"\ud83c\udfce\ufe0f Performance Optimized","text":"<p>Built for speed and efficiency: - AST Caching: 11.9x speedup for repeated files - Parallel Processing: Process directories with multiple workers - Streaming Support: Handle files larger than memory - LRU Parser Caching: Efficient parser reuse - Thread-Safe Operations: Safe concurrent processing</p>"},{"location":"#developer-friendly","title":"\ud83d\udee0\ufe0f Developer Friendly","text":"<p>Simple API with powerful capabilities: - Plugin System: Easy language extensibility - Multiple Export Formats: JSON, JSONL, Parquet - Rich Configuration: TOML/YAML/JSON config files - Comprehensive CLI: Batch processing, filtering, progress tracking - Detailed Documentation: API reference, guides, and examples</p>"},{"location":"#multi-language-support","title":"\ud83c\udf10 Multi-Language Support","text":"<p>Built-in support for common languages (Python, JavaScript/TypeScript, Rust, C, C++), with dynamic discovery for many more via Tree-sitter grammar registry.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code># Using uv (recommended)\nuv pip install -e \".[dev]\"\n\n# Install py-tree-sitter with ABI 15 support\nuv pip install git+https://github.com/tree-sitter/py-tree-sitter.git\n\n# Build language grammars\npython scripts/fetch_grammars.py\npython scripts/build_lib.py\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#code-embeddings","title":"\ud83d\udcca Code Embeddings","text":"<p>Generate embeddings for semantic code search:</p> <pre><code>chunks = chunk_file(\"module.py\", \"python\")\nembeddings = [generate_embedding(chunk.content) for chunk in chunks]\n</code></pre>"},{"location":"#documentation-generation","title":"\ud83d\udcdd Documentation Generation","text":"<p>Extract functions with docstrings:</p> <pre><code>for chunk in chunks:\n    if chunk.node_type == \"function_definition\":\n        # Extract and process docstring\n        generate_docs(chunk)\n</code></pre>"},{"location":"#code-analysis","title":"\ud83d\udd0d Code Analysis","text":"<p>Analyze code structure and complexity:</p> <pre><code>functions = [c for c in chunks if \"function\" in c.node_type]\nlarge_functions = [f for f in functions if f.end_line - f.start_line &gt; 50]\n</code></pre>"},{"location":"#aiml-applications","title":"\ud83e\udd16 AI/ML Applications","text":"<p>Prepare code for language models:</p> <pre><code># Create training data\nfor chunk in chunks:\n    context = f\"{chunk.node_type} in {chunk.parent_context or 'module'}\"\n    training_data.append({\"code\": chunk.content, \"context\": context})\n</code></pre>"},{"location":"#documentation-overview","title":"Documentation Overview","text":""},{"location":"#for-new-users","title":"For New Users","text":"<ol> <li>Start with Getting Started to install and run your first example</li> <li>Follow the tutorial to build practical tools</li> <li>Check the Quick Reference Card</li> </ol>"},{"location":"#for-developers","title":"For Developers","text":"<ol> <li>Read the User Guide for comprehensive coverage</li> <li>Explore the API Reference for detailed function documentation</li> <li>Check the Cookbook for ready-to-use solutions</li> </ol>"},{"location":"#for-contributors","title":"For Contributors","text":"<ol> <li>Study the Architecture document</li> <li>Understand the plugin system and extension points</li> <li>Review the troubleshooting guide</li> </ol>"},{"location":"#common-tasks","title":"Common Tasks","text":""},{"location":"#list-available-languages","title":"List Available Languages","text":"<pre><code>from chunker.parser import list_languages\nprint(list_languages())\n# Output: ['c', 'cpp', 'javascript', 'python', 'rust']\n</code></pre>"},{"location":"#get-language-information","title":"Get Language Information","text":"<pre><code>from chunker.parser import get_language_info\ninfo = get_language_info(\"python\")\nprint(f\"ABI Version: {info.version}\")\nprint(f\"Node types: {info.node_types_count}\")\n</code></pre>"},{"location":"#handle-errors-gracefully","title":"Handle Errors Gracefully","text":"<pre><code>from chunker.exceptions import LanguageNotFoundError, LibraryNotFoundError\n\ntry:\n    chunks = chunk_file(\"file.xyz\", \"xyz\")\nexcept LanguageNotFoundError as e:\n    print(f\"Language not supported: {e}\")\nexcept LibraryNotFoundError:\n    print(\"Run: python scripts/build_lib.py\")\n</code></pre>"},{"location":"#process-multiple-files","title":"Process Multiple Files","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\n\ndef process_files(file_list, language):\n    with ThreadPoolExecutor(max_workers=8) as executor:\n        results = executor.map(lambda f: chunk_file(f, language), file_list)\n        return list(results)\n</code></pre>"},{"location":"#capabilities-overview","title":"Capabilities Overview","text":""},{"location":"#plugin-architecture","title":"\ud83d\udd0c Plugin Architecture","text":"<ul> <li>Dynamic plugin discovery and loading</li> <li>Abstract base class for custom languages</li> <li>Configuration management per plugin</li> </ul>"},{"location":"#performance-enhancements","title":"\u26a1 Performance Enhancements","text":"<ul> <li>AST Caching (repeated files)</li> <li>Parallel Processing: <code>chunk_files_parallel()</code> / <code>chunk_directory_parallel()</code></li> <li>Streaming API: <code>chunk_file_streaming()</code> for huge files</li> </ul>"},{"location":"#export-formats","title":"\ud83d\udce4 Export Formats","text":"<ul> <li>JSON / JSONL / Parquet with compression &amp; partitioning (see Export Formats)</li> </ul>"},{"location":"#configuration","title":"\ud83c\udf9b\ufe0f Configuration","text":"<ul> <li><code>.chunkerrc</code> (TOML/YAML/JSON), per-language chunk types and rules, env vars</li> </ul>"},{"location":"#cli","title":"\ud83d\udda5\ufe0f CLI","text":"<ul> <li>Batch processing, filters, progress, JSON/JSONL output, zero-config modes</li> </ul>"},{"location":"#performance-tips","title":"Performance Tips","text":"<ol> <li>Enable Caching: Use ASTCache for 11.9x speedup on repeated files</li> <li>Parallel Processing: Use <code>chunk_files_parallel()</code> for multiple files</li> <li>Stream Large Files: Use <code>chunk_file_streaming()</code> for files &gt;10MB</li> <li>Optimize Workers: Set <code>max_workers</code> based on CPU count</li> <li>Choose Right Export: Parquet for analytics, JSONL for streaming</li> </ol>"},{"location":"#community","title":"Community","text":""},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Check the Troubleshooting Guide</li> <li>Review Common Issues</li> <li>Enable debug logging for detailed information</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Areas of interest:</p> <ul> <li>Adding new language support</li> <li>Performance optimizations</li> <li>Documentation improvements</li> <li>Bug fixes and tests</li> </ul>"},{"location":"#roadmap-high-level","title":"Roadmap (High Level)","text":"<ul> <li>Dev tooling &amp; packaging improvements</li> <li>CI/CD and docs automation</li> <li>Additional language examples and exporters</li> </ul>"},{"location":"#license","title":"License","text":"<p>Tree-sitter Chunker is open source software. See the LICENSE file for details.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>Built on top of the excellent Tree-sitter parsing library.</p> <p>Happy chunking! \ud83d\ude80</p>"},{"location":"api-reference/","title":"Tree-sitter Chunker API Reference","text":""},{"location":"api-reference/#overview","title":"Overview","text":"<p>Tree-sitter Chunker provides a comprehensive API for semantically chunking source code files using Tree-sitter parsers. The library features dynamic language discovery, efficient parser caching, plugin architecture, parallel processing, and multiple export formats.</p>"},{"location":"api-reference/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Quick Start</li> <li>Core APIs</li> <li>chunk_file</li> <li>CodeChunk</li> <li>Parser Management</li> <li>get_parser</li> <li>list_languages</li> <li>get_language_info</li> <li>return_parser</li> <li>clear_cache</li> <li>ParserConfig</li> <li>Plugin System</li> <li>PluginManager</li> <li>LanguagePlugin</li> <li>PluginConfig</li> <li>get_plugin_manager</li> <li>Configuration</li> <li>ChunkerConfig</li> <li>Performance Features</li> <li>ASTCache</li> <li>chunk_files_parallel</li> <li>chunk_directory_parallel</li> <li>chunk_file_streaming</li> <li>StreamingChunker</li> <li>ParallelChunker</li> <li>Export Formats</li> <li>JSON Export</li> <li>JSONL Export</li> <li>Parquet Export</li> <li>Exception Handling</li> <li>Thread Safety</li> <li>Performance Optimization</li> </ul>"},{"location":"api-reference/#installation","title":"Installation","text":"<pre><code># Install with uv (recommended)\nuv pip install -e \".[dev]\"\n\n# Required dependencies\nuv pip install git+https://github.com/tree-sitter/py-tree-sitter.git\nuv pip install pyarrow&gt;=11.0.0\n\n# Build language grammars\npython scripts/fetch_grammars.py\npython scripts/build_lib.py\n</code></pre>"},{"location":"api-reference/#quick-start","title":"Quick Start","text":"<pre><code>from chunker.core import chunk_file\nfrom chunker.plugin_manager import get_plugin_manager\n\n# Basic usage\nchunks = chunk_file(\"example.py\", \"python\")\n\n# With plugins\nmanager = get_plugin_manager()\nmanager.load_built_in_plugins()\nchunks = chunk_file(\"example.py\", \"python\")\n\n# Parallel processing\nfrom chunker.parallel import chunk_files_parallel\nresults = chunk_files_parallel([\"file1.py\", \"file2.py\", \"file3.py\"], \"python\")\n\n# Export to Parquet\nfrom chunker.exporters import ParquetExporter\nexporter = ParquetExporter()\nexporter.export(chunks, \"output.parquet\")\n</code></pre>"},{"location":"api-reference/#core-apis","title":"Core APIs","text":""},{"location":"api-reference/#chunk_file","title":"chunk_file","text":"<pre><code>chunk_file(path: str | Path, language: str) -&gt; list[CodeChunk]\n</code></pre> <p>Parse a file and extract semantic code chunks. This is the main function for extracting meaningful code blocks from source files.</p> <p>Parameters: - <code>path</code> (str | Path): Path to the file to chunk - <code>language</code> (str): Programming language of the file</p> <p>Returns: - <code>list[CodeChunk]</code>: List of extracted code chunks</p> <p>Example:</p> <pre><code>from chunker.core import chunk_file\n\nchunks = chunk_file(\"src/main.py\", \"python\")\nfor chunk in chunks:\n    print(f\"{chunk.node_type} at lines {chunk.start_line}-{chunk.end_line}\")\n</code></pre>"},{"location":"api-reference/#codechunk","title":"CodeChunk","text":"<pre><code>@dataclass\nclass CodeChunk:\n    language: str\n    file_path: str\n    node_type: str\n    start_line: int\n    end_line: int\n    byte_start: int\n    byte_end: int\n    parent_context: str\n    content: str\n    chunk_id: str = \"\"\n    parent_chunk_id: str | None = None\n    references: list[str] = field(default_factory=list)\n    dependencies: list[str] = field(default_factory=list)\n</code></pre> <p>Represents a semantic chunk of code extracted from a file.</p> <p>Attributes: - <code>language</code> (str): Programming language - <code>file_path</code> (str): Path to the source file - <code>node_type</code> (str): Type of syntax node (e.g., \"function_definition\", \"class_definition\") - <code>start_line</code> (int): Starting line number (1-indexed) - <code>end_line</code> (int): Ending line number (1-indexed) - <code>byte_start</code> (int): Starting byte offset in the file - <code>byte_end</code> (int): Ending byte offset in the file - <code>parent_context</code> (str): Parent node context (e.g., \"class:MyClass\" for methods) - <code>content</code> (str): The actual code content - <code>chunk_id</code> (str): Unique identifier for the chunk (auto-generated if not provided) - <code>parent_chunk_id</code> (str | None): ID of the parent chunk if nested - <code>references</code> (list[str]): List of references to other chunks - <code>dependencies</code> (list[str]): List of dependencies on other chunks</p> <p>Methods: - <code>generate_id() -&gt; str</code>: Generate a unique ID based on content and location</p>"},{"location":"api-reference/#parser-management","title":"Parser Management","text":""},{"location":"api-reference/#get_parser","title":"get_parser","text":"<pre><code>get_parser(language: str, config: Optional[ParserConfig] = None) -&gt; Parser\n</code></pre> <p>Get a parser instance for the specified language with optional configuration. Parsers are cached and pooled for efficiency.</p> <p>Parameters: - <code>language</code> (str): The name of the language (e.g., \"python\", \"javascript\", \"rust\") - <code>config</code> (Optional[ParserConfig]): Optional parser configuration</p> <p>Returns: - <code>Parser</code>: A configured tree-sitter parser instance</p> <p>Raises: - <code>LanguageNotFoundError</code>: If the language is not available - <code>ParserError</code>: If parser initialization fails</p>"},{"location":"api-reference/#list_languages","title":"list_languages","text":"<pre><code>list_languages() -&gt; List[str]\n</code></pre> <p>List all available languages discovered from the compiled shared library.</p> <p>Returns: - <code>List[str]</code>: Sorted list of available language names</p> <p>Example:</p> <pre><code>languages = list_languages()\nprint(languages)  # ['c', 'cpp', 'javascript', 'python', 'rust']\n</code></pre>"},{"location":"api-reference/#get_language_info","title":"get_language_info","text":"<pre><code>get_language_info(language: str) -&gt; LanguageMetadata\n</code></pre> <p>Get detailed metadata about a specific language including version, capabilities, and node types.</p> <p>Parameters: - <code>language</code> (str): The name of the language</p> <p>Returns: - <code>LanguageMetadata</code>: Language metadata object with detailed information</p>"},{"location":"api-reference/#return_parser","title":"return_parser","text":"<pre><code>return_parser(language: str, parser: Parser) -&gt; None\n</code></pre> <p>Return a parser to the pool for reuse. This is a performance optimization - parsers will be automatically cleaned up if not returned, but returning them enables better reuse.</p>"},{"location":"api-reference/#clear_cache","title":"clear_cache","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear the parser cache. This forces all parsers to be recreated on next request. Useful for freeing memory or ensuring fresh parser instances.</p>"},{"location":"api-reference/#parserconfig","title":"ParserConfig","text":"<pre><code>@dataclass\nclass ParserConfig:\n    timeout_ms: Optional[int] = None\n    included_ranges: Optional[List[Tuple[int, int]]] = None\n    logger: Optional[Any] = None\n</code></pre> <p>Configuration options for parser instances.</p> <p>Attributes: - <code>timeout_ms</code>: Parser timeout in milliseconds (prevents infinite loops) - <code>included_ranges</code>: List of (start_byte, end_byte) ranges to parse (for partial parsing) - <code>logger</code>: Optional logger for parser debug events</p>"},{"location":"api-reference/#plugin-system","title":"Plugin System","text":""},{"location":"api-reference/#pluginmanager","title":"PluginManager","text":"<pre><code>class PluginManager:\n    def __init__(self, config: Optional[ChunkerConfig] = None)\n    def register_plugin(self, plugin_class: Type[LanguagePlugin]) -&gt; None\n    def load_plugin_directory(self, directory: Path) -&gt; List[Type[LanguagePlugin]]\n    def load_built_in_plugins() -&gt; None\n    def get_plugin(self, language: str) -&gt; Optional[LanguagePlugin]\n    def list_plugins() -&gt; List[str]\n</code></pre> <p>Manages plugin discovery, loading, and lifecycle.</p> <p>Key Methods: - <code>register_plugin</code>: Register a plugin class - <code>load_plugin_directory</code>: Load plugins from a directory - <code>load_built_in_plugins</code>: Load all built-in language plugins - <code>get_plugin</code>: Get a plugin instance for a language - <code>list_plugins</code>: List all registered plugin languages</p>"},{"location":"api-reference/#languageplugin","title":"LanguagePlugin","text":"<pre><code>class LanguagePlugin(ABC):\n    @property\n    @abstractmethod\n    def language_name(self) -&gt; str\n\n    @property\n    @abstractmethod\n    def supported_extensions(self) -&gt; Set[str]\n\n    @property\n    @abstractmethod\n    def default_chunk_types(self) -&gt; Set[str]\n\n    @abstractmethod\n    def get_node_name(self, node: Node, source: bytes) -&gt; Optional[str]\n</code></pre> <p>Abstract base class for language plugins. All language plugins must inherit from this class.</p> <p>Built-in Plugins: - <code>PythonPlugin</code>: Python language support - <code>RustPlugin</code>: Rust language support - <code>JavaScriptPlugin</code>: JavaScript/TypeScript support - <code>CPlugin</code>: C language support - <code>CppPlugin</code>: C++ language support</p>"},{"location":"api-reference/#pluginconfig","title":"PluginConfig","text":"<pre><code>@dataclass\nclass PluginConfig:\n    enabled: bool = True\n    chunk_types: Optional[Set[str]] = None\n    min_chunk_size: int = 0\n    max_chunk_size: int = 1000000\n    custom_options: Dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>Configuration for individual plugins.</p> <p>Attributes: - <code>enabled</code>: Whether the plugin is enabled - <code>chunk_types</code>: Override default chunk types - <code>min_chunk_size</code>: Minimum chunk size in lines - <code>max_chunk_size</code>: Maximum chunk size in lines - <code>custom_options</code>: Plugin-specific options</p>"},{"location":"api-reference/#get_plugin_manager","title":"get_plugin_manager","text":"<pre><code>get_plugin_manager() -&gt; PluginManager\n</code></pre> <p>Get the global plugin manager instance (singleton).</p> <p>Example:</p> <pre><code>from chunker.plugin_manager import get_plugin_manager\n\nmanager = get_plugin_manager()\nmanager.load_built_in_plugins()\n\n# List available plugins\nplugins = manager.list_plugins()\nprint(plugins)  # ['python', 'rust', 'javascript', 'c', 'cpp']\n</code></pre>"},{"location":"api-reference/#configuration","title":"Configuration","text":""},{"location":"api-reference/#chunkerconfig","title":"ChunkerConfig","text":"<pre><code>class ChunkerConfig:\n    def __init__(self, config_path: Optional[Path] = None)\n    def load(self, config_path: Path) -&gt; None\n    def save(self, config_path: Optional[Path] = None) -&gt; None\n    def set_plugin_config(self, language: str, config: PluginConfig) -&gt; None\n    def get_plugin_config(self, language: str) -&gt; PluginConfig\n\n    @classmethod\n    def find_config(cls, start_path: Path = Path.cwd()) -&gt; Optional[Path]\n</code></pre> <p>Configuration manager supporting TOML, YAML, and JSON formats.</p> <p>Supported Formats: - <code>.toml</code> - TOML configuration - <code>.yaml</code> / <code>.yml</code> - YAML configuration - <code>.json</code> - JSON configuration</p> <p>Example Configuration (TOML):</p> <pre><code># chunker.config.toml\nplugin_dirs = [\"./plugins\", \"~/.chunker/plugins\"]\nenabled_languages = [\"python\", \"rust\", \"javascript\"]\n\n[plugins.python]\nenabled = true\nchunk_types = [\"function_definition\", \"class_definition\", \"async_function_definition\"]\nmin_chunk_size = 3\nmax_chunk_size = 500\n\n[plugins.python.custom_options]\ninclude_docstrings = true\ninclude_type_hints = true\n</code></pre>"},{"location":"api-reference/#performance-features","title":"Performance Features","text":""},{"location":"api-reference/#astcache","title":"ASTCache","text":"<pre><code>class ASTCache:\n    def __init__(self, max_size: int = 100)\n    def get(self, file_path: Path, language: str) -&gt; Optional[ParsedAST]\n    def put(self, file_path: Path, language: str, ast: ParsedAST) -&gt; None\n    def clear() -&gt; None\n    def get_stats() -&gt; Dict[str, Any]\n</code></pre> <p>LRU cache for parsed ASTs providing up to 11.9x speedup for repeated file processing.</p> <p>Key Methods: - <code>get</code>: Retrieve cached AST if available - <code>put</code>: Store AST in cache - <code>clear</code>: Clear all cached entries - <code>get_stats</code>: Get cache performance statistics</p> <p>Example:</p> <pre><code>from chunker.cache import ASTCache\n\ncache = ASTCache(max_size=200)\n# Cache is used automatically by chunk_file when available\nchunks = chunk_file(\"large_file.py\", \"python\")  # First run: parses\nchunks = chunk_file(\"large_file.py\", \"python\")  # Second run: uses cache (11.9x faster)\n\n# Check cache stats\nstats = cache.get_stats()\nprint(f\"Cache hits: {stats['hits']}, misses: {stats['misses']}\")\nprint(f\"Hit rate: {stats['hit_rate']:.2%}\")\n</code></pre>"},{"location":"api-reference/#chunk_files_parallel","title":"chunk_files_parallel","text":"<pre><code>chunk_files_parallel(\n    file_paths: List[Union[str, Path]], \n    language: str,\n    max_workers: Optional[int] = None,\n    show_progress: bool = True\n) -&gt; Dict[str, List[CodeChunk]]\n</code></pre> <p>Process multiple files in parallel using thread pool.</p> <p>Parameters: - <code>file_paths</code>: List of file paths to process - <code>language</code>: Programming language - <code>max_workers</code>: Maximum number of worker threads (defaults to CPU count) - <code>show_progress</code>: Whether to show progress bar</p> <p>Returns: - <code>Dict[str, List[CodeChunk]]</code>: Map of file path to chunks</p> <p>Example:</p> <pre><code>from chunker.parallel import chunk_files_parallel\n\nfiles = [\"src/main.py\", \"src/utils.py\", \"src/models.py\"]\nresults = chunk_files_parallel(files, \"python\", max_workers=4)\n\nfor file_path, chunks in results.items():\n    print(f\"{file_path}: {len(chunks)} chunks\")\n</code></pre>"},{"location":"api-reference/#chunk_directory_parallel","title":"chunk_directory_parallel","text":"<pre><code>chunk_directory_parallel(\n    directory: Union[str, Path],\n    language: str,\n    pattern: str = \"**/*\",\n    max_workers: Optional[int] = None,\n    show_progress: bool = True\n) -&gt; Dict[str, List[CodeChunk]]\n</code></pre> <p>Process all matching files in a directory in parallel.</p> <p>Parameters: - <code>directory</code>: Directory to process - <code>language</code>: Programming language - <code>pattern</code>: Glob pattern for file matching - <code>max_workers</code>: Maximum number of worker threads - <code>show_progress</code>: Whether to show progress bar</p>"},{"location":"api-reference/#chunk_file_streaming","title":"chunk_file_streaming","text":"<pre><code>chunk_file_streaming(\n    path: Union[str, Path],\n    language: str,\n    chunk_size: int = 1048576  # 1MB\n) -&gt; Iterator[CodeChunk]\n</code></pre> <p>Stream chunks from a file without loading the entire file into memory. Ideal for very large files.</p> <p>Parameters: - <code>path</code>: Path to the file - <code>language</code>: Programming language - <code>chunk_size</code>: Size of each read chunk in bytes</p> <p>Returns: - <code>Iterator[CodeChunk]</code>: Iterator yielding chunks as they are found</p> <p>Example:</p> <pre><code>from chunker.streaming import chunk_file_streaming\n\n# Process a very large file\nfor chunk in chunk_file_streaming(\"huge_codebase.py\", \"python\"):\n    # Process each chunk as it's found\n    process_chunk(chunk)\n</code></pre>"},{"location":"api-reference/#streamingchunker","title":"StreamingChunker","text":"<pre><code>class StreamingChunker:\n    def __init__(self, language: str, chunk_size: int = 1048576)\n    def process_stream(self, stream: IO[bytes]) -&gt; Iterator[CodeChunk]\n</code></pre> <p>Low-level streaming chunker for custom stream processing.</p>"},{"location":"api-reference/#parallelchunker","title":"ParallelChunker","text":"<pre><code>class ParallelChunker:\n    def __init__(self, language: str, max_workers: Optional[int] = None)\n    def process_files(self, file_paths: List[Path]) -&gt; Dict[str, List[CodeChunk]]\n    def process_directory(self, directory: Path, pattern: str = \"**/*\") -&gt; Dict[str, List[CodeChunk]]\n</code></pre> <p>Low-level parallel processing API for advanced use cases.</p>"},{"location":"api-reference/#export-formats","title":"Export Formats","text":""},{"location":"api-reference/#json-export","title":"JSON Export","text":"<pre><code>from chunker.export import JSONExporter, SchemaType\n\nexporter = JSONExporter(schema_type=SchemaType.FLAT)\nexporter.export(chunks, \"output.json\", compress=True, indent=2)\n\n# Available schema types:\n# - SchemaType.FLAT: Simple flat structure\n# - SchemaType.NESTED: Nested hierarchy preserving relationships\n# - SchemaType.RELATIONAL: Normalized relational structure\n</code></pre> <p>JSONExporter Methods:</p> <pre><code>class JSONExporter:\n    def __init__(self, schema_type: SchemaType = SchemaType.FLAT)\n    def export(self, chunks: list[CodeChunk], output: Union[str, Path, IO[str]], \n               compress: bool = False, indent: Optional[int] = 2) -&gt; None\n    def export_to_string(self, chunks: list[CodeChunk], indent: Optional[int] = 2) -&gt; str\n</code></pre>"},{"location":"api-reference/#jsonl-export","title":"JSONL Export","text":"<pre><code>from chunker.export import JSONLExporter\n\nexporter = JSONLExporter(schema_type=SchemaType.FLAT)\nexporter.export(chunks, \"output.jsonl\", compress=True)\n\n# Streaming export for large datasets\nexporter.export_streaming(chunk_iterator, \"large_output.jsonl\")\n</code></pre> <p>JSONLExporter Methods:</p> <pre><code>class JSONLExporter:\n    def __init__(self, schema_type: SchemaType = SchemaType.FLAT)\n    def export(self, chunks: list[CodeChunk], output: Union[str, Path, IO[str]], \n               compress: bool = False) -&gt; None\n    def export_streaming(self, chunks: Iterator[CodeChunk], \n                        output: Union[str, Path, IO[str]], compress: bool = False) -&gt; None\n</code></pre>"},{"location":"api-reference/#parquet-export","title":"Parquet Export","text":"<pre><code>from chunker.exporters import ParquetExporter\n\nexporter = ParquetExporter(\n    columns=[\"language\", \"file_path\", \"node_type\", \"content\"],\n    partition_by=[\"language\"],\n    compression=\"snappy\"\n)\nexporter.export(chunks, \"output.parquet\")\n\n# Export with custom schema\nexporter.export_partitioned(chunks, \"output_dir/\", partition_cols=[\"language\", \"node_type\"])\n</code></pre> <p>ParquetExporter Methods:</p> <pre><code>class ParquetExporter:\n    def __init__(self, columns: Optional[List[str]] = None,\n                 partition_by: Optional[List[str]] = None,\n                 compression: str = \"snappy\")\n    def export(self, chunks: List[CodeChunk], output_path: Union[str, Path]) -&gt; None\n    def export_partitioned(self, chunks: List[CodeChunk], output_dir: Union[str, Path],\n                          partition_cols: Optional[List[str]] = None) -&gt; None\n    def export_streaming(self, chunk_iterator: Iterator[CodeChunk],\n                        output_path: Union[str, Path], batch_size: int = 1000) -&gt; None\n</code></pre> <p>Compression Options: - <code>\"snappy\"</code> - Fast compression (default) - <code>\"gzip\"</code> - Higher compression ratio - <code>\"brotli\"</code> - Best compression ratio - <code>\"lz4\"</code> - Fastest compression - <code>\"zstd\"</code> - Good balance of speed and ratio - <code>None</code> - No compression</p>"},{"location":"api-reference/#exception-handling","title":"Exception Handling","text":"<p>The library provides a comprehensive exception hierarchy for precise error handling:</p>"},{"location":"api-reference/#base-exception","title":"Base Exception","text":"<pre><code>class ChunkerError(Exception):\n    \"\"\"Base exception for all chunker errors\"\"\"\n</code></pre>"},{"location":"api-reference/#language-errors","title":"Language Errors","text":"<pre><code>class LanguageError(ChunkerError):\n    \"\"\"Base class for language-related errors\"\"\"\n\nclass LanguageNotFoundError(LanguageError):\n    \"\"\"Raised when requested language is not available\"\"\"\n</code></pre>"},{"location":"api-reference/#parser-errors","title":"Parser Errors","text":"<pre><code>class ParserError(ChunkerError):\n    \"\"\"Base class for parser-related errors\"\"\"\n</code></pre>"},{"location":"api-reference/#library-errors","title":"Library Errors","text":"<pre><code>class LibraryError(ChunkerError):\n    \"\"\"Base class for shared library errors\"\"\"\n\nclass LibraryNotFoundError(LibraryError):\n    \"\"\"Raised when .so file is missing\"\"\"\n</code></pre>"},{"location":"api-reference/#error-handling-examples","title":"Error Handling Examples","text":"<pre><code>from chunker.core import chunk_file\nfrom chunker.parser import list_languages\nfrom chunker.exceptions import LanguageNotFoundError, LibraryNotFoundError\n\ntry:\n    chunks = chunk_file(\"example.py\", \"python\")\nexcept LanguageNotFoundError as e:\n    print(f\"Language not available: {e}\")\n    available = list_languages()\n    print(f\"Available languages: {', '.join(available)}\")\nexcept LibraryNotFoundError as e:\n    print(f\"Library not found: {e}\")\n    print(\"Run: python scripts/build_lib.py\")\n</code></pre>"},{"location":"api-reference/#thread-safety","title":"Thread Safety","text":"<p>The library is designed to be thread-safe for concurrent processing:</p>"},{"location":"api-reference/#thread-safe-components","title":"Thread-Safe Components","text":"<ul> <li>LanguageRegistry: Thread-safe for all read operations after initialization</li> <li>ParserFactory: Thread-safe with internal locking for cache and pool operations</li> <li>PluginManager: Thread-safe plugin registration and retrieval</li> <li>ASTCache: Thread-safe with concurrent access support</li> <li>get_parser/return_parser: Thread-safe API functions</li> </ul>"},{"location":"api-reference/#non-thread-safe-components","title":"Non Thread-Safe Components","text":"<ul> <li>Parser instances: NOT thread-safe - each thread must use its own parser</li> <li>Tree objects: NOT thread-safe - parse results should not be shared</li> <li>CodeChunk objects: Safe to share after creation (immutable)</li> </ul>"},{"location":"api-reference/#concurrent-usage-example","title":"Concurrent Usage Example","text":"<pre><code>import threading\nfrom concurrent.futures import ThreadPoolExecutor\nfrom chunker.core import chunk_file\nfrom chunker.parallel import chunk_files_parallel\n\n# Safe concurrent processing using high-level API\nfiles = [\"file1.py\", \"file2.py\", \"file3.py\"]\nresults = chunk_files_parallel(files, \"python\", max_workers=4)\n\n# Manual concurrent processing\ndef process_file(file_path):\n    # Each thread gets its own parser automatically\n    return chunk_file(file_path, \"python\")\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    futures = [executor.submit(process_file, f) for f in files]\n    results = [f.result() for f in futures]\n</code></pre>"},{"location":"api-reference/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api-reference/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use AST Caching: Enable caching for repeated file processing    <code>python    # Cache is enabled by default    chunks1 = chunk_file(\"file.py\", \"python\")  # Parses    chunks2 = chunk_file(\"file.py\", \"python\")  # Uses cache (11.9x faster)</code></p> </li> <li> <p>Process Files in Parallel: Use parallel processing for multiple files    <code>python    results = chunk_files_parallel(file_list, \"python\", max_workers=8)</code></p> </li> <li> <p>Stream Large Files: Use streaming for very large files    <code>python    for chunk in chunk_file_streaming(\"huge_file.py\", \"python\"):        process_chunk(chunk)</code></p> </li> <li> <p>Configure Cache Size: Adjust cache size based on available memory    <code>python    from chunker.cache import ASTCache    cache = ASTCache(max_size=500)  # Cache up to 500 ASTs</code></p> </li> <li> <p>Use Appropriate Export Format: Choose format based on use case</p> </li> <li>JSON: Human-readable, good for small datasets</li> <li>JSONL: Streaming-friendly, good for large datasets</li> <li>Parquet: Best for analytics, supports compression and partitioning</li> </ol>"},{"location":"api-reference/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>Parser Creation: ~10-50ms (one-time cost)</li> <li>Parsing: O(n) with file size</li> <li>Caching: 11.9x speedup for cached files</li> <li>Parallel Processing: Near-linear speedup with CPU cores</li> <li>Memory Usage: ~10x source file size for AST</li> </ul>"},{"location":"api-reference/#see-also","title":"See Also","text":"<ul> <li>Getting Started - Quick introduction tutorial</li> <li>User Guide - Comprehensive usage guide</li> <li>Plugin Development - Creating custom language plugins</li> <li>Configuration - Configuration file reference</li> <li>Performance Guide - Optimization strategies</li> <li>Export Formats - Detailed export documentation</li> <li>Architecture - System design and internals</li> <li>Cookbook - Common recipes and examples</li> </ul>"},{"location":"architecture/","title":"Tree-sitter Chunker Architecture","text":"<p>This document provides a comprehensive overview of the Tree-sitter Chunker architecture, including system design, component interactions, and implementation details.</p>"},{"location":"architecture/#table-of-contents","title":"Table of Contents","text":"<ol> <li>System Overview</li> <li>Architecture Diagrams</li> <li>Core Components</li> <li>Data Flow</li> <li>Design Decisions</li> <li>Extension Points</li> <li>Performance Considerations</li> <li>Security Considerations</li> <li>Future Enhancements</li> </ol>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>Tree-sitter Chunker is designed as a modular, efficient system for semantic code chunking. The architecture emphasizes:</p> <ul> <li>Dynamic Language Discovery: Languages are discovered at runtime from compiled shared libraries</li> <li>Efficient Resource Management: Parser caching and pooling for optimal performance</li> <li>Thread Safety: Concurrent processing support with proper synchronization</li> <li>Extensibility: Easy addition of new languages and chunk types</li> </ul>"},{"location":"architecture/#architecture-diagrams","title":"Architecture Diagrams","text":""},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph \"User Interface Layer\"\n        CLI[CLI Interface&lt;br/&gt;Rich UI]\n        API[Python API&lt;br/&gt;27 Exports]\n    end\n\n    subgraph \"Plugin Layer\"\n        PLM[PluginManager]\n        PL[Language Plugins]\n        CFG[ChunkerConfig]\n    end\n\n    subgraph \"Core Chunking Layer\"\n        CM[Chunker Module]\n        PC[ParallelChunker]\n        SC[StreamingChunker]\n        CC[CodeChunk Dataclass]\n    end\n\n    subgraph \"Performance Layer\"\n        AC[ASTCache&lt;br/&gt;11.9x Speedup]\n        TP[Thread Pool]\n    end\n\n    subgraph \"Parser Management Layer\"\n        PM[Parser Module&lt;br/&gt;API Functions]\n        PF[ParserFactory&lt;br/&gt;Caching &amp; Pooling]\n    end\n\n    subgraph \"Language Layer\"\n        LR[LanguageRegistry&lt;br/&gt;Dynamic Discovery]\n        LM[Language Metadata]\n    end\n\n    subgraph \"Export Layer\"\n        JE[JSON/JSONL&lt;br/&gt;Exporters]\n        PE[Parquet&lt;br/&gt;Exporter]\n    end\n\n    subgraph \"Native Layer\"\n        SO[Compiled .so Library&lt;br/&gt;tree-sitter grammars]\n        TS[tree-sitter&lt;br/&gt;Parser Runtime]\n    end\n\n    CLI --&gt; CFG\n    CLI --&gt; CM\n    API --&gt; CM\n    API --&gt; PC\n    API --&gt; SC\n    API --&gt; PLM\n\n    CFG --&gt; PLM\n    PLM --&gt; PL\n    PL --&gt; CM\n\n    CM --&gt; AC\n    PC --&gt; TP\n    PC --&gt; CM\n    SC --&gt; CM\n\n    CM --&gt; PM\n    CM --&gt; CC\n    AC --&gt; PM\n    PM --&gt; PF\n    PF --&gt; LR\n    LR --&gt; LM\n    LR --&gt; SO\n    PF --&gt; TS\n    TS --&gt; SO\n\n    CC --&gt; JE\n    CC --&gt; PE\n</code></pre>"},{"location":"architecture/#component-interaction-diagram","title":"Component Interaction Diagram","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI/API\n    participant Chunker\n    participant Parser\n    participant Factory\n    participant Registry\n    participant Library\n\n    User-&gt;&gt;CLI/API: chunk_file(\"example.py\", \"python\")\n    CLI/API-&gt;&gt;Chunker: Process file\n    Chunker-&gt;&gt;Parser: get_parser(\"python\")\n    Parser-&gt;&gt;Factory: Request parser\n\n    alt Parser in cache\n        Factory--&gt;&gt;Parser: Return cached parser\n    else Parser not in cache\n        Factory-&gt;&gt;Registry: get_language(\"python\")\n        Registry-&gt;&gt;Library: Load language function\n        Library--&gt;&gt;Registry: Language object\n        Registry--&gt;&gt;Factory: Language metadata\n        Factory-&gt;&gt;Factory: Create new parser\n        Factory--&gt;&gt;Parser: New parser instance\n    end\n\n    Parser--&gt;&gt;Chunker: Parser instance\n    Chunker-&gt;&gt;Chunker: Parse file &amp; extract chunks\n    Chunker--&gt;&gt;CLI/API: List[CodeChunk]\n    CLI/API--&gt;&gt;User: Formatted output\n\n    Note over Parser,Factory: Parser returned to pool for reuse\n</code></pre>"},{"location":"architecture/#parser-factory-architecture","title":"Parser Factory Architecture","text":"<pre><code>graph LR\n    subgraph \"ParserFactory\"\n        subgraph \"LRU Cache\"\n            C1[Parser 1]\n            C2[Parser 2]\n            C3[Parser N]\n        end\n\n        subgraph \"Language Pools\"\n            subgraph \"Python Pool\"\n                PP1[Parser]\n                PP2[Parser]\n            end\n            subgraph \"JavaScript Pool\"\n                JP1[Parser]\n                JP2[Parser]\n            end\n            subgraph \"Rust Pool\"\n                RP1[Parser]\n            end\n        end\n\n        PM[Pool Manager&lt;br/&gt;Thread-safe]\n        CM[Cache Manager&lt;br/&gt;LRU Eviction]\n    end\n\n    REQ[Parser Request] --&gt; PM\n    PM --&gt; CM\n    CM --&gt; C1\n    PM --&gt; PP1\n    RET[Parser Return] --&gt; PM\n</code></pre>"},{"location":"architecture/#language-discovery-flow","title":"Language Discovery Flow","text":"<pre><code>flowchart TD\n    Start([Library Load]) --&gt; Check{Library exists?}\n    Check --&gt;|No| Error[LibraryNotFoundError]\n    Check --&gt;|Yes| Load[Load .so file]\n\n    Load --&gt; Scan[Scan for symbols]\n    Scan --&gt; Filter[Filter tree_sitter_* functions]\n\n    Filter --&gt; Loop{For each symbol}\n    Loop --&gt; Extract[Extract language name]\n    Extract --&gt; LoadLang[Load language function]\n    LoadLang --&gt; GetMeta[Get language metadata]\n    GetMeta --&gt; Store[Store in registry]\n    Store --&gt; Loop\n\n    Loop --&gt;|Done| Complete[Registry initialized]\n\n    style Error fill:#f9f,stroke:#333,stroke-width:2px\n    style Complete fill:#9f9,stroke:#333,stroke-width:2px\n</code></pre>"},{"location":"architecture/#chunking-process-flow","title":"Chunking Process Flow","text":"<pre><code>flowchart TD\n    Input[Source File] --&gt; Read[Read file content]\n    Read --&gt; Parse[Parse with tree-sitter]\n    Parse --&gt; AST[Abstract Syntax Tree]\n\n    AST --&gt; Traverse[Traverse AST]\n    Traverse --&gt; Check{Is chunk node?}\n\n    Check --&gt;|Yes| Extract[Extract node info]\n    Extract --&gt; Context[Determine parent context]\n    Context --&gt; Create[Create CodeChunk]\n    Create --&gt; Collect[Add to results]\n\n    Check --&gt;|No| Next[Next node]\n    Next --&gt; Check\n\n    Collect --&gt; More{More nodes?}\n    More --&gt;|Yes| Next\n    More --&gt;|No| Output[List of CodeChunks]\n\n    style Input fill:#99f,stroke:#333,stroke-width:2px\n    style Output fill:#9f9,stroke:#333,stroke-width:2px\n</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-parser-module-chunkerparserpy","title":"1. Parser Module (<code>chunker/parser.py</code>)","text":"<p>The main API entry point providing simple functions for common operations:</p> <pre><code># Singleton instances\n_registry: Optional[LanguageRegistry] = None\n_factory: Optional[ParserFactory] = None\n\n# Public API\ndef get_parser(language: str, config: Optional[ParserConfig] = None) -&gt; Parser\ndef list_languages() -&gt; List[str]\ndef get_language_info(language: str) -&gt; LanguageMetadata\ndef return_parser(language: str, parser: Parser) -&gt; None\ndef clear_cache() -&gt; None\n</code></pre> <p>Responsibilities: - Lazy initialization of registry and factory - Simple API for parser management - Thread-safe singleton pattern</p>"},{"location":"architecture/#2-language-registry-chunkerregistrypy","title":"2. Language Registry (<code>chunker/registry.py</code>)","text":"<p>Manages dynamic language discovery and loading:</p> <pre><code>class LanguageRegistry:\n    def __init__(self, library_path: Path)\n    def discover_languages() -&gt; Dict[str, LanguageMetadata]\n    def get_language(name: str) -&gt; Language\n    def has_language(name: str) -&gt; bool\n</code></pre> <p>Key Features: - Dynamic symbol discovery from .so files - Metadata extraction (version, node types, scanner info) - Language function loading via ctypes - Compatibility checking</p>"},{"location":"architecture/#3-parser-factory-chunkerfactorypy","title":"3. Parser Factory (<code>chunker/factory.py</code>)","text":"<p>Provides efficient parser creation with caching and pooling:</p> <pre><code>class ParserFactory:\n    def __init__(self, registry: LanguageRegistry, cache_size: int = 10, pool_size: int = 5)\n    def get_parser(language: str, config: Optional[ParserConfig] = None) -&gt; Parser\n    def return_parser(language: str, parser: Parser) -&gt; None\n    def clear_cache() -&gt; None\n    def get_stats() -&gt; Dict[str, Any]\n</code></pre> <p>Optimization Strategies: - LRU cache for recently used parsers - Per-language parser pools - Thread-safe operations with locking - Configurable cache and pool sizes</p>"},{"location":"architecture/#4-chunker-module-chunkerchunkerpy","title":"4. Chunker Module (<code>chunker/chunker.py</code>)","text":"<p>Implements the core chunking logic:</p> <pre><code>def chunk_file(path: Union[str, Path], language: str) -&gt; List[CodeChunk]\n\n@dataclass\nclass CodeChunk:\n    language: str\n    file_path: str\n    node_type: str\n    start_line: int\n    end_line: int\n    byte_start: int\n    byte_end: int\n    parent_context: str\n    content: str\n</code></pre> <p>Chunking Strategy: - Language-specific node type patterns - Parent context tracking for nested structures - Line and byte offset preservation</p>"},{"location":"architecture/#5-exception-hierarchy-chunkerexceptionspy","title":"5. Exception Hierarchy (<code>chunker/exceptions.py</code>)","text":"<p>Comprehensive error handling with recovery suggestions:</p> <pre><code>ChunkerError (base)\n\u251c\u2500\u2500 LanguageError\n\u2502   \u251c\u2500\u2500 LanguageNotFoundError\n\u2502   \u2514\u2500\u2500 LanguageLoadError\n\u251c\u2500\u2500 ParserError\n\u2502   \u251c\u2500\u2500 ParserInitError\n\u2502   \u2514\u2500\u2500 ParserConfigError\n\u2514\u2500\u2500 LibraryError\n    \u251c\u2500\u2500 LibraryNotFoundError\n    \u251c\u2500\u2500 LibraryLoadError\n    \u2514\u2500\u2500 LibrarySymbolError\n</code></pre>"},{"location":"architecture/#6-plugin-manager-chunkerpluginsmanagerpy","title":"6. Plugin Manager (<code>chunker/plugins/manager.py</code>)","text":"<p>Manages dynamic plugin loading and registration:</p> <pre><code>class PluginManager:\n    def __init__(self)\n    def register_plugin(plugin: LanguagePlugin) -&gt; None\n    def get_plugin(language: str) -&gt; Optional[LanguagePlugin]\n    def list_plugins() -&gt; List[str]\n    def load_built_in_plugins() -&gt; None\n    def load_plugin_directory(directory: Path) -&gt; int\n</code></pre> <p>Key Features: - Dynamic plugin discovery from directories - Built-in plugin support (Python, JavaScript, Rust, C, C++) - Plugin validation and error handling - Thread-safe plugin registration</p>"},{"location":"architecture/#7-ast-cache-chunkercachepy","title":"7. AST Cache (<code>chunker/cache.py</code>)","text":"<p>Provides intelligent caching for parsed ASTs with 11.9x performance improvement:</p> <pre><code>class ASTCache:\n    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600)\n    def get(file_path: str, language: str) -&gt; Optional[CachedAST]\n    def set(file_path: str, language: str, tree: Tree) -&gt; None\n    def clear() -&gt; None\n    def get_stats() -&gt; Dict[str, Any]\n</code></pre> <p>Cache Strategy: - LRU eviction policy - File modification time tracking - Size-based eviction - Thread-safe operations - Memory usage monitoring</p>"},{"location":"architecture/#8-parallel-chunker-chunkerparallelpy","title":"8. Parallel Chunker (<code>chunker/parallel.py</code>)","text":"<p>Enables high-performance parallel processing:</p> <pre><code>def chunk_files_parallel(file_paths, language, max_workers=None, show_progress=True)\ndef chunk_directory_parallel(directory, language, pattern=\"**/*\", max_workers=None)\n</code></pre> <p>Features: - Thread pool executor for concurrent processing - Progress tracking with rich progress bars - Automatic worker count optimization - Error isolation per file - Batch result aggregation</p>"},{"location":"architecture/#9-streaming-chunker-chunkerstreamingpy","title":"9. Streaming Chunker (<code>chunker/streaming.py</code>)","text":"<p>Memory-efficient processing for large files:</p> <pre><code>def chunk_file_streaming(file_path, language, chunk_size=1024*1024)\n</code></pre> <p>Streaming Features: - Incremental file reading - Partial AST parsing - Generator-based chunk yield - Configurable buffer size - Memory usage bounds</p>"},{"location":"architecture/#10-configuration-system-chunkerconfigpy","title":"10. Configuration System (<code>chunker/config.py</code>)","text":"<p>Flexible configuration management:</p> <pre><code>@dataclass\nclass ChunkerConfig:\n    min_chunk_size: int = 3\n    max_chunk_size: int = 300\n    chunk_types: List[str] = None\n    plugin_dirs: List[str] = None\n\n@dataclass\nclass LanguageConfig:\n    chunk_types: List[str]\n    min_chunk_size: int\n    max_chunk_size: int\n    include_comments: bool = False\n</code></pre> <p>Configuration Sources: - <code>.chunkerrc</code> files - TOML/YAML/JSON configs - Environment variables - Runtime overrides - Language-specific settings</p>"},{"location":"architecture/#11-export-system-chunkerexport-chunkerexporters","title":"11. Export System (<code>chunker/export/</code>, <code>chunker/exporters/</code>)","text":"<p>Comprehensive export format support:</p> <pre><code># JSON/JSONL Exporters\nclass JSONExporter:\n    def export(chunks, output_path, schema_type=SchemaType.FLAT)\n\nclass JSONLExporter:\n    def export_streaming(chunk_generator, output_path)\n\n# Parquet Exporter\nclass ParquetExporter:\n    def export(chunks, output_path, compression=\"snappy\")\n    def export_partitioned(chunks, output_dir, partition_cols)\n</code></pre> <p>Export Features: - Multiple schema types (flat, nested, relational) - Compression support (gzip, snappy, brotli, lz4, zstd) - Streaming export for large datasets - Partitioned output for analytics - Column selection and filtering</p>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#1-initialization-flow","title":"1. Initialization Flow","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Uninitialized\n    Uninitialized --&gt; LoadingLibrary: First API call\n    LoadingLibrary --&gt; DiscoveringLanguages: Library loaded\n    DiscoveringLanguages --&gt; CreatingFactory: Languages discovered\n    CreatingFactory --&gt; Ready: Factory created\n    Ready --&gt; Processing: Handle requests\n\n    LoadingLibrary --&gt; Error: Library not found\n    DiscoveringLanguages --&gt; Error: No languages found\n    Error --&gt; [*]\n</code></pre>"},{"location":"architecture/#2-parser-lifecycle","title":"2. Parser Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Created: get_parser()\n    Created --&gt; InUse: Parser returned\n    InUse --&gt; Cached: return_parser()\n    Cached --&gt; InUse: Cache hit\n    Cached --&gt; Evicted: LRU eviction\n    InUse --&gt; Released: No return (cleanup)\n    Evicted --&gt; [*]\n    Released --&gt; [*]\n</code></pre>"},{"location":"architecture/#3-chunk-extraction-process","title":"3. Chunk Extraction Process","text":"<ol> <li>File Reading: Read source file as bytes</li> <li>Cache Check: Check ASTCache for existing parse tree</li> <li>Parser Selection: Get appropriate parser from factory</li> <li>Plugin Resolution: Get language plugin for chunk rules</li> <li>Parsing: Generate AST using tree-sitter (or use cached)</li> <li>Traversal: Walk AST using plugin's chunk types</li> <li>Context Building: Track parent nodes using plugin's context extraction</li> <li>Chunk Creation: Extract node information into CodeChunk</li> <li>Collection: Aggregate all chunks from file</li> <li>Export: Optional export to JSON/JSONL/Parquet</li> </ol>"},{"location":"architecture/#4-parallel-processing-flow","title":"4. Parallel Processing Flow","text":"<pre><code>flowchart TD\n    Input[File List] --&gt; TPE[ThreadPoolExecutor]\n    TPE --&gt; W1[Worker 1]\n    TPE --&gt; W2[Worker 2]\n    TPE --&gt; W3[Worker N]\n\n    W1 --&gt; C1[Chunk File 1]\n    W2 --&gt; C2[Chunk File 2]\n    W3 --&gt; C3[Chunk File N]\n\n    C1 --&gt; AC[AST Cache]\n    C2 --&gt; AC\n    C3 --&gt; AC\n\n    AC --&gt; Results[Aggregated Results]\n    Results --&gt; PB[Progress Bar]\n    Results --&gt; Output[Dict[file, chunks]]\n\n    style AC fill:#99f,stroke:#333,stroke-width:2px\n    style PB fill:#9f9,stroke:#333,stroke-width:2px\n</code></pre>"},{"location":"architecture/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/#1-dynamic-language-discovery","title":"1. Dynamic Language Discovery","text":"<p>Decision: Discover languages at runtime from compiled .so file</p> <p>Rationale: - No hardcoded language list to maintain - Easy addition of new languages - Single source of truth (the compiled library) - Flexible deployment</p> <p>Implementation:</p> <pre><code># Scan for tree_sitter_* symbols\nsymbols = self._get_exported_symbols()\nfor symbol in symbols:\n    if symbol.startswith('tree_sitter_'):\n        language_name = symbol[12:]  # Remove prefix\n        # Load and register language\n</code></pre>"},{"location":"architecture/#2-parser-caching-strategy","title":"2. Parser Caching Strategy","text":"<p>Decision: Two-tier caching with LRU cache + language pools</p> <p>Rationale: - LRU cache for hot paths (recently used parsers) - Pools for concurrent access to same language - Balance between memory usage and performance - Configurable limits</p> <p>Trade-offs: - Memory overhead vs. parser creation cost - Complexity vs. performance gains</p>"},{"location":"architecture/#3-thread-safety-approach","title":"3. Thread Safety Approach","text":"<p>Decision: Coarse-grained locking at factory level</p> <p>Rationale: - Simple to implement and reason about - Parser creation is relatively infrequent - Prevents race conditions in cache/pool management - Individual parsers are not thread-safe anyway</p> <p>Implementation:</p> <pre><code>with self._lock:\n    # Check cache\n    # Check pool\n    # Create if needed\n</code></pre>"},{"location":"architecture/#4-error-handling-philosophy","title":"4. Error Handling Philosophy","text":"<p>Decision: Rich exception hierarchy with recovery suggestions</p> <p>Rationale: - Clear error categorization - Actionable error messages - Easy error handling for library users - Self-documenting error conditions</p> <p>Example:</p> <pre><code>class LibraryNotFoundError(LibraryError):\n    def __str__(self):\n        return f\"{super().__str__()}. Run 'python scripts/build_lib.py' to build.\"\n</code></pre>"},{"location":"architecture/#5-language-specific-patterns","title":"5. Language-Specific Patterns","text":"<p>Decision: Configurable node type patterns per language</p> <p>Rationale: - Different languages have different AST structures - Flexibility to extract meaningful chunks - Easy to extend for new languages</p> <p>Implementation:</p> <pre><code>LANGUAGE_CONFIGS = {\n    \"python\": {\n        \"chunk_node_types\": [\"function_definition\", \"class_definition\"],\n        \"method_indicator\": \"function_definition\",\n    },\n    \"javascript\": {\n        \"chunk_node_types\": [\"function_declaration\", \"class_declaration\", \n                            \"method_definition\", \"arrow_function\"],\n    },\n    # ...\n}\n</code></pre>"},{"location":"architecture/#extension-points","title":"Extension Points","text":""},{"location":"architecture/#1-adding-new-languages","title":"1. Adding New Languages","text":"<ol> <li>Add Grammar: Place grammar in <code>vendor/</code> directory</li> <li>Update Build Script: Add to <code>scripts/fetch_grammars.py</code></li> <li>Configure Patterns: Add to <code>LANGUAGE_CONFIGS</code> in chunker</li> <li>Compile: Run build scripts</li> </ol>"},{"location":"architecture/#2-custom-chunk-types","title":"2. Custom Chunk Types","text":"<p>Extend the chunking logic:</p> <pre><code># In chunker.py\nLANGUAGE_CONFIGS[\"python\"][\"chunk_node_types\"].append(\"decorated_definition\")\n</code></pre>"},{"location":"architecture/#3-parser-configuration","title":"3. Parser Configuration","text":"<p>Create custom parser configurations:</p> <pre><code>config = ParserConfig(\n    timeout_ms=10000,  # 10 second timeout\n    included_ranges=[(0, 1000)],  # Parse only first 1000 bytes\n)\nparser = get_parser(\"python\", config)\n</code></pre>"},{"location":"architecture/#4-custom-caching-strategy","title":"4. Custom Caching Strategy","text":"<p>Replace or extend the factory:</p> <pre><code>class CustomParserFactory(ParserFactory):\n    def __init__(self, registry, custom_cache):\n        super().__init__(registry)\n        self.cache = custom_cache\n</code></pre>"},{"location":"architecture/#5-language-metadata-enhancement","title":"5. Language Metadata Enhancement","text":"<p>Add custom metadata extraction:</p> <pre><code>class ExtendedLanguageRegistry(LanguageRegistry):\n    def _extract_metadata(self, language):\n        metadata = super()._extract_metadata(language)\n        # Add custom fields\n        metadata.capabilities[\"supports_async\"] = self._check_async_support(language)\n        return metadata\n</code></pre>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/#memory-usage","title":"Memory Usage","text":"<ul> <li>Parser Size: ~1MB per parser instance</li> <li>AST Size: ~10x source file size</li> <li>Parser Cache: N parsers \u00d7 parser size</li> <li>Pool Overhead: M languages \u00d7 pool size \u00d7 parser size</li> <li>AST Cache: Configurable limit (default 100 cached trees)</li> <li>Streaming Buffer: Configurable (default 1MB chunks)</li> </ul>"},{"location":"architecture/#performance-benchmarks","title":"Performance Benchmarks","text":"<ul> <li>Without AST Cache: ~50ms per file</li> <li>With AST Cache: ~4.2ms per file (11.9x speedup)</li> <li>Sequential Processing: O(n) with file count</li> <li>Parallel Processing: O(n/workers) with sufficient cores</li> <li>Streaming Overhead: ~5% vs full file parsing</li> </ul>"},{"location":"architecture/#cpu-usage","title":"CPU Usage","text":"<ul> <li>Parser Creation: ~10-50ms (one-time cost)</li> <li>Parsing: O(n) with file size</li> <li>Chunk Extraction: O(m) with node count</li> <li>Cache Lookup: O(1) average case</li> </ul>"},{"location":"architecture/#optimization-opportunities","title":"Optimization Opportunities","text":"<ol> <li>Parallel Processing: Thread pool for multiple files</li> <li>Incremental Parsing: Use tree-sitter's incremental parsing</li> <li>Lazy Loading: Load languages only when needed</li> <li>Memory Mapping: For very large files</li> <li>Custom Allocators: For AST memory management</li> </ol>"},{"location":"architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"architecture/#input-validation","title":"Input Validation","text":"<ul> <li>File size limits to prevent DoS</li> <li>Parser timeouts for malformed input</li> <li>Path validation for file access</li> </ul>"},{"location":"architecture/#memory-safety","title":"Memory Safety","text":"<ul> <li>Tree-sitter is written in C (memory-safe design)</li> <li>Python bindings handle memory management</li> <li>Bounded cache/pool sizes</li> </ul>"},{"location":"architecture/#thread-safety","title":"Thread Safety","text":"<ul> <li>Immutable language registry after init</li> <li>Synchronized factory operations</li> <li>No shared mutable state in chunks</li> </ul>"},{"location":"architecture/#plugin-development","title":"Plugin Development","text":""},{"location":"architecture/#plugin-architecture-overview","title":"Plugin Architecture Overview","text":"<p>The plugin system is now fully implemented with support for dynamic language extensions:</p> <pre><code>graph TB\n    subgraph \"Plugin System\"\n        PM[PluginManager]\n        LP[LanguagePlugin ABC]\n        PC[PluginConfig]\n\n        subgraph \"Built-in Plugins\"\n            PP[PythonPlugin]\n            JP[JavaScriptPlugin]\n            RP[RustPlugin]\n            CP[CPlugin]\n            CPP[CppPlugin]\n        end\n\n        subgraph \"Custom Plugins\"\n            UP[User Plugins]\n        end\n    end\n\n    PM --&gt; LP\n    LP --&gt; PP\n    LP --&gt; JP\n    LP --&gt; RP\n    LP --&gt; CP\n    LP --&gt; CPP\n    LP --&gt; UP\n    PC --&gt; PM\n</code></pre>"},{"location":"architecture/#creating-a-language-plugin","title":"Creating a Language Plugin","text":"<p>The plugin architecture is fully implemented with an abstract base class:</p> <pre><code>from chunker.plugins import LanguagePlugin\nfrom typing import Set, Dict, Any, Optional\n\nclass GoPlugin(LanguagePlugin):\n    \"\"\"Plugin for Go language support.\"\"\"\n\n    @property\n    def language_name(self) -&gt; str:\n        return \"go\"\n\n    @property\n    def supported_extensions(self) -&gt; Set[str]:\n        return {\".go\"}\n\n    @property\n    def default_config(self) -&gt; Dict[str, Any]:\n        return {\n            \"chunk_types\": [\n                \"function_declaration\",\n                \"method_declaration\", \n                \"type_declaration\",\n                \"interface_declaration\"\n            ],\n            \"min_chunk_size\": 3,\n            \"max_chunk_size\": 300,\n            \"include_tests\": False\n        }\n\n    def get_chunk_types(self) -&gt; List[str]:\n        \"\"\"Return node types to chunk.\"\"\"\n        return self.config.get(\"chunk_types\", self.default_config[\"chunk_types\"])\n\n    def extract_context(self, node, parent_context: Optional[str] = None) -&gt; str:\n        \"\"\"Extract context name for Go nodes.\"\"\"\n        if node.type == \"type_declaration\":\n            name_node = node.child_by_field_name('name')\n            if name_node:\n                return f\"type:{name_node.text.decode('utf-8')}\"\n        elif node.type == \"interface_declaration\":\n            name_node = node.child_by_field_name('name')\n            if name_node:\n                return f\"interface:{name_node.text.decode('utf-8')}\"\n        return parent_context\n\n    def should_chunk(self, node, file_path: str) -&gt; bool:\n        \"\"\"Determine if node should be chunked.\"\"\"\n        if not super().should_chunk(node, file_path):\n            return False\n\n        # Skip test functions if configured\n        if not self.config.get(\"include_tests\", False):\n            if node.type == \"function_declaration\" and file_path.endswith('_test.go'):\n                name_node = node.child_by_field_name('name')\n                if name_node and name_node.text.startswith(b'Test'):\n                    return False\n\n        return True\n\n    def validate_config(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"Validate plugin configuration.\"\"\"\n        super().validate_config(config)\n\n        # Validate Go-specific settings\n        valid_chunk_types = {\n            \"function_declaration\", \"method_declaration\",\n            \"type_declaration\", \"interface_declaration\",\n            \"package_declaration\", \"import_declaration\"\n        }\n\n        chunk_types = config.get(\"chunk_types\", [])\n        invalid_types = set(chunk_types) - valid_chunk_types\n        if invalid_types:\n            raise ValueError(f\"Invalid chunk types for Go: {invalid_types}\")\n</code></pre>"},{"location":"architecture/#plugin-registration-and-management","title":"Plugin Registration and Management","text":"<pre><code>from chunker import get_plugin_manager\nfrom pathlib import Path\n\n# Get the global plugin manager\nmanager = get_plugin_manager()\n\n# Load built-in plugins\nmanager.load_built_in_plugins()\nprint(f\"Loaded plugins: {manager.list_plugins()}\")\n# Output: ['python', 'javascript', 'rust', 'c', 'cpp']\n\n# Register custom plugin\ngo_plugin = GoPlugin()\nmanager.register_plugin(go_plugin)\n\n# Load plugins from directory\nplugin_count = manager.load_plugin_directory(Path(\"./custom_plugins\"))\nprint(f\"Loaded {plugin_count} custom plugins\")\n\n# Get plugin for a language\nplugin = manager.get_plugin(\"go\")\nif plugin:\n    print(f\"Go plugin chunk types: {plugin.get_chunk_types()}\")\n\n# Configure plugin\nmanager.configure_plugin(\"go\", {\n    \"min_chunk_size\": 5,\n    \"include_tests\": True\n})\n</code></pre>"},{"location":"architecture/#built-in-plugin-examples","title":"Built-in Plugin Examples","text":""},{"location":"architecture/#python-plugin","title":"Python Plugin","text":"<pre><code>class PythonPlugin(LanguagePlugin):\n    \"\"\"Built-in plugin for Python language.\"\"\"\n\n    @property\n    def language_name(self) -&gt; str:\n        return \"python\"\n\n    @property\n    def supported_extensions(self) -&gt; Set[str]:\n        return {\".py\", \".pyi\"}\n\n    @property\n    def default_config(self) -&gt; Dict[str, Any]:\n        return {\n            \"chunk_types\": [\n                \"function_definition\",\n                \"class_definition\",\n                \"async_function_definition\"\n            ],\n            \"min_chunk_size\": 3,\n            \"max_chunk_size\": 300,\n            \"include_decorators\": True,\n            \"include_docstrings\": True\n        }\n</code></pre>"},{"location":"architecture/#javascript-plugin","title":"JavaScript Plugin","text":"<pre><code>class JavaScriptPlugin(LanguagePlugin):\n    \"\"\"Built-in plugin for JavaScript/TypeScript.\"\"\"\n\n    @property\n    def supported_extensions(self) -&gt; Set[str]:\n        return {\".js\", \".jsx\", \".ts\", \".tsx\", \".mjs\"}\n\n    def extract_context(self, node, parent_context: Optional[str] = None) -&gt; str:\n        \"\"\"Handle ES6 classes and React components.\"\"\"\n        if node.type == \"class_declaration\":\n            # Check for React component\n            if self._is_react_component(node):\n                name = self._get_node_name(node)\n                return f\"component:{name}\"\n        return super().extract_context(node, parent_context)\n</code></pre>"},{"location":"architecture/#best-practices","title":"Best Practices","text":""},{"location":"architecture/#1-resource-management","title":"1. Resource Management","text":"<pre><code># Always use context managers or try/finally\nparser = get_parser(\"python\")\ntry:\n    # Use parser\n    tree = parser.parse(code)\nfinally:\n    return_parser(\"python\", parser)\n</code></pre>"},{"location":"architecture/#2-error-handling","title":"2. Error Handling","text":"<pre><code>try:\n    chunks = chunk_file(\"file.py\", \"python\")\nexcept LanguageNotFoundError as e:\n    # Handle missing language\n    logger.error(f\"Language not supported: {e}\")\n    # Use fallback or skip\nexcept LibraryNotFoundError as e:\n    # Handle missing library\n    logger.error(f\"Library not built: {e}\")\n    # Trigger build or alert user\n</code></pre>"},{"location":"architecture/#3-performance-optimization","title":"3. Performance Optimization","text":"<pre><code># Process files in parallel\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef process_codebase(files, language):\n    with ThreadPoolExecutor(max_workers=8) as executor:\n        futures = [executor.submit(chunk_file, f, language) for f in files]\n        return [f.result() for f in futures]\n</code></pre>"},{"location":"architecture/#4-memory-management","title":"4. Memory Management","text":"<pre><code># Clear cache when processing many files\nfrom chunker.parser import clear_cache\n\ndef process_large_codebase(directory):\n    files_processed = 0\n    for file in directory.glob(\"**/*.py\"):\n        chunks = chunk_file(file, \"python\")\n        process_chunks(chunks)\n\n        files_processed += 1\n        if files_processed % 100 == 0:\n            # Clear cache periodically\n            clear_cache()\n</code></pre>"},{"location":"architecture/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"architecture/#common-issues","title":"Common Issues","text":"<ol> <li>LibraryNotFoundError</li> <li>Cause: Compiled .so file missing</li> <li> <p>Solution: Run <code>python scripts/build_lib.py</code></p> </li> <li> <p>LanguageNotFoundError</p> </li> <li>Cause: Language not in compiled library</li> <li> <p>Solution: Add grammar and rebuild</p> </li> <li> <p>Parser Version Mismatch</p> </li> <li>Cause: ABI version incompatibility</li> <li> <p>Solution: Update py-tree-sitter from GitHub</p> </li> <li> <p>Empty Chunk Results</p> </li> <li>Cause: No matching node types</li> <li>Solution: Check language configuration</li> </ol>"},{"location":"architecture/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Will show:\n# - Language discovery process\n# - Parser creation/reuse\n# - Cache hits/misses\n# - Node traversal details\n</code></pre>"},{"location":"architecture/#current-state-and-future-enhancements","title":"Current State and Future Enhancements","text":""},{"location":"architecture/#recently-implemented-features","title":"Recently Implemented Features","text":"<p>Many features from the roadmap have been successfully implemented:</p> <ol> <li>Plugin System \u2705</li> <li>Complete plugin architecture with abstract base class</li> <li>Built-in plugins for Python, JavaScript, Rust, C, C++</li> <li>Dynamic plugin loading from directories</li> <li> <p>Per-plugin configuration management</p> </li> <li> <p>Performance Enhancements \u2705</p> </li> <li>AST Caching with 11.9x speedup</li> <li>Parallel processing with configurable workers</li> <li>Streaming API for large file handling</li> <li> <p>Progress tracking with rich UI</p> </li> <li> <p>Configuration Framework \u2705</p> </li> <li>Support for .chunkerrc, TOML, YAML, JSON</li> <li>Language-specific configurations</li> <li>Environment variable support</li> <li> <p>Runtime configuration overrides</p> </li> <li> <p>Export System \u2705</p> </li> <li>JSON export with multiple schema types</li> <li>JSONL streaming export</li> <li>Parquet export with compression and partitioning</li> <li> <p>Extensible export framework</p> </li> <li> <p>Advanced CLI \u2705</p> </li> <li>Batch processing capabilities</li> <li>File and chunk filtering</li> <li>Configuration file support</li> <li>Progress tracking and reporting</li> </ol>"},{"location":"architecture/#upcoming-features","title":"Upcoming Features","text":"<ol> <li>Incremental Processing: Track file changes and re-chunk only modified sections</li> <li>Query Language: Tree-sitter query support for custom chunk extraction</li> <li>Context-Aware Chunking: Include surrounding context with configurable overlap</li> <li>Token Counting: Integration with tokenizers for LLM applications</li> <li>Semantic Merging: Intelligently merge small related chunks</li> <li>Graph Export: Export chunk relationships as graph formats</li> </ol>"},{"location":"architecture/#potential-optimizations","title":"Potential Optimizations","text":"<ol> <li>Zero-Copy Parsing: Avoid content duplication</li> <li>Parallel AST Traversal: Multi-threaded chunk extraction</li> <li>Smart Caching: Predictive parser pre-loading</li> <li>Memory Pools: Reuse AST memory allocations</li> </ol>"},{"location":"architecture/#conclusion","title":"Conclusion","text":"<p>The Tree-sitter Chunker architecture prioritizes:</p> <ul> <li>Flexibility: Easy language addition and configuration</li> <li>Performance: Efficient caching and resource management</li> <li>Reliability: Comprehensive error handling and thread safety</li> <li>Usability: Simple API with powerful capabilities</li> </ul> <p>The modular design allows for easy extension and optimization while maintaining a clean, understandable structure suitable for both simple scripts and large-scale applications.</p>"},{"location":"cli-reference/","title":"CLI Reference","text":"<p>This page summarizes the command-line interface for Tree-sitter Chunker.</p>"},{"location":"cli-reference/#installation","title":"Installation","text":"<p>Run the CLI from the repository:</p> <pre><code>python -m pip install -e \".[dev]\"\n# or use uv\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"cli-reference/#commands","title":"Commands","text":""},{"location":"cli-reference/#chunk-a-single-file","title":"Chunk a single file","text":"<pre><code>python cli/main.py chunk example.py -l python\n# Output options\npython cli/main.py chunk example.py -l python --json &gt; chunks.json\n</code></pre>"},{"location":"cli-reference/#batch-process-a-directory","title":"Batch process a directory","text":"<pre><code>python cli/main.py batch src/ --recursive\n# Include / exclude patterns\npython cli/main.py batch src/ --include \"**/*.py\" --exclude \"**/tests/**,**/*.tmp\"\n</code></pre>"},{"location":"cli-reference/#zero-config-auto-detection","title":"Zero-config auto-detection","text":"<pre><code># Automatically detect language for a file and chunk it\npython cli/main.py auto-chunk path/to/file\n\n# Auto-chunk an entire directory using detection + intelligent fallbacks\npython cli/main.py auto-batch path/to/repo\n</code></pre>"},{"location":"cli-reference/#configuration","title":"Configuration","text":"<p>You can pass a configuration file to adjust chunk sizes, language rules, and filters:</p> <pre><code>python cli/main.py chunk src/ --config .chunkerrc\n</code></pre> <p>Supported formats: TOML, YAML, JSON. See the Configuration guide for details.</p>"},{"location":"cli-reference/#export-helpers","title":"Export helpers","text":"<p>Use exporters from Python for structured outputs (JSON, JSONL, Parquet, GraphML, Neo4j). See the Export Formats guide for examples.</p>"},{"location":"cli-reference/#environment-variables","title":"Environment variables","text":"<ul> <li><code>CHUNKER_BUILD_VERBOSE=1</code> \u2014 enable verbose build logs (build system)</li> <li><code>CHUNKER_WHEEL_LANGS=python,javascript,rust</code> \u2014 limit grammars compiled into wheels</li> <li><code>CHUNKER_BUILD_TIMEOUT=240</code> \u2014 build timeout in seconds</li> </ul> <p>These are primarily for contributors building distribution artifacts.</p>"},{"location":"config_processor/","title":"Configuration File Processor","text":"<p>The Configuration File Processor is a specialized component of the tree-sitter-chunker that intelligently handles various configuration file formats with section-based chunking.</p>"},{"location":"config_processor/#overview","title":"Overview","text":"<p>The ConfigProcessor provides intelligent chunking for configuration files by preserving logical sections and maintaining configuration relationships. It supports multiple formats including INI, TOML, YAML, and JSON.</p>"},{"location":"config_processor/#supported-formats","title":"Supported Formats","text":""},{"location":"config_processor/#ini-files-ini-cfg-conf","title":"INI Files (.ini, .cfg, .conf)","text":"<ul> <li>Section-based structure with <code>[sections]</code></li> <li>Key-value pairs with <code>key = value</code> format</li> <li>Comment preservation</li> <li>Multi-line value support</li> </ul>"},{"location":"config_processor/#toml-files-toml","title":"TOML Files (.toml)","text":"<ul> <li>Table-based structure with <code>[table]</code> and <code>[[array.tables]]</code></li> <li>Nested table support</li> <li>Type-aware value handling</li> <li>Array and inline table support</li> </ul>"},{"location":"config_processor/#yaml-files-yaml-yml","title":"YAML Files (.yaml, .yml)","text":"<ul> <li>Indentation-aware parsing</li> <li>Nested structure preservation</li> <li>Multi-document support (---)</li> <li>Anchor and alias handling</li> </ul>"},{"location":"config_processor/#json-files-json","title":"JSON Files (.json)","text":"<ul> <li>Object and array chunking</li> <li>Nested structure preservation</li> <li>Pretty-print aware parsing</li> <li>Schema-based chunking options</li> </ul>"},{"location":"config_processor/#features","title":"Features","text":""},{"location":"config_processor/#format-auto-detection","title":"Format Auto-detection","text":"<p>The processor automatically detects the configuration format based on: - File extension - Content patterns - Syntax markers</p>"},{"location":"config_processor/#section-based-chunking","title":"Section-based Chunking","text":"<ul> <li>Preserves logical configuration sections</li> <li>Maintains parent-child relationships</li> <li>Groups related sections when appropriate</li> <li>Respects configuration boundaries</li> </ul>"},{"location":"config_processor/#comment-preservation","title":"Comment Preservation","text":"<ul> <li>Maintains inline and block comments</li> <li>Associates comments with their sections</li> <li>Preserves documentation value</li> </ul>"},{"location":"config_processor/#structure-preservation","title":"Structure Preservation","text":"<ul> <li>Keeps nested configurations intact</li> <li>Maintains references between sections</li> <li>Preserves import/include directives</li> <li>Handles environment variable references</li> </ul>"},{"location":"config_processor/#usage","title":"Usage","text":""},{"location":"config_processor/#basic-usage","title":"Basic Usage","text":"<pre><code>from chunker.processors.config import ConfigProcessor\n\nprocessor = ConfigProcessor()\nchunks = processor.process_file(\"config.toml\")\n</code></pre>"},{"location":"config_processor/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code>from chunker.processors.config import ConfigProcessor, ProcessorConfig\n\nconfig = ProcessorConfig(\n    chunk_size=100,           # Target lines per chunk\n    preserve_structure=True,  # Keep sections intact\n    group_related=True,       # Group related sections\n    include_comments=True     # Include comments in chunks\n)\n\nprocessor = ConfigProcessor(config)\nchunks = processor.process_file(\"app.ini\")\n</code></pre>"},{"location":"config_processor/#integration-with-main-chunker","title":"Integration with Main Chunker","text":"<p>The ConfigProcessor is automatically used by the intelligent fallback system:</p> <pre><code>from chunker import IntelligentFallbackChunker\n\nchunker = IntelligentFallbackChunker()\nchunks = chunker.chunk_text(config_content, \"config.yaml\")\n</code></pre>"},{"location":"config_processor/#examples","title":"Examples","text":""},{"location":"config_processor/#ini-file-chunking","title":"INI File Chunking","text":"<pre><code>[database]\nhost = localhost\nport = 5432\nuser = admin\n\n[cache]\nenabled = true\nttl = 3600\n</code></pre> <p>This would produce chunks preserving each section with its configuration values.</p>"},{"location":"config_processor/#toml-file-chunking","title":"TOML File Chunking","text":"<pre><code>[package]\nname = \"my-app\"\nversion = \"1.0.0\"\n\n[[dependencies]]\nname = \"lib1\"\nversion = \"2.0\"\n\n[[dependencies]]\nname = \"lib2\"\nversion = \"3.0\"\n</code></pre> <p>Array tables are kept together while maintaining relationships.</p>"},{"location":"config_processor/#yaml-file-chunking","title":"YAML File Chunking","text":"<pre><code>server:\n  host: localhost\n  port: 8080\n\ndatabase:\n  primary:\n    host: db1.example.com\n  replica:\n    host: db2.example.com\n</code></pre> <p>Nested structures are preserved with proper indentation context.</p>"},{"location":"config_processor/#configuration-options","title":"Configuration Options","text":"Option Type Default Description <code>chunk_size</code> int 50 Target lines per chunk <code>preserve_structure</code> bool True Keep configuration sections intact <code>group_related</code> bool True Group small related sections <code>include_comments</code> bool True Include comments in chunks <code>min_section_size</code> int 3 Minimum lines to create separate chunk <code>max_section_size</code> int 200 Maximum lines per section chunk"},{"location":"config_processor/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Section Integrity: The processor prioritizes keeping configuration sections intact over strict size limits.</p> </li> <li> <p>Related Grouping: Small related sections (like server1, server2) can be grouped for better context.</p> </li> <li> <p>Comment Association: Comments immediately preceding sections are included with those sections.</p> </li> <li> <p>Format Detection: While auto-detection works well, specifying the format explicitly can improve performance.</p> </li> </ol>"},{"location":"config_processor/#integration-with-phase-11","title":"Integration with Phase 11","text":"<p>The ConfigProcessor is part of Phase 11's text processing capabilities and integrates with: - Sliding Window Fallback system - Intelligent Fallback Chunker - Token limit handling - Multi-format processing pipeline</p>"},{"location":"config_processor/#see-also","title":"See Also","text":"<ul> <li>Intelligent Fallback - Automatic processor selection</li> <li>Log Processor - Log file processing</li> <li>Token Limits - Token-aware chunking</li> </ul>"},{"location":"configuration/","title":"Configuration Reference","text":"<p>Tree-sitter Chunker supports flexible configuration through multiple formats and sources. This guide covers all configuration options, file formats, and best practices.</p>"},{"location":"configuration/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Configuration Overview</li> <li>Configuration Files</li> <li>File Formats</li> <li>Configuration Options</li> <li>Plugin Configuration</li> <li>CLI Configuration</li> <li>Environment Variables</li> <li>Configuration Precedence</li> <li>Examples</li> <li>Best Practices</li> </ol>"},{"location":"configuration/#configuration-overview","title":"Configuration Overview","text":"<p>Tree-sitter Chunker can be configured through:</p> <ol> <li>Configuration files - <code>.chunkerrc</code>, <code>chunker.config.toml</code>, etc.</li> <li>Command-line arguments - Override specific settings</li> <li>Environment variables - System-wide settings</li> <li>Programmatic configuration - In-code configuration</li> </ol>"},{"location":"configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"configuration/#file-locations","title":"File Locations","text":"<p>The configuration system searches for files in the following order:</p> <ol> <li>Current directory: <code>.chunkerrc</code>, <code>chunker.config.*</code></li> <li>Parent directories (up to root)</li> <li>User home directory: <code>~/.chunker/config.*</code></li> <li>System-wide: <code>/etc/chunker/config.*</code></li> </ol>"},{"location":"configuration/#file-names","title":"File Names","text":"<p>Supported configuration file names: - <code>.chunkerrc</code> (TOML format by default) - <code>chunker.config.toml</code> - <code>chunker.config.yaml</code> / <code>chunker.config.yml</code> - <code>chunker.config.json</code></p>"},{"location":"configuration/#discovery","title":"Discovery","text":"<pre><code>from chunker import ChunkerConfig\n\n# Automatically find configuration\nconfig = ChunkerConfig.find_config()\n\n# Or specify explicit path\nconfig = ChunkerConfig(\"/path/to/config.toml\")\n</code></pre>"},{"location":"configuration/#file-formats","title":"File Formats","text":""},{"location":"configuration/#toml-recommended","title":"TOML (Recommended)","text":"<pre><code># chunker.config.toml\n\n# Global settings\nchunk_types = [\"function_definition\", \"class_definition\", \"method_definition\"]\nmin_chunk_size = 3\nmax_chunk_size = 200\n\n# File filtering\ninclude_patterns = [\"*.py\", \"*.js\", \"*.rs\"]\nexclude_patterns = [\"*test*\", \"*__pycache__*\", \"*.min.js\"]\n\n# Processing options\nparallel_workers = 4\ncache_enabled = true\ncache_size = 100\n\n# Plugin directories\nplugin_dirs = [\"./plugins\", \"~/.chunker/plugins\"]\n\n# Language-specific settings\n[languages.python]\nenabled = true\nchunk_types = [\"function_definition\", \"class_definition\", \"async_function_definition\"]\nmin_chunk_size = 5\nmax_chunk_size = 300\n\n[languages.python.custom_options]\ninclude_docstrings = true\ninclude_type_hints = true\nskip_private = false\n\n[languages.javascript]\nenabled = true\nchunk_types = [\"function_declaration\", \"arrow_function\", \"class_declaration\"]\ninclude_jsx = true\n\n[languages.rust]\nenabled = true\nchunk_types = [\"function_item\", \"impl_item\", \"trait_item\", \"struct_item\"]\ninclude_tests = false\n\n# Export settings\n[export]\ndefault_format = \"json\"\ncompression = true\ninclude_metadata = true\n\n[export.json]\nindent = 2\nschema_type = \"nested\"\n\n[export.parquet]\ncompression = \"snappy\"\npartition_by = [\"language\", \"file_path\"]\n</code></pre>"},{"location":"configuration/#yaml","title":"YAML","text":"<pre><code># chunker.config.yaml\n\n# Global settings\nchunk_types:\n  - function_definition\n  - class_definition\n  - method_definition\nmin_chunk_size: 3\nmax_chunk_size: 200\n\n# File filtering\ninclude_patterns:\n  - \"*.py\"\n  - \"*.js\"\n  - \"*.rs\"\nexclude_patterns:\n  - \"*test*\"\n  - \"*__pycache__*\"\n  - \"*.min.js\"\n\n# Processing options\nparallel_workers: 4\ncache_enabled: true\ncache_size: 100\n\n# Plugin directories\nplugin_dirs:\n  - ./plugins\n  - ~/.chunker/plugins\n\n# Language-specific settings\nlanguages:\n  python:\n    enabled: true\n    chunk_types:\n      - function_definition\n      - class_definition\n      - async_function_definition\n    min_chunk_size: 5\n    max_chunk_size: 300\n    custom_options:\n      include_docstrings: true\n      include_type_hints: true\n      skip_private: false\n\n  javascript:\n    enabled: true\n    chunk_types:\n      - function_declaration\n      - arrow_function\n      - class_declaration\n    include_jsx: true\n\n  rust:\n    enabled: true\n    chunk_types:\n      - function_item\n      - impl_item\n      - trait_item\n      - struct_item\n    include_tests: false\n\n# Export settings\nexport:\n  default_format: json\n  compression: true\n  include_metadata: true\n\n  json:\n    indent: 2\n    schema_type: nested\n\n  parquet:\n    compression: snappy\n    partition_by:\n      - language\n      - file_path\n</code></pre>"},{"location":"configuration/#json","title":"JSON","text":"<pre><code>{\n  \"chunk_types\": [\n    \"function_definition\",\n    \"class_definition\",\n    \"method_definition\"\n  ],\n  \"min_chunk_size\": 3,\n  \"max_chunk_size\": 200,\n  \"include_patterns\": [\"*.py\", \"*.js\", \"*.rs\"],\n  \"exclude_patterns\": [\"*test*\", \"*__pycache__*\", \"*.min.js\"],\n  \"parallel_workers\": 4,\n  \"cache_enabled\": true,\n  \"cache_size\": 100,\n  \"plugin_dirs\": [\"./plugins\", \"~/.chunker/plugins\"],\n  \"languages\": {\n    \"python\": {\n      \"enabled\": true,\n      \"chunk_types\": [\n        \"function_definition\",\n        \"class_definition\",\n        \"async_function_definition\"\n      ],\n      \"min_chunk_size\": 5,\n      \"max_chunk_size\": 300,\n      \"custom_options\": {\n        \"include_docstrings\": true,\n        \"include_type_hints\": true,\n        \"skip_private\": false\n      }\n    }\n  },\n  \"export\": {\n    \"default_format\": \"json\",\n    \"compression\": true,\n    \"include_metadata\": true,\n    \"json\": {\n      \"indent\": 2,\n      \"schema_type\": \"nested\"\n    },\n    \"parquet\": {\n      \"compression\": \"snappy\",\n      \"partition_by\": [\"language\", \"file_path\"]\n    }\n  }\n}\n</code></pre>"},{"location":"configuration/#configuration-options","title":"Configuration Options","text":""},{"location":"configuration/#global-options","title":"Global Options","text":"Option Type Default Description <code>chunk_types</code> List[str] Language defaults Node types to extract as chunks <code>min_chunk_size</code> int 0 Minimum chunk size in lines <code>max_chunk_size</code> int 1000000 Maximum chunk size in lines <code>include_patterns</code> List[str] [] File patterns to include <code>exclude_patterns</code> List[str] [] File patterns to exclude <code>parallel_workers</code> int CPU count Number of parallel workers <code>cache_enabled</code> bool true Enable AST caching <code>cache_size</code> int 100 Maximum cache entries <code>plugin_dirs</code> List[str] [] Additional plugin directories <code>enabled_languages</code> List[str] All Languages to enable <code>log_level</code> str \"INFO\" Logging level"},{"location":"configuration/#language-specific-options","title":"Language-Specific Options","text":"<p>Each language can have its own configuration under the <code>languages</code> section:</p> Option Type Default Description <code>enabled</code> bool true Enable this language <code>chunk_types</code> List[str] Plugin defaults Override chunk types <code>min_chunk_size</code> int Global value Language-specific minimum <code>max_chunk_size</code> int Global value Language-specific maximum <code>file_extensions</code> List[str] Plugin defaults File extensions to process <code>custom_options</code> Dict {} Plugin-specific options"},{"location":"configuration/#export-options","title":"Export Options","text":"<p>Configure export formats under the <code>export</code> section:</p> Option Type Default Description <code>default_format</code> str \"json\" Default export format <code>compression</code> bool false Enable compression <code>include_metadata</code> bool true Include chunk metadata"},{"location":"configuration/#json-export-options","title":"JSON Export Options","text":"Option Type Default Description <code>indent</code> int 2 JSON indentation <code>schema_type</code> str \"flat\" Schema type: flat, nested, relational <code>include_line_numbers</code> bool true Include line numbers"},{"location":"configuration/#parquet-export-options","title":"Parquet Export Options","text":"Option Type Default Description <code>compression</code> str \"snappy\" Compression: snappy, gzip, brotli, lz4, zstd <code>partition_by</code> List[str] [] Columns to partition by <code>row_group_size</code> int 5000 Rows per group"},{"location":"configuration/#plugin-configuration","title":"Plugin Configuration","text":""},{"location":"configuration/#plugin-discovery","title":"Plugin Discovery","text":"<pre><code># Enable/disable plugin discovery\n[plugins]\nauto_discover = true\nentry_points = true  # Discover from Python entry points\nbuiltin = true       # Load built-in plugins\n\n# Additional plugin directories\nplugin_dirs = [\n    \"./my_plugins\",\n    \"~/.chunker/plugins\",\n    \"/usr/local/share/chunker/plugins\"\n]\n\n# Explicitly load plugins\nload_plugins = [\"swift\", \"kotlin\", \"scala\"]\n</code></pre>"},{"location":"configuration/#per-plugin-configuration","title":"Per-Plugin Configuration","text":"<pre><code>[plugins.python]\nenabled = true\nchunk_types = [\"function_definition\", \"class_definition\"]\nmin_chunk_size = 3\nmax_chunk_size = 500\n\n# Plugin-specific options\n[plugins.python.custom_options]\ninclude_docstrings = true\ninclude_decorators = true\ninclude_type_hints = true\nskip_private = false\nskip_tests = false\n\n[plugins.javascript]\nenabled = true\ninclude_jsx = true\ninclude_typescript = true\nes_module_syntax = true\n\n[plugins.rust]\nenabled = true\ninclude_tests = false\ninclude_docs = true\ninclude_macros = false\n</code></pre>"},{"location":"configuration/#cli-configuration","title":"CLI Configuration","text":"<p>Use command-line flags to override file settings. See the CLI Reference.</p>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Environment variables provide system-wide defaults:</p>"},{"location":"configuration/#general-variables","title":"General Variables","text":"<pre><code># Configuration file location\nexport CHUNKER_CONFIG_PATH=/etc/chunker/config.toml\n\n# Logging\nexport CHUNKER_LOG_LEVEL=DEBUG\nexport CHUNKER_LOG_FILE=/var/log/chunker.log\n\n# Performance\nexport CHUNKER_CACHE_SIZE=200\nexport CHUNKER_PARALLEL_WORKERS=8\n\n# Plugin directories\nexport CHUNKER_PLUGIN_PATH=/opt/chunker/plugins:/usr/local/lib/chunker\n</code></pre>"},{"location":"configuration/#language-specific-variables","title":"Language-Specific Variables","text":"<pre><code># Python configuration\nexport CHUNKER_PYTHON_ENABLED=true\nexport CHUNKER_PYTHON_MIN_SIZE=5\nexport CHUNKER_PYTHON_INCLUDE_DOCSTRINGS=true\n\n# JavaScript configuration\nexport CHUNKER_JAVASCRIPT_ENABLED=true\nexport CHUNKER_JAVASCRIPT_INCLUDE_JSX=true\n\n# Rust configuration\nexport CHUNKER_RUST_ENABLED=true\nexport CHUNKER_RUST_INCLUDE_TESTS=false\n</code></pre>"},{"location":"configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>Configuration sources are applied in the following order (later sources override earlier ones):</p> <ol> <li>Built-in defaults</li> <li>System configuration (<code>/etc/chunker/config.*</code>)</li> <li>User configuration (<code>~/.chunker/config.*</code>)</li> <li>Project configuration (<code>.chunkerrc</code>, <code>chunker.config.*</code>)</li> <li>Environment variables</li> <li>Command-line arguments</li> <li>Programmatic configuration</li> </ol>"},{"location":"configuration/#example-precedence","title":"Example Precedence","text":"<pre><code># Built-in default: min_chunk_size = 0\n# User config: min_chunk_size = 3\n# Project config: min_chunk_size = 5\n# Environment: CHUNKER_MIN_SIZE = 10\n# CLI: --min-size 15\n\n# Final value: min_chunk_size = 15\n</code></pre>"},{"location":"configuration/#examples","title":"Examples","text":""},{"location":"configuration/#basic-configuration","title":"Basic Configuration","text":"<p>Simple configuration for a Python project:</p> <pre><code># .chunkerrc\nchunk_types = [\"function_definition\", \"class_definition\"]\nmin_chunk_size = 5\nexclude_patterns = [\"*test*.py\", \"__pycache__\"]\n</code></pre>"},{"location":"configuration/#multi-language-project","title":"Multi-Language Project","text":"<p>Configuration for a project with multiple languages:</p> <pre><code># chunker.config.toml\n\n# Global defaults\nmin_chunk_size = 3\nmax_chunk_size = 300\nparallel_workers = 4\n\n# Python files\n[languages.python]\nchunk_types = [\"function_definition\", \"class_definition\", \"async_function_definition\"]\nexclude_patterns = [\"*_test.py\", \"test_*.py\"]\n\n[languages.python.custom_options]\ninclude_docstrings = true\nskip_private = true\n\n# JavaScript/TypeScript files\n[languages.javascript]\nchunk_types = [\"function_declaration\", \"arrow_function\", \"class_declaration\"]\nfile_extensions = [\".js\", \".jsx\", \".ts\", \".tsx\"]\ninclude_jsx = true\n\n# Rust files\n[languages.rust]\nchunk_types = [\"function_item\", \"impl_item\", \"trait_item\"]\ninclude_tests = false\ninclude_docs = false\n</code></pre>"},{"location":"configuration/#cicd-configuration","title":"CI/CD Configuration","text":"<p>Configuration for continuous integration:</p> <pre><code># ci.config.toml\n\n# Strict settings for CI\nmin_chunk_size = 1  # Don't skip small functions\nmax_chunk_size = 500  # Flag large functions\nparallel_workers = 2  # Limit resource usage\n\n# Only process source files\ninclude_patterns = [\"src/**/*.py\", \"lib/**/*.js\"]\nexclude_patterns = [\"**/*test*\", \"**/vendor/**\", \"**/node_modules/**\"]\n\n# Export settings for analysis\n[export]\ndefault_format = \"json\"\ncompression = false  # Faster processing\ninclude_metadata = true\n\n[export.json]\nindent = 0  # Compact output\nschema_type = \"flat\"  # Simple structure\n</code></pre>"},{"location":"configuration/#development-configuration","title":"Development Configuration","text":"<p>Configuration for local development:</p> <pre><code># dev.config.toml\n\n# Verbose output for debugging\nlog_level = \"DEBUG\"\nshow_progress = true\nverbose = true\n\n# Include everything for analysis\ninclude_tests = true\ninclude_docs = true\nmin_chunk_size = 0  # Show all chunks\n\n# Fast iteration\ncache_enabled = true\ncache_size = 500  # Large cache for development\n\n# Custom plugin development\nplugin_dirs = [\"./dev_plugins\", \"./experimental_plugins\"]\n\n[plugins.my_custom_plugin]\nenabled = true\ndebug_mode = true\n</code></pre>"},{"location":"configuration/#best-practices","title":"Best Practices","text":""},{"location":"configuration/#1-use-version-control","title":"1. Use Version Control","text":"<p>Always version control your configuration files:</p> <pre><code># .gitignore\n# Don't ignore project configuration\n!.chunkerrc\n!chunker.config.toml\n\n# But ignore personal configuration\n.chunkerrc.local\nchunker.config.local.toml\n</code></pre>"},{"location":"configuration/#2-environment-specific-configs","title":"2. Environment-Specific Configs","text":"<p>Use different configurations for different environments:</p> <pre><code>import os\nfrom chunker import ChunkerConfig\n\nenv = os.getenv(\"CHUNKER_ENV\", \"development\")\nconfig_file = f\"chunker.config.{env}.toml\"\nconfig = ChunkerConfig(config_file)\n</code></pre>"},{"location":"configuration/#3-validate-configuration","title":"3. Validate Configuration","text":"<p>Always validate configuration files:</p> <pre><code>from chunker import ChunkerConfig\n\ntry:\n    config = ChunkerConfig(\"chunker.config.toml\")\n    config.validate()\nexcept Exception as e:\n    print(f\"Invalid configuration: {e}\")\n</code></pre>"},{"location":"configuration/#4-document-custom-options","title":"4. Document Custom Options","text":"<p>Document all custom options in your configuration:</p> <pre><code># chunker.config.toml\n\n# Custom options for our Python analyzer\n[languages.python.custom_options]\n# Skip functions with \"deprecated\" in their name or docstring\nskip_deprecated = true\n\n# Minimum complexity score to include a function\nmin_complexity = 5\n\n# Extract type annotations as separate metadata\nextract_type_annotations = true\n</code></pre>"},{"location":"configuration/#5-use-hierarchical-configuration","title":"5. Use Hierarchical Configuration","text":"<p>Organize configuration hierarchically:</p> <pre><code># Base configuration\n[base]\nmin_chunk_size = 3\nmax_chunk_size = 200\n\n# Language-specific overrides\n[languages.python]\nmin_chunk_size = 5  # Python tends to have more boilerplate\n\n[languages.rust]\nmax_chunk_size = 150  # Rust functions are typically more concise\n</code></pre>"},{"location":"configuration/#6-secure-sensitive-information","title":"6. Secure Sensitive Information","text":"<p>Never store sensitive information in configuration files:</p> <pre><code># DON'T DO THIS\napi_key = \"sk-1234567890abcdef\"\n\n# DO THIS INSTEAD\napi_key_env = \"CHUNKER_API_KEY\"  # Read from environment\n</code></pre>"},{"location":"configuration/#7-performance-tuning","title":"7. Performance Tuning","text":"<p>Profile and tune configuration for performance:</p> <pre><code># Performance tuning\n[performance]\n# Adjust based on system resources\nparallel_workers = 8  # Number of CPU cores\ncache_size = 200     # Based on available memory\n\n# File size thresholds\nstreaming_threshold = 10485760  # 10MB - use streaming for larger files\nbatch_size = 100  # Process files in batches\n\n# Language-specific tuning\n[languages.python.performance]\nparser_timeout = 5000  # 5 seconds for complex files\n</code></pre>"},{"location":"configuration/#see-also","title":"See Also","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Plugin Development - Creating custom plugins</li> <li>CLI Reference - Command-line options</li> <li>User Guide - General usage guide</li> </ul>"},{"location":"cookbook/","title":"Tree-sitter Chunker Cookbook","text":"<p>This cookbook contains practical recipes for common use cases with Tree-sitter Chunker. Each recipe is self-contained and can be adapted to your specific needs.</p>"},{"location":"cookbook/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Code Search and Indexing</li> <li>Documentation Generation</li> <li>Code Quality Analysis</li> <li>AI/ML Integration</li> <li>Plugin Development Recipes</li> <li>Export Format Recipes</li> <li>Performance Optimization</li> <li>Configuration Recipes</li> <li>Language-Specific Recipes</li> <li>Build Tool Integration</li> <li>Advanced Patterns</li> </ol>"},{"location":"cookbook/#code-search-and-indexing","title":"Code Search and Indexing","text":""},{"location":"cookbook/#build-a-function-search-engine","title":"Build a Function Search Engine","text":"<p>Create a fast, searchable database of all functions in your codebase.</p> <pre><code>import sqlite3\nfrom pathlib import Path\nfrom chunker.chunker import chunk_file\nfrom chunker.parser import list_languages\nfrom chunker.exceptions import LanguageNotFoundError\nfrom datetime import datetime\nimport re\n\nclass CodeSearchEngine:\n    \"\"\"A SQLite-based searchable index of code chunks.\"\"\"\n\n    def __init__(self, db_path=\"code_index.db\"):\n        self.conn = sqlite3.connect(db_path)\n        self.conn.row_factory = sqlite3.Row\n        self._create_tables()\n\n    def _create_tables(self):\n        \"\"\"Create database schema.\"\"\"\n        self.conn.executescript('''\n            CREATE TABLE IF NOT EXISTS chunks (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                file_path TEXT NOT NULL,\n                language TEXT NOT NULL,\n                node_type TEXT NOT NULL,\n                name TEXT,\n                start_line INTEGER,\n                end_line INTEGER,\n                parent_context TEXT,\n                content TEXT,\n                signature TEXT,\n                indexed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            );\n\n            CREATE INDEX IF NOT EXISTS idx_name ON chunks(name);\n            CREATE INDEX IF NOT EXISTS idx_type ON chunks(node_type);\n            CREATE INDEX IF NOT EXISTS idx_file ON chunks(file_path);\n            CREATE INDEX IF NOT EXISTS idx_parent ON chunks(parent_context);\n        ''')\n        self.conn.commit()\n\n    def index_directory(self, directory):\n        \"\"\"Index all supported files in a directory.\"\"\"\n        path = Path(directory)\n        language_extensions = {\n            '.py': 'python',\n            '.js': 'javascript',\n            '.rs': 'rust',\n            '.c': 'c',\n            '.cpp': 'cpp',\n            '.cc': 'cpp',\n            '.h': 'c',\n            '.hpp': 'cpp'\n        }\n\n        indexed_count = 0\n        for ext, language in language_extensions.items():\n            for file_path in path.rglob(f\"*{ext}\"):\n                if self._should_skip(file_path):\n                    continue\n\n                try:\n                    chunks = chunk_file(str(file_path), language)\n                    for chunk in chunks:\n                        self._index_chunk(chunk, file_path)\n                        indexed_count += 1\n                except Exception as e:\n                    print(f\"Error indexing {file_path}: {e}\")\n\n        self.conn.commit()\n        print(f\"Indexed {indexed_count} code chunks\")\n        return indexed_count\n\n    def _should_skip(self, file_path):\n        \"\"\"Check if file should be skipped.\"\"\"\n        skip_dirs = {'__pycache__', 'node_modules', '.git', 'venv', '.venv'}\n        return any(part in skip_dirs for part in file_path.parts)\n\n    def _index_chunk(self, chunk, file_path):\n        \"\"\"Index a single chunk.\"\"\"\n        name = self._extract_name(chunk)\n        signature = chunk.content.split('\\n')[0].strip()\n\n        self.conn.execute('''\n            INSERT INTO chunks \n            (file_path, language, node_type, name, start_line, \n             end_line, parent_context, content, signature)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n        ''', (\n            str(file_path),\n            chunk.language,\n            chunk.node_type,\n            name,\n            chunk.start_line,\n            chunk.end_line,\n            chunk.parent_context,\n            chunk.content,\n            signature\n        ))\n\n    def _extract_name(self, chunk):\n        \"\"\"Extract function/class name from chunk.\"\"\"\n        patterns = {\n            'python': r'(?:def|class)\\s+(\\w+)',\n            'javascript': r'(?:function|class|const|let|var)\\s+(\\w+)',\n            'rust': r'(?:fn|struct|impl|trait)\\s+(\\w+)',\n            'c': r'(?:\\w+\\s+)?(\\w+)\\s*\\(',\n            'cpp': r'(?:class|struct|(?:\\w+\\s+)?(\\w+)\\s*\\()'\n        }\n\n        pattern = patterns.get(chunk.language, r'(\\w+)')\n        match = re.search(pattern, chunk.content)\n        return match.group(1) if match else None\n\n    def search(self, query, limit=20):\n        \"\"\"Search for chunks by name or content.\"\"\"\n        cursor = self.conn.execute('''\n            SELECT * FROM chunks\n            WHERE name LIKE ? OR content LIKE ?\n            ORDER BY \n                CASE \n                    WHEN name = ? THEN 0\n                    WHEN name LIKE ? THEN 1\n                    ELSE 2\n                END,\n                name\n            LIMIT ?\n        ''', (\n            f'%{query}%', f'%{query}%',\n            query, f'{query}%',\n            limit\n        ))\n\n        return [dict(row) for row in cursor]\n\n    def find_by_type(self, node_type):\n        \"\"\"Find all chunks of a specific type.\"\"\"\n        cursor = self.conn.execute(\n            'SELECT * FROM chunks WHERE node_type = ? ORDER BY file_path, start_line',\n            (node_type,)\n        )\n        return [dict(row) for row in cursor]\n\n    def find_in_file(self, file_path):\n        \"\"\"Find all chunks in a specific file.\"\"\"\n        cursor = self.conn.execute(\n            'SELECT * FROM chunks WHERE file_path = ? ORDER BY start_line',\n            (str(file_path),)\n        )\n        return [dict(row) for row in cursor]\n\n    def get_statistics(self):\n        \"\"\"Get index statistics.\"\"\"\n        stats = {}\n\n        # Total chunks\n        cursor = self.conn.execute('SELECT COUNT(*) as count FROM chunks')\n        stats['total_chunks'] = cursor.fetchone()['count']\n\n        # By language\n        cursor = self.conn.execute('''\n            SELECT language, COUNT(*) as count \n            FROM chunks \n            GROUP BY language\n        ''')\n        stats['by_language'] = {row['language']: row['count'] for row in cursor}\n\n        # By type\n        cursor = self.conn.execute('''\n            SELECT node_type, COUNT(*) as count \n            FROM chunks \n            GROUP BY node_type\n            ORDER BY count DESC\n        ''')\n        stats['by_type'] = {row['node_type']: row['count'] for row in cursor}\n\n        return stats\n\n# Usage example\nif __name__ == \"__main__\":\n    # Create search engine\n    engine = CodeSearchEngine()\n\n    # Index your project\n    engine.index_directory(\"./src\")\n\n    # Search for functions\n    results = engine.search(\"process\")\n    for result in results[:5]:\n        print(f\"{result['file_path']}:{result['start_line']} - {result['name']}\")\n        print(f\"  {result['signature']}\")\n\n    # Get statistics\n    stats = engine.get_statistics()\n    print(f\"\\nTotal chunks: {stats['total_chunks']}\")\n    print(\"By language:\", stats['by_language'])\n</code></pre>"},{"location":"cookbook/#create-a-symbol-navigator","title":"Create a Symbol Navigator","text":"<p>Build an interactive symbol navigator for your codebase.</p> <pre><code>from chunker.chunker import chunk_file\nfrom pathlib import Path\nfrom collections import defaultdict\nimport json\n\nclass SymbolNavigator:\n    \"\"\"Navigate code symbols hierarchically.\"\"\"\n\n    def __init__(self, root_dir):\n        self.root_dir = Path(root_dir)\n        self.symbol_tree = {}\n        self.build_tree()\n\n    def build_tree(self):\n        \"\"\"Build hierarchical symbol tree.\"\"\"\n        for py_file in self.root_dir.rglob(\"*.py\"):\n            if \"__pycache__\" in str(py_file):\n                continue\n\n            try:\n                chunks = chunk_file(str(py_file), \"python\")\n                rel_path = py_file.relative_to(self.root_dir)\n\n                file_symbols = {\n                    'path': str(rel_path),\n                    'classes': {},\n                    'functions': [],\n                    'total_lines': 0\n                }\n\n                # First pass: collect classes\n                for chunk in chunks:\n                    if chunk.node_type == \"class_definition\":\n                        class_name = self._extract_name(chunk)\n                        file_symbols['classes'][class_name] = {\n                            'line': chunk.start_line,\n                            'end_line': chunk.end_line,\n                            'methods': [],\n                            'docstring': self._extract_docstring(chunk)\n                        }\n\n                # Second pass: collect methods and functions\n                for chunk in chunks:\n                    if chunk.node_type == \"function_definition\":\n                        func_info = {\n                            'name': self._extract_name(chunk),\n                            'line': chunk.start_line,\n                            'end_line': chunk.end_line,\n                            'is_async': 'async def' in chunk.content.split('\\n')[0],\n                            'decorators': self._extract_decorators(chunk)\n                        }\n\n                        if chunk.parent_context:\n                            # It's a method\n                            class_name = chunk.parent_context.split(':')[1]\n                            if class_name in file_symbols['classes']:\n                                file_symbols['classes'][class_name]['methods'].append(func_info)\n                        else:\n                            # It's a standalone function\n                            file_symbols['functions'].append(func_info)\n\n                # Calculate total lines\n                if chunks:\n                    file_symbols['total_lines'] = max(c.end_line for c in chunks)\n\n                self.symbol_tree[str(rel_path)] = file_symbols\n\n            except Exception as e:\n                print(f\"Error processing {py_file}: {e}\")\n\n    def _extract_name(self, chunk):\n        \"\"\"Extract name from chunk.\"\"\"\n        import re\n        match = re.search(r'(?:def|class)\\s+(\\w+)', chunk.content)\n        return match.group(1) if match else \"unknown\"\n\n    def _extract_docstring(self, chunk):\n        \"\"\"Extract first line of docstring.\"\"\"\n        lines = chunk.content.split('\\n')\n        for i, line in enumerate(lines[1:3]):\n            if '\"\"\"' in line or \"'''\" in line:\n                return line.strip().strip('\"\"\"').strip(\"'''\")\n        return None\n\n    def _extract_decorators(self, chunk):\n        \"\"\"Extract decorator names.\"\"\"\n        decorators = []\n        lines = chunk.content.split('\\n')\n        for line in lines:\n            if line.strip().startswith('@'):\n                decorator = line.strip().lstrip('@').split('(')[0]\n                decorators.append(decorator)\n            elif line.strip().startswith('def '):\n                break\n        return decorators\n\n    def print_tree(self):\n        \"\"\"Print symbol tree in a nice format.\"\"\"\n        for file_path, symbols in sorted(self.symbol_tree.items()):\n            print(f\"\\n\ud83d\udcc4 {file_path} ({symbols['total_lines']} lines)\")\n\n            # Print classes\n            for class_name, class_info in symbols['classes'].items():\n                print(f\"  \ud83d\udce6 {class_name} (line {class_info['line']})\")\n                if class_info['docstring']:\n                    print(f\"     \ud83d\udcdd {class_info['docstring']}\")\n\n                for method in sorted(class_info['methods'], key=lambda x: x['line']):\n                    icon = \"\u26a1\" if method['is_async'] else \"\ud83d\udd27\"\n                    print(f\"    {icon} {method['name']} (line {method['line']})\")\n                    if method['decorators']:\n                        print(f\"       \ud83c\udff7\ufe0f  {', '.join(method['decorators'])}\")\n\n            # Print standalone functions\n            for func in sorted(symbols['functions'], key=lambda x: x['line']):\n                icon = \"\u26a1\" if func['is_async'] else \"\ud83d\udd27\"\n                print(f\"  {icon} {func['name']} (line {func['line']})\")\n                if func['decorators']:\n                    print(f\"     \ud83c\udff7\ufe0f  {', '.join(func['decorators'])}\")\n\n    def export_json(self, output_file):\n        \"\"\"Export symbol tree as JSON.\"\"\"\n        with open(output_file, 'w') as f:\n            json.dump(self.symbol_tree, f, indent=2)\n\n    def find_symbol(self, symbol_name):\n        \"\"\"Find all occurrences of a symbol.\"\"\"\n        results = []\n\n        for file_path, symbols in self.symbol_tree.items():\n            # Check classes\n            if symbol_name in symbols['classes']:\n                results.append({\n                    'file': file_path,\n                    'type': 'class',\n                    'line': symbols['classes'][symbol_name]['line']\n                })\n\n            # Check methods\n            for class_name, class_info in symbols['classes'].items():\n                for method in class_info['methods']:\n                    if method['name'] == symbol_name:\n                        results.append({\n                            'file': file_path,\n                            'type': 'method',\n                            'class': class_name,\n                            'line': method['line']\n                        })\n\n            # Check functions\n            for func in symbols['functions']:\n                if func['name'] == symbol_name:\n                    results.append({\n                        'file': file_path,\n                        'type': 'function',\n                        'line': func['line']\n                    })\n\n        return results\n\n# Usage\nnavigator = SymbolNavigator(\"./src\")\nnavigator.print_tree()\n\n# Find specific symbol\nresults = navigator.find_symbol(\"__init__\")\nprint(f\"\\nFound {len(results)} occurrences of '__init__':\")\nfor r in results:\n    print(f\"  {r['file']}:{r['line']} ({r['type']})\")\n</code></pre>"},{"location":"cookbook/#documentation-generation","title":"Documentation Generation","text":""},{"location":"cookbook/#generate-api-documentation-with-type-hints","title":"Generate API Documentation with Type Hints","text":"<p>Extract comprehensive API documentation including type hints and examples.</p> <pre><code>import ast\nfrom chunker.chunker import chunk_file\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport re\n\nclass APIDocGenerator:\n    \"\"\"Generate rich API documentation from Python code.\"\"\"\n\n    def __init__(self):\n        self.docs = []\n\n    def process_file(self, file_path: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"Process a Python file and extract API documentation.\"\"\"\n        chunks = chunk_file(file_path, \"python\")\n        file_docs = []\n\n        for chunk in chunks:\n            if chunk.node_type in [\"function_definition\", \"class_definition\"]:\n                try:\n                    doc = self._extract_documentation(chunk, file_path)\n                    if doc:\n                        file_docs.append(doc)\n                except Exception as e:\n                    print(f\"Error processing {chunk.node_type} at line {chunk.start_line}: {e}\")\n\n        return file_docs\n\n    def _extract_documentation(self, chunk, file_path):\n        \"\"\"Extract comprehensive documentation from a chunk.\"\"\"\n        try:\n            tree = ast.parse(chunk.content)\n            if not tree.body:\n                return None\n\n            node = tree.body[0]\n\n            doc = {\n                'name': node.name if hasattr(node, 'name') else 'unknown',\n                'type': chunk.node_type,\n                'file': file_path,\n                'line': chunk.start_line,\n                'end_line': chunk.end_line,\n                'parent': chunk.parent_context,\n                'docstring': ast.get_docstring(node),\n                'source': chunk.content\n            }\n\n            if isinstance(node, ast.FunctionDef) or isinstance(node, ast.AsyncFunctionDef):\n                doc.update(self._extract_function_details(node))\n            elif isinstance(node, ast.ClassDef):\n                doc.update(self._extract_class_details(node, chunk))\n\n            return doc\n\n        except Exception as e:\n            return None\n\n    def _extract_function_details(self, node):\n        \"\"\"Extract function-specific details.\"\"\"\n        details = {\n            'is_async': isinstance(node, ast.AsyncFunctionDef),\n            'decorators': [self._get_decorator_name(d) for d in node.decorator_list],\n            'parameters': self._extract_parameters(node),\n            'returns': self._extract_return_type(node),\n            'raises': self._extract_raises(node),\n            'yields': self._check_yields(node),\n            'examples': self._extract_examples(ast.get_docstring(node))\n        }\n        return details\n\n    def _extract_class_details(self, node, chunk):\n        \"\"\"Extract class-specific details.\"\"\"\n        details = {\n            'bases': [ast.unparse(base) for base in node.bases],\n            'decorators': [self._get_decorator_name(d) for d in node.decorator_list],\n            'methods': self._count_methods(node),\n            'class_variables': self._extract_class_variables(node),\n            'is_dataclass': any(self._get_decorator_name(d) == 'dataclass' \n                               for d in node.decorator_list)\n        }\n        return details\n\n    def _get_decorator_name(self, decorator):\n        \"\"\"Get decorator name as string.\"\"\"\n        if isinstance(decorator, ast.Name):\n            return decorator.id\n        elif isinstance(decorator, ast.Call) and isinstance(decorator.func, ast.Name):\n            return decorator.func.id\n        else:\n            return ast.unparse(decorator)\n\n    def _extract_parameters(self, func_node):\n        \"\"\"Extract function parameters with type hints.\"\"\"\n        params = []\n        args = func_node.args\n\n        # Regular arguments\n        for i, arg in enumerate(args.args):\n            param = {\n                'name': arg.arg,\n                'type': ast.unparse(arg.annotation) if arg.annotation else None,\n                'default': None\n            }\n\n            # Check for defaults\n            defaults_start = len(args.args) - len(args.defaults)\n            if i &gt;= defaults_start:\n                default_index = i - defaults_start\n                param['default'] = ast.unparse(args.defaults[default_index])\n\n            params.append(param)\n\n        # *args\n        if args.vararg:\n            params.append({\n                'name': f\"*{args.vararg.arg}\",\n                'type': ast.unparse(args.vararg.annotation) if args.vararg.annotation else None,\n                'default': None\n            })\n\n        # **kwargs\n        if args.kwarg:\n            params.append({\n                'name': f\"**{args.kwarg.arg}\",\n                'type': ast.unparse(args.kwarg.annotation) if args.kwarg.annotation else None,\n                'default': None\n            })\n\n        return params\n\n    def _extract_return_type(self, func_node):\n        \"\"\"Extract return type annotation.\"\"\"\n        if func_node.returns:\n            return ast.unparse(func_node.returns)\n        return None\n\n    def _extract_raises(self, node):\n        \"\"\"Extract exceptions that might be raised.\"\"\"\n        raises = []\n        for child in ast.walk(node):\n            if isinstance(child, ast.Raise):\n                if child.exc:\n                    if isinstance(child.exc, ast.Call) and isinstance(child.exc.func, ast.Name):\n                        raises.append(child.exc.func.id)\n                    elif isinstance(child.exc, ast.Name):\n                        raises.append(child.exc.id)\n        return list(set(raises))\n\n    def _check_yields(self, node):\n        \"\"\"Check if function is a generator.\"\"\"\n        for child in ast.walk(node):\n            if isinstance(child, ast.Yield) or isinstance(child, ast.YieldFrom):\n                return True\n        return False\n\n    def _extract_examples(self, docstring):\n        \"\"\"Extract code examples from docstring.\"\"\"\n        if not docstring:\n            return []\n\n        examples = []\n        in_example = False\n        current_example = []\n\n        for line in docstring.split('\\n'):\n            if '&gt;&gt;&gt;' in line:\n                in_example = True\n                current_example.append(line)\n            elif in_example and line.strip() and not line.startswith(' '):\n                # End of example\n                if current_example:\n                    examples.append('\\n'.join(current_example))\n                current_example = []\n                in_example = False\n            elif in_example:\n                current_example.append(line)\n\n        if current_example:\n            examples.append('\\n'.join(current_example))\n\n        return examples\n\n    def _count_methods(self, class_node):\n        \"\"\"Count methods in a class.\"\"\"\n        return sum(1 for node in class_node.body \n                  if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)))\n\n    def _extract_class_variables(self, class_node):\n        \"\"\"Extract class variables with type annotations.\"\"\"\n        variables = []\n        for node in class_node.body:\n            if isinstance(node, ast.AnnAssign) and isinstance(node.target, ast.Name):\n                variables.append({\n                    'name': node.target.id,\n                    'type': ast.unparse(node.annotation),\n                    'value': ast.unparse(node.value) if node.value else None\n                })\n        return variables\n\n    def generate_markdown(self, docs: List[Dict[str, Any]]) -&gt; str:\n        \"\"\"Generate comprehensive Markdown documentation.\"\"\"\n        md = [\"# API Documentation\\n\"]\n        md.append(\"*Generated with Tree-sitter Chunker*\\n\")\n\n        # Group by file\n        by_file = {}\n        for doc in docs:\n            by_file.setdefault(doc['file'], []).append(doc)\n\n        # Generate table of contents\n        md.append(\"## Table of Contents\\n\")\n        for file_path in sorted(by_file.keys()):\n            file_name = Path(file_path).name\n            md.append(f\"- [{file_name}](#{file_name.replace('.', '').lower()})\")\n        md.append(\"\")\n\n        # Generate documentation for each file\n        for file_path, file_docs in sorted(by_file.items()):\n            file_name = Path(file_path).name\n            md.append(f\"\\n## {file_name}\\n\")\n            md.append(f\"*{file_path}*\\n\")\n\n            # Sort by type (classes first) and line number\n            sorted_docs = sorted(file_docs, \n                               key=lambda x: (0 if x['type'] == 'class_definition' else 1, x['line']))\n\n            for doc in sorted_docs:\n                if doc['type'] == 'class_definition':\n                    md.extend(self._format_class_doc(doc))\n                else:\n                    md.extend(self._format_function_doc(doc))\n\n        return '\\n'.join(md)\n\n    def _format_class_doc(self, doc):\n        \"\"\"Format class documentation.\"\"\"\n        lines = [f\"\\n### class `{doc['name']}`\\n\"]\n\n        if doc.get('bases'):\n            lines.append(f\"*Inherits from: {', '.join(doc['bases'])}*\\n\")\n\n        if doc.get('decorators'):\n            lines.append(f\"**Decorators:** `{', '.join(doc['decorators'])}`\\n\")\n\n        if doc.get('docstring'):\n            lines.append(doc['docstring'] + '\\n')\n\n        if doc.get('class_variables'):\n            lines.append(\"**Class Variables:**\")\n            for var in doc['class_variables']:\n                lines.append(f\"- `{var['name']}: {var['type']}`\" + \n                           (f\" = `{var['value']}`\" if var['value'] else \"\"))\n            lines.append(\"\")\n\n        lines.append(f\"**Methods:** {doc.get('methods', 0)}\\n\")\n\n        return lines\n\n    def _format_function_doc(self, doc):\n        \"\"\"Format function documentation.\"\"\"\n        name = doc['name']\n        if doc.get('parent'):\n            name = f\"{doc['parent'].split(':')[1]}.{name}\"\n\n        lines = [f\"\\n### {'async ' if doc.get('is_async') else ''}function `{name}`\\n\"]\n\n        if doc.get('decorators'):\n            lines.append(f\"**Decorators:** `{', '.join(doc['decorators'])}`\\n\")\n\n        # Signature\n        if doc.get('parameters'):\n            params = []\n            for p in doc['parameters']:\n                param_str = p['name']\n                if p['type']:\n                    param_str += f\": {p['type']}\"\n                if p['default']:\n                    param_str += f\" = {p['default']}\"\n                params.append(param_str)\n\n            returns = f\" -&gt; {doc['returns']}\" if doc.get('returns') else \"\"\n            lines.append(f\"```python\\n{name}({', '.join(params)}){returns}\\n```\\n\")\n\n        if doc.get('docstring'):\n            lines.append(doc['docstring'] + '\\n')\n\n        if doc.get('raises'):\n            lines.append(f\"**Raises:** {', '.join(f'`{e}`' for e in doc['raises'])}\\n\")\n\n        if doc.get('yields'):\n            lines.append(\"**Yields:** This is a generator function\\n\")\n\n        if doc.get('examples'):\n            lines.append(\"**Examples:**\")\n            for example in doc['examples']:\n                lines.append(\"```python\")\n                lines.append(example)\n                lines.append(\"```\")\n            lines.append(\"\")\n\n        return lines\n\n# Usage\ngenerator = APIDocGenerator()\n\n# Process a file or directory\nall_docs = []\nfor py_file in Path(\"src\").rglob(\"*.py\"):\n    docs = generator.process_file(str(py_file))\n    all_docs.extend(docs)\n\n# Generate markdown\nmarkdown = generator.generate_markdown(all_docs)\n\n# Save documentation\nwith open(\"API_DOCUMENTATION.md\", \"w\") as f:\n    f.write(markdown)\n\nprint(f\"Generated documentation for {len(all_docs)} items\")\n</code></pre>"},{"location":"cookbook/#generate-readme-from-code-structure","title":"Generate README from Code Structure","text":"<p>Automatically generate project documentation from your codebase.</p> <pre><code>from chunker.chunker import chunk_file\nfrom chunker.parser import list_languages\nfrom pathlib import Path\nimport re\n\nclass ReadmeGenerator:\n    \"\"\"Generate README.md from code analysis.\"\"\"\n\n    def __init__(self, project_path):\n        self.project_path = Path(project_path)\n        self.project_name = self.project_path.name\n\n    def generate(self):\n        \"\"\"Generate complete README.\"\"\"\n        sections = []\n\n        # Title and description\n        sections.append(self._generate_header())\n\n        # Project structure\n        sections.append(self._generate_structure())\n\n        # API overview\n        sections.append(self._generate_api_overview())\n\n        # Installation\n        sections.append(self._generate_installation())\n\n        # Usage examples\n        sections.append(self._generate_usage_examples())\n\n        # Contributing\n        sections.append(self._generate_contributing())\n\n        return '\\n\\n'.join(filter(None, sections))\n\n    def _generate_header(self):\n        \"\"\"Generate header section.\"\"\"\n        header = [f\"# {self.project_name}\"]\n\n        # Try to extract description from main module\n        init_file = self.project_path / \"__init__.py\"\n        if init_file.exists():\n            with open(init_file) as f:\n                content = f.read()\n                if content.strip().startswith('\"\"\"'):\n                    docstring = content.split('\"\"\"')[1].strip()\n                    header.append(f\"\\n{docstring}\")\n\n        # Add badges (placeholder)\n        header.append(\"\\n![Python](https://img.shields.io/badge/python-3.8+-blue.svg)\")\n        header.append(\"![License](https://img.shields.io/badge/license-MIT-green.svg)\")\n\n        return '\\n'.join(header)\n\n    def _generate_structure(self):\n        \"\"\"Generate project structure overview.\"\"\"\n        lines = [\"## Project Structure\\n\"]\n\n        # Find main modules\n        modules = {}\n        for py_file in self.project_path.rglob(\"*.py\"):\n            if \"__pycache__\" in str(py_file):\n                continue\n\n            rel_path = py_file.relative_to(self.project_path)\n            if rel_path.parts[0] not in ['tests', 'docs', 'examples']:\n                module_path = str(rel_path.parent).replace('/', '.')\n                if module_path == '.':\n                    module_path = 'root'\n                modules.setdefault(module_path, []).append(py_file.stem)\n\n        lines.append(\"```\")\n        lines.append(f\"{self.project_name}/\")\n        for module, files in sorted(modules.items()):\n            if module != 'root':\n                lines.append(f\"\u251c\u2500\u2500 {module.replace('.', '/')}/\")\n                for file in sorted(files):\n                    if file != '__init__':\n                        lines.append(f\"\u2502   \u251c\u2500\u2500 {file}.py\")\n            else:\n                for file in sorted(files):\n                    if file != '__init__':\n                        lines.append(f\"\u251c\u2500\u2500 {file}.py\")\n        lines.append(\"```\")\n\n        return '\\n'.join(lines)\n\n    def _generate_api_overview(self):\n        \"\"\"Generate API overview from code analysis.\"\"\"\n        lines = [\"## API Overview\\n\"]\n\n        # Analyze main modules\n        api_items = []\n        for py_file in self.project_path.rglob(\"*.py\"):\n            if any(skip in str(py_file) for skip in ['__pycache__', 'tests', '_test']):\n                continue\n\n            try:\n                chunks = chunk_file(str(py_file), \"python\")\n                module_name = py_file.stem\n\n                # Find public classes and functions\n                for chunk in chunks:\n                    if chunk.node_type in [\"class_definition\", \"function_definition\"]:\n                        if not chunk.parent_context:  # Top-level only\n                            name = self._extract_name(chunk)\n                            if name and not name.startswith('_'):\n                                docstring = self._extract_first_line_docstring(chunk)\n                                api_items.append({\n                                    'module': module_name,\n                                    'name': name,\n                                    'type': 'class' if 'class' in chunk.node_type else 'function',\n                                    'docstring': docstring\n                                })\n            except:\n                continue\n\n        # Group by module\n        by_module = {}\n        for item in api_items:\n            by_module.setdefault(item['module'], []).append(item)\n\n        for module, items in sorted(by_module.items()):\n            if items:\n                lines.append(f\"### {module}\\n\")\n                for item in sorted(items, key=lambda x: (x['type'], x['name'])):\n                    icon = \"\ud83d\udd37\" if item['type'] == 'class' else \"\ud83d\udd38\"\n                    desc = f\" - {item['docstring']}\" if item['docstring'] else \"\"\n                    lines.append(f\"- {icon} **{item['name']}**{desc}\")\n                lines.append(\"\")\n\n        return '\\n'.join(lines)\n\n    def _generate_installation(self):\n        \"\"\"Generate installation instructions.\"\"\"\n        lines = [\"## Installation\\n\"]\n\n        # Check for setup.py or pyproject.toml\n        if (self.project_path / \"setup.py\").exists() or (self.project_path / \"pyproject.toml\").exists():\n            lines.append(\"```bash\")\n            lines.append(f\"pip install {self.project_name}\")\n            lines.append(\"```\")\n            lines.append(\"\\nFor development:\")\n            lines.append(\"```bash\")\n            lines.append(f\"git clone https://github.com/yourusername/{self.project_name}.git\")\n            lines.append(f\"cd {self.project_name}\")\n            lines.append(\"pip install -e .[dev]\")\n            lines.append(\"```\")\n        else:\n            lines.append(\"```bash\")\n            lines.append(f\"git clone https://github.com/yourusername/{self.project_name}.git\")\n            lines.append(f\"cd {self.project_name}\")\n            lines.append(\"pip install -r requirements.txt\")\n            lines.append(\"```\")\n\n        return '\\n'.join(lines)\n\n    def _generate_usage_examples(self):\n        \"\"\"Generate usage examples from code.\"\"\"\n        lines = [\"## Usage\\n\"]\n\n        # Look for main entry points\n        main_funcs = []\n        for py_file in self.project_path.rglob(\"*.py\"):\n            if \"__pycache__\" in str(py_file):\n                continue\n\n            try:\n                chunks = chunk_file(str(py_file), \"python\")\n                for chunk in chunks:\n                    if chunk.node_type == \"function_definition\":\n                        name = self._extract_name(chunk)\n                        if name in ['main', 'run', 'start'] or py_file.stem == '__main__':\n                            main_funcs.append({\n                                'file': py_file,\n                                'name': name,\n                                'chunk': chunk\n                            })\n            except:\n                continue\n\n        if main_funcs:\n            lines.append(\"### Basic Usage\\n\")\n            lines.append(\"```python\")\n            lines.append(f\"from {self.project_name} import ...\")\n            lines.append(\"```\")\n\n        # Look for example files\n        example_files = list(self.project_path.glob(\"examples/*.py\"))\n        if example_files:\n            lines.append(\"\\n### Examples\\n\")\n            for example in example_files[:3]:  # Show first 3\n                lines.append(f\"See `{example.relative_to(self.project_path)}` for a complete example.\")\n\n        return '\\n'.join(lines)\n\n    def _generate_contributing(self):\n        \"\"\"Generate contributing section.\"\"\"\n        lines = [\"## Contributing\\n\"]\n        lines.append(\"Contributions are welcome! Please feel free to submit a Pull Request.\")\n\n        # Check for tests\n        if (self.project_path / \"tests\").exists() or list(self.project_path.glob(\"test_*.py\")):\n            lines.append(\"\\nPlease make sure to update tests as appropriate:\")\n            lines.append(\"```bash\")\n            lines.append(\"pytest\")\n            lines.append(\"```\")\n\n        return '\\n'.join(lines)\n\n    def _extract_name(self, chunk):\n        \"\"\"Extract name from chunk.\"\"\"\n        match = re.search(r'(?:def|class)\\s+(\\w+)', chunk.content)\n        return match.group(1) if match else None\n\n    def _extract_first_line_docstring(self, chunk):\n        \"\"\"Extract first line of docstring.\"\"\"\n        lines = chunk.content.split('\\n')\n        for i, line in enumerate(lines[1:3]):\n            if '\"\"\"' in line or \"'''\" in line:\n                # Extract just the first sentence\n                docstring = line.strip().strip('\"\"\"').strip(\"'''\")\n                if '. ' in docstring:\n                    docstring = docstring.split('. ')[0] + '.'\n                return docstring\n        return None\n\n# Usage\ngenerator = ReadmeGenerator(\"./my_project\")\nreadme_content = generator.generate()\n\nwith open(\"README_generated.md\", \"w\") as f:\n    f.write(readme_content)\n\nprint(\"README.md generated successfully!\")\n</code></pre>"},{"location":"cookbook/#code-quality-analysis","title":"Code Quality Analysis","text":""},{"location":"cookbook/#complexity-analysis","title":"Complexity Analysis","text":"<p>Analyze code complexity using various metrics.</p> <pre><code>from chunker.chunker import chunk_file\nfrom chunker.parser import get_parser\nimport ast\nfrom dataclasses import dataclass\nfrom typing import List\nimport statistics\n\n@dataclass\nclass ComplexityReport:\n    \"\"\"Code complexity metrics for a function.\"\"\"\n    name: str\n    file: str\n    line: int\n    end_line: int\n    complexity: int  # Cyclomatic complexity\n    cognitive_complexity: int\n    lines_of_code: int\n    max_nesting: int\n    parameter_count: int\n    return_points: int\n\nclass ComplexityAnalyzer:\n    \"\"\"Comprehensive code complexity analyzer.\"\"\"\n\n    def analyze_file(self, file_path: str) -&gt; List[ComplexityReport]:\n        \"\"\"Analyze all functions in a file.\"\"\"\n        chunks = chunk_file(file_path, \"python\")\n        reports = []\n\n        for chunk in chunks:\n            if chunk.node_type == \"function_definition\":\n                report = self._analyze_function(chunk, file_path)\n                if report:\n                    reports.append(report)\n\n        return reports\n\n    def _analyze_function(self, chunk, file_path):\n        \"\"\"Analyze a single function.\"\"\"\n        try:\n            tree = ast.parse(chunk.content)\n            if not tree.body:\n                return None\n\n            func = tree.body[0]\n            if not isinstance(func, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                return None\n\n            return ComplexityReport(\n                name=func.name,\n                file=file_path,\n                line=chunk.start_line,\n                end_line=chunk.end_line,\n                complexity=self._cyclomatic_complexity(func),\n                cognitive_complexity=self._cognitive_complexity(func),\n                lines_of_code=chunk.end_line - chunk.start_line + 1,\n                max_nesting=self._max_nesting_depth(func),\n                parameter_count=len(func.args.args),\n                return_points=self._count_returns(func)\n            )\n        except Exception as e:\n            print(f\"Error analyzing function at line {chunk.start_line}: {e}\")\n            return None\n\n    def _cyclomatic_complexity(self, node):\n        \"\"\"Calculate McCabe cyclomatic complexity.\"\"\"\n        complexity = 1  # Base complexity\n\n        for child in ast.walk(node):\n            # Decision points\n            if isinstance(child, (ast.If, ast.While, ast.For)):\n                complexity += 1\n            elif isinstance(child, ast.ExceptHandler):\n                complexity += 1\n            # Boolean operators\n            elif isinstance(child, ast.BoolOp):\n                # Each 'and'/'or' adds a branch\n                complexity += len(child.values) - 1\n            # Comprehensions\n            elif isinstance(child, (ast.ListComp, ast.SetComp, ast.DictComp, ast.GeneratorExp)):\n                complexity += sum(1 for _ in child.generators)\n\n        return complexity\n\n    def _cognitive_complexity(self, node, nesting=0):\n        \"\"\"Calculate cognitive complexity (how hard to understand).\"\"\"\n        complexity = 0\n\n        for child in node.body:\n            if isinstance(child, ast.If):\n                # If statements increase complexity, more with nesting\n                complexity += 1 + nesting\n                # Check for else\n                if child.orelse:\n                    complexity += 1\n                # Recurse\n                complexity += self._cognitive_complexity(child, nesting + 1)\n\n            elif isinstance(child, (ast.For, ast.While)):\n                complexity += 1 + nesting\n                complexity += self._cognitive_complexity(child, nesting + 1)\n\n            elif isinstance(child, ast.Try):\n                complexity += 1 + nesting\n                for handler in child.handlers:\n                    complexity += 1\n                    complexity += self._cognitive_complexity(handler, nesting + 1)\n\n            elif isinstance(child, ast.With):\n                complexity += 1 + nesting\n                complexity += self._cognitive_complexity(child, nesting + 1)\n\n            elif hasattr(child, 'body'):\n                complexity += self._cognitive_complexity(child, nesting)\n\n        return complexity\n\n    def _max_nesting_depth(self, node, depth=0):\n        \"\"\"Calculate maximum nesting depth.\"\"\"\n        max_depth = depth\n\n        for child in ast.iter_child_nodes(node):\n            if isinstance(child, (ast.If, ast.For, ast.While, ast.With, ast.Try)):\n                if hasattr(child, 'body'):\n                    for nested in child.body:\n                        child_depth = self._max_nesting_depth(nested, depth + 1)\n                        max_depth = max(max_depth, child_depth)\n\n        return max_depth\n\n    def _count_returns(self, node):\n        \"\"\"Count number of return statements.\"\"\"\n        return sum(1 for child in ast.walk(node) \n                  if isinstance(child, (ast.Return, ast.Yield, ast.YieldFrom)))\n\n    def generate_report(self, reports: List[ComplexityReport]):\n        \"\"\"Generate a complexity report.\"\"\"\n        if not reports:\n            print(\"No functions analyzed.\")\n            return\n\n        print(\"Code Complexity Report\")\n        print(\"=\" * 100)\n        print(f\"{'Function':&lt;30} {'CC':&lt;5} {'Cog':&lt;5} {'LOC':&lt;5} {'Nest':&lt;5} {'Params':&lt;7} {'Returns':&lt;7} {'Status'}\")\n        print(\"-\" * 100)\n\n        # Sort by cyclomatic complexity\n        for r in sorted(reports, key=lambda x: x.complexity, reverse=True):\n            # Determine status\n            if r.complexity &gt; 10:\n                status = \"\ud83d\udd34 High\"\n            elif r.complexity &gt; 5:\n                status = \"\ud83d\udfe1 Medium\"\n            else:\n                status = \"\ud83d\udfe2 Low\"\n\n            print(f\"{r.name:&lt;30} {r.complexity:&lt;5} {r.cognitive_complexity:&lt;5} \"\n                  f\"{r.lines_of_code:&lt;5} {r.max_nesting:&lt;5} {r.parameter_count:&lt;7} \"\n                  f\"{r.return_points:&lt;7} {status}\")\n\n        # Summary statistics\n        print(\"\\nSummary Statistics:\")\n        print(f\"  Total functions: {len(reports)}\")\n        print(f\"  Average cyclomatic complexity: {statistics.mean(r.complexity for r in reports):.2f}\")\n        print(f\"  Average cognitive complexity: {statistics.mean(r.cognitive_complexity for r in reports):.2f}\")\n        print(f\"  High complexity functions (CC &gt; 10): {sum(1 for r in reports if r.complexity &gt; 10)}\")\n        print(f\"  Total lines of code: {sum(r.lines_of_code for r in reports)}\")\n\n    def find_complex_functions(self, reports: List[ComplexityReport], threshold=10):\n        \"\"\"Find functions exceeding complexity threshold.\"\"\"\n        complex_funcs = [r for r in reports if r.complexity &gt; threshold]\n\n        if complex_funcs:\n            print(f\"\\nFunctions exceeding complexity threshold ({threshold}):\")\n            for r in sorted(complex_funcs, key=lambda x: x.complexity, reverse=True):\n                print(f\"  {r.file}:{r.line} - {r.name} (CC: {r.complexity})\")\n                print(f\"    Suggestions:\")\n                if r.max_nesting &gt; 3:\n                    print(f\"    - Reduce nesting (current: {r.max_nesting} levels)\")\n                if r.parameter_count &gt; 5:\n                    print(f\"    - Consider using configuration object (current: {r.parameter_count} params)\")\n                if r.lines_of_code &gt; 50:\n                    print(f\"    - Split into smaller functions (current: {r.lines_of_code} lines)\")\n\n        return complex_funcs\n\n# Usage\nanalyzer = ComplexityAnalyzer()\n\n# Analyze a single file\nreports = analyzer.analyze_file(\"complex_module.py\")\nanalyzer.generate_report(reports)\n\n# Find overly complex functions\ncomplex_functions = analyzer.find_complex_functions(reports, threshold=10)\n\n# Analyze entire project\nall_reports = []\nfor py_file in Path(\"src\").rglob(\"*.py\"):\n    reports = analyzer.analyze_file(str(py_file))\n    all_reports.extend(reports)\n\nanalyzer.generate_report(all_reports)\n</code></pre>"},{"location":"cookbook/#code-duplication-detection","title":"Code Duplication Detection","text":"<p>Find duplicate or similar code blocks.</p> <pre><code>from chunker.chunker import chunk_file\nfrom pathlib import Path\nimport hashlib\nfrom difflib import SequenceMatcher\nfrom collections import defaultdict\nfrom typing import List, Tuple\n\nclass DuplicationDetector:\n    \"\"\"Detect duplicate and similar code blocks.\"\"\"\n\n    def __init__(self, similarity_threshold=0.85):\n        self.similarity_threshold = similarity_threshold\n        self.chunks_by_hash = defaultdict(list)\n        self.all_chunks = []\n\n    def analyze_directory(self, directory):\n        \"\"\"Analyze all Python files in directory.\"\"\"\n        for py_file in Path(directory).rglob(\"*.py\"):\n            if \"__pycache__\" in str(py_file):\n                continue\n\n            try:\n                chunks = chunk_file(str(py_file), \"python\")\n                for chunk in chunks:\n                    if chunk.node_type in [\"function_definition\", \"method_definition\"]:\n                        # Normalize content for comparison\n                        normalized = self._normalize_code(chunk.content)\n                        chunk_hash = hashlib.md5(normalized.encode()).hexdigest()\n\n                        self.chunks_by_hash[chunk_hash].append(chunk)\n                        self.all_chunks.append(chunk)\n            except Exception as e:\n                print(f\"Error processing {py_file}: {e}\")\n\n    def _normalize_code(self, code):\n        \"\"\"Normalize code for comparison (remove comments, normalize whitespace).\"\"\"\n        lines = []\n        for line in code.split('\\n'):\n            # Remove comments\n            if '#' in line:\n                line = line[:line.index('#')]\n            # Normalize whitespace\n            line = ' '.join(line.split())\n            if line:\n                lines.append(line)\n        return '\\n'.join(lines)\n\n    def find_exact_duplicates(self):\n        \"\"\"Find exact duplicate functions.\"\"\"\n        duplicates = []\n\n        for chunk_hash, chunks in self.chunks_by_hash.items():\n            if len(chunks) &gt; 1:\n                duplicates.append({\n                    'hash': chunk_hash,\n                    'chunks': chunks,\n                    'count': len(chunks)\n                })\n\n        return sorted(duplicates, key=lambda x: x['count'], reverse=True)\n\n    def find_similar_code(self):\n        \"\"\"Find similar (but not identical) code blocks.\"\"\"\n        similar_groups = []\n        checked_pairs = set()\n\n        for i, chunk1 in enumerate(self.all_chunks):\n            similar_chunks = []\n\n            for j, chunk2 in enumerate(self.all_chunks[i+1:], i+1):\n                pair_key = (i, j)\n                if pair_key in checked_pairs:\n                    continue\n\n                similarity = self._calculate_similarity(chunk1.content, chunk2.content)\n                if similarity &gt;= self.similarity_threshold:\n                    similar_chunks.append({\n                        'chunk': chunk2,\n                        'similarity': similarity\n                    })\n                    checked_pairs.add(pair_key)\n\n            if similar_chunks:\n                similar_groups.append({\n                    'original': chunk1,\n                    'similar': similar_chunks\n                })\n\n        return similar_groups\n\n    def _calculate_similarity(self, code1, code2):\n        \"\"\"Calculate similarity between two code blocks.\"\"\"\n        # Normalize for comparison\n        norm1 = self._normalize_code(code1)\n        norm2 = self._normalize_code(code2)\n\n        # Use sequence matcher for similarity\n        return SequenceMatcher(None, norm1, norm2).ratio()\n\n    def generate_report(self):\n        \"\"\"Generate duplication report.\"\"\"\n        exact_duplicates = self.find_exact_duplicates()\n        similar_code = self.find_similar_code()\n\n        print(\"Code Duplication Report\")\n        print(\"=\" * 80)\n\n        # Exact duplicates\n        if exact_duplicates:\n            print(f\"\\n\ud83d\udd34 Exact Duplicates Found: {len(exact_duplicates)} groups\")\n            print(\"-\" * 80)\n\n            for dup_group in exact_duplicates:\n                print(f\"\\nDuplicate group ({dup_group['count']} instances):\")\n                for chunk in dup_group['chunks']:\n                    print(f\"  - {chunk.file_path}:{chunk.start_line}-{chunk.end_line}\")\n                    name = chunk.content.split('\\n')[0].strip()\n                    print(f\"    {name}\")\n\n                # Show the duplicate code\n                print(\"\\n  Code:\")\n                preview = dup_group['chunks'][0].content.split('\\n')[:5]\n                for line in preview:\n                    print(f\"    {line}\")\n                if len(dup_group['chunks'][0].content.split('\\n')) &gt; 5:\n                    print(\"    ...\")\n        else:\n            print(\"\\n\u2705 No exact duplicates found\")\n\n        # Similar code\n        if similar_code:\n            print(f\"\\n\ud83d\udfe1 Similar Code Blocks: {len(similar_code)} groups\")\n            print(\"-\" * 80)\n\n            for group in similar_code[:10]:  # Show top 10\n                orig = group['original']\n                print(f\"\\nOriginal: {orig.file_path}:{orig.start_line}\")\n\n                for similar in group['similar']:\n                    chunk = similar['chunk']\n                    similarity = similar['similarity']\n                    print(f\"  Similar ({similarity:.0%}): {chunk.file_path}:{chunk.start_line}\")\n\n        # Summary\n        total_chunks = len(self.all_chunks)\n        duplicate_chunks = sum(len(g['chunks']) for g in exact_duplicates)\n\n        print(f\"\\n\ud83d\udcca Summary:\")\n        print(f\"  Total functions analyzed: {total_chunks}\")\n        print(f\"  Exact duplicate functions: {duplicate_chunks}\")\n        print(f\"  Duplication rate: {duplicate_chunks/total_chunks*100:.1f}%\")\n\n        # Calculate saved lines\n        saved_lines = 0\n        for dup_group in exact_duplicates:\n            if len(dup_group['chunks']) &gt; 1:\n                chunk = dup_group['chunks'][0]\n                lines = chunk.end_line - chunk.start_line + 1\n                saved_lines += lines * (len(dup_group['chunks']) - 1)\n\n        if saved_lines &gt; 0:\n            print(f\"  Potential lines saved by removing duplicates: {saved_lines}\")\n\n    def suggest_refactoring(self, duplicates):\n        \"\"\"Suggest refactoring for duplicate code.\"\"\"\n        suggestions = []\n\n        for dup_group in duplicates:\n            if len(dup_group['chunks']) &gt;= 3:\n                # Multiple duplicates - suggest extracting to shared module\n                suggestions.append({\n                    'type': 'extract_to_module',\n                    'chunks': dup_group['chunks'],\n                    'reason': f\"{len(dup_group['chunks'])} duplicate implementations found\"\n                })\n            elif len(dup_group['chunks']) == 2:\n                # Two duplicates - check if they're in related modules\n                chunk1, chunk2 = dup_group['chunks']\n                if Path(chunk1.file_path).parent == Path(chunk2.file_path).parent:\n                    suggestions.append({\n                        'type': 'extract_to_base',\n                        'chunks': dup_group['chunks'],\n                        'reason': \"Duplicates in same module - consider extracting\"\n                    })\n\n        return suggestions\n\n# Usage\ndetector = DuplicationDetector(similarity_threshold=0.85)\n\n# Analyze codebase\ndetector.analyze_directory(\"./src\")\n\n# Generate report\ndetector.generate_report()\n\n# Get suggestions\nduplicates = detector.find_exact_duplicates()\nsuggestions = detector.suggest_refactoring(duplicates)\n\nif suggestions:\n    print(\"\\n\ud83d\udca1 Refactoring Suggestions:\")\n    for suggestion in suggestions:\n        print(f\"\\n- {suggestion['type']}:\")\n        print(f\"  Reason: {suggestion['reason']}\")\n        print(\"  Affected files:\")\n        for chunk in suggestion['chunks']:\n            print(f\"    - {chunk.file_path}:{chunk.start_line}\")\n</code></pre>"},{"location":"cookbook/#plugin-development-recipes","title":"Plugin Development Recipes","text":""},{"location":"cookbook/#creating-a-custom-language-plugin","title":"Creating a Custom Language Plugin","text":"<p>Create a plugin for a language not yet supported.</p> <pre><code>from chunker.languages.plugin_base import LanguagePlugin\nfrom chunker import get_plugin_manager, chunk_file\nfrom typing import Set, Optional, Dict, Any\nfrom tree_sitter import Node\nimport re\n\nclass SwiftPlugin(LanguagePlugin):\n    \"\"\"Plugin for Swift language support.\"\"\"\n\n    def __init__(self, config=None):\n        super().__init__(config)\n        self.function_pattern = re.compile(r'func\\s+(\\w+)')\n        self.class_pattern = re.compile(r'class\\s+(\\w+)')\n        self.struct_pattern = re.compile(r'struct\\s+(\\w+)')\n\n    @property\n    def language_name(self) -&gt; str:\n        return \"swift\"\n\n    @property\n    def supported_extensions(self) -&gt; Set[str]:\n        return {\".swift\"}\n\n    @property\n    def default_chunk_types(self) -&gt; Set[str]:\n        return {\n            \"function_declaration\",\n            \"init_declaration\",\n            \"class_declaration\",\n            \"struct_declaration\",\n            \"enum_declaration\",\n            \"protocol_declaration\",\n            \"extension_declaration\"\n        }\n\n    def get_node_name(self, node: Node, source: bytes) -&gt; Optional[str]:\n        \"\"\"Extract the name from a Swift node.\"\"\"\n        content = source[node.start_byte:node.end_byte].decode('utf-8')\n        first_line = content.split('\\n')[0]\n\n        # Try different patterns\n        patterns = {\n            'function_declaration': self.function_pattern,\n            'class_declaration': self.class_pattern,\n            'struct_declaration': self.struct_pattern\n        }\n\n        pattern = patterns.get(node.type)\n        if pattern:\n            match = pattern.search(first_line)\n            if match:\n                return match.group(1)\n\n        # Fallback: extract identifier\n        for child in node.children:\n            if child.type == \"identifier\":\n                return source[child.start_byte:child.end_byte].decode('utf-8')\n\n        return None\n\n    def should_include_chunk(self, chunk) -&gt; bool:\n        \"\"\"Apply Swift-specific filtering.\"\"\"\n        # Skip test files if configured\n        if self.config and self.config.custom_options.get('skip_tests', False):\n            if 'test' in chunk.file_path.lower() or 'Test' in chunk.content:\n                return False\n\n        # Skip private functions if configured\n        if self.config and self.config.custom_options.get('skip_private', False):\n            if chunk.content.strip().startswith('private '):\n                return False\n\n        return super().should_include_chunk(chunk)\n\n    def process_node(self, node: Node, source: bytes, file_path: str, parent_context=None):\n        \"\"\"Process Swift nodes with special handling.\"\"\"\n        chunk = super().process_node(node, source, file_path, parent_context)\n\n        if chunk and node.type == \"computed_property\":\n            # Add metadata for computed properties\n            chunk.metadata = {\n                'property_type': 'computed',\n                'has_getter': 'get {' in chunk.content,\n                'has_setter': 'set {' in chunk.content\n            }\n\n        return chunk\n\n# Usage example\nclass SwiftProjectAnalyzer:\n    \"\"\"Analyze Swift projects using the custom plugin.\"\"\"\n\n    def __init__(self):\n        self.manager = get_plugin_manager()\n        self.manager.register_plugin(SwiftPlugin)\n\n    def analyze_swift_project(self, project_path: str):\n        \"\"\"Analyze a Swift project structure.\"\"\"\n        from pathlib import Path\n\n        swift_files = list(Path(project_path).rglob(\"*.swift\"))\n        print(f\"Found {len(swift_files)} Swift files\\n\")\n\n        stats = {\n            'total_chunks': 0,\n            'by_type': {},\n            'access_levels': {},\n            'protocols': [],\n            'extensions': []\n        }\n\n        for file_path in swift_files:\n            chunks = chunk_file(str(file_path), \"swift\")\n            stats['total_chunks'] += len(chunks)\n\n            for chunk in chunks:\n                # Count by type\n                stats['by_type'][chunk.node_type] = stats['by_type'].get(chunk.node_type, 0) + 1\n\n                # Analyze access levels\n                for level in ['public', 'internal', 'fileprivate', 'private']:\n                    if chunk.content.strip().startswith(f\"{level} \"):\n                        stats['access_levels'][level] = stats['access_levels'].get(level, 0) + 1\n\n                # Collect protocols and extensions\n                if chunk.node_type == \"protocol_declaration\":\n                    stats['protocols'].append(self._extract_name(chunk))\n                elif chunk.node_type == \"extension_declaration\":\n                    stats['extensions'].append(self._extract_name(chunk))\n\n        # Print analysis\n        print(\"Swift Project Analysis:\")\n        print(\"=\" * 50)\n        print(f\"Total code chunks: {stats['total_chunks']}\")\n        print(\"\\nChunk types:\")\n        for node_type, count in sorted(stats['by_type'].items(), key=lambda x: x[1], reverse=True):\n            print(f\"  {node_type}: {count}\")\n\n        print(\"\\nAccess levels:\")\n        for level, count in stats['access_levels'].items():\n            print(f\"  {level}: {count}\")\n\n        print(f\"\\nProtocols defined: {len(stats['protocols'])}\")\n        print(f\"Extensions: {len(stats['extensions'])}\")\n\n        return stats\n\n    def _extract_name(self, chunk):\n        \"\"\"Extract name from chunk content.\"\"\"\n        first_line = chunk.content.split('\\n')[0]\n        match = re.search(r'(?:protocol|extension)\\s+(\\w+)', first_line)\n        return match.group(1) if match else \"Unknown\"\n\n# Register and use the plugin\nanalyzer = SwiftProjectAnalyzer()\n\n# Analyze a Swift project\nif Path(\"MySwiftProject\").exists():\n    stats = analyzer.analyze_swift_project(\"MySwiftProject\")\n</code></pre>"},{"location":"cookbook/#plugin-with-custom-chunk-rules","title":"Plugin with Custom Chunk Rules","text":"<p>Create a plugin with advanced chunking rules.</p> <pre><code>from chunker.languages.plugin_base import LanguagePlugin\nfrom chunker.languages.base import ChunkRule\nfrom chunker import get_plugin_manager\nimport re\n\nclass AdvancedPythonPlugin(LanguagePlugin):\n    \"\"\"Enhanced Python plugin with custom chunk rules.\"\"\"\n\n    @property\n    def language_name(self) -&gt; str:\n        return \"python_advanced\"\n\n    @property\n    def supported_extensions(self) -&gt; Set[str]:\n        return {\".py\"}\n\n    @property\n    def default_chunk_types(self) -&gt; Set[str]:\n        # Standard Python chunk types\n        return {\n            \"function_definition\",\n            \"class_definition\",\n            \"async_function_definition\",\n            \"decorated_definition\"\n        }\n\n    def get_chunk_rules(self) -&gt; list[ChunkRule]:\n        \"\"\"Define custom chunking rules.\"\"\"\n        return [\n            # Extract FastAPI endpoints\n            ChunkRule(\n                name=\"fastapi_endpoint\",\n                node_type=\"decorated_definition\",\n                pattern=r'@(app|router)\\.(get|post|put|delete|patch)',\n                priority=100,\n                metadata_extractor=self._extract_endpoint_metadata\n            ),\n\n            # Extract Django views\n            ChunkRule(\n                name=\"django_view\",\n                node_type=\"class_definition\",\n                pattern=r'class\\s+\\w+\\s*\\(.*View.*\\)',\n                priority=90\n            ),\n\n            # Extract test classes and functions\n            ChunkRule(\n                name=\"test_case\",\n                node_type=\"function_definition\",\n                pattern=r'def\\s+test_\\w+',\n                priority=80,\n                parent_type=\"class_definition\"\n            ),\n\n            # Extract dataclasses\n            ChunkRule(\n                name=\"dataclass\",\n                node_type=\"decorated_definition\",\n                pattern=r'@dataclass',\n                priority=85\n            ),\n\n            # Extract Pydantic models\n            ChunkRule(\n                name=\"pydantic_model\",\n                node_type=\"class_definition\",\n                pattern=r'class\\s+\\w+\\s*\\(.*BaseModel.*\\)',\n                priority=85\n            )\n        ]\n\n    def _extract_endpoint_metadata(self, chunk) -&gt; dict:\n        \"\"\"Extract metadata from FastAPI endpoints.\"\"\"\n        content = chunk.content\n        metadata = {}\n\n        # Extract HTTP method and path\n        match = re.search(r'@(?:app|router)\\.(\\w+)\\([\"\\']([^\"\\']*)[\"\\'\\]', content)\n        if match:\n            metadata['http_method'] = match.group(1).upper()\n            metadata['path'] = match.group(2)\n\n        # Extract response model\n        response_match = re.search(r'response_model=([\\w\\.]+)', content)\n        if response_match:\n            metadata['response_model'] = response_match.group(1)\n\n        # Extract dependencies\n        deps = re.findall(r'Depends\\(([\\w\\.]+)\\)', content)\n        if deps:\n            metadata['dependencies'] = deps\n\n        return metadata\n\n    def get_node_name(self, node: Node, source: bytes) -&gt; Optional[str]:\n        \"\"\"Extract name with special handling for decorators.\"\"\"\n        content = source[node.start_byte:node.end_byte].decode('utf-8')\n\n        # For decorated functions, skip decorators\n        lines = content.split('\\n')\n        for line in lines:\n            if line.strip().startswith('def ') or line.strip().startswith('class '):\n                match = re.search(r'(?:def|class)\\s+(\\w+)', line)\n                if match:\n                    return match.group(1)\n\n        return None\n\n# Example: Analyze a FastAPI project\nclass FastAPIAnalyzer:\n    \"\"\"Analyze FastAPI projects with custom rules.\"\"\"\n\n    def __init__(self):\n        self.manager = get_plugin_manager()\n        self.manager.register_plugin(AdvancedPythonPlugin)\n\n    def analyze_api_structure(self, project_path: str):\n        \"\"\"Analyze FastAPI project structure.\"\"\"\n        from pathlib import Path\n        from chunker import chunk_file\n\n        endpoints = []\n        models = []\n\n        for py_file in Path(project_path).rglob(\"*.py\"):\n            chunks = chunk_file(str(py_file), \"python_advanced\")\n\n            for chunk in chunks:\n                if hasattr(chunk, 'metadata'):\n                    if 'http_method' in chunk.metadata:\n                        endpoints.append({\n                            'file': str(py_file),\n                            'name': chunk.content.split('\\n')[0],\n                            'method': chunk.metadata['http_method'],\n                            'path': chunk.metadata.get('path', 'Unknown'),\n                            'response_model': chunk.metadata.get('response_model'),\n                            'dependencies': chunk.metadata.get('dependencies', [])\n                        })\n                    elif chunk.node_type == \"pydantic_model\":\n                        models.append({\n                            'file': str(py_file),\n                            'name': self._extract_class_name(chunk),\n                            'fields': self._extract_model_fields(chunk)\n                        })\n\n        # Generate API documentation\n        print(\"FastAPI Project Structure:\")\n        print(\"=\" * 60)\n        print(f\"\\nEndpoints ({len(endpoints)}):\")\n        for ep in sorted(endpoints, key=lambda x: (x['path'], x['method'])):\n            print(f\"  {ep['method']:6} {ep['path']:30} -&gt; {Path(ep['file']).name}\")\n            if ep['response_model']:\n                print(f\"         Response: {ep['response_model']}\")\n\n        print(f\"\\nModels ({len(models)}):\")\n        for model in models:\n            print(f\"  {model['name']} ({len(model['fields'])} fields)\")\n\n        return {'endpoints': endpoints, 'models': models}\n\n    def _extract_class_name(self, chunk):\n        \"\"\"Extract class name from chunk.\"\"\"\n        match = re.search(r'class\\s+(\\w+)', chunk.content)\n        return match.group(1) if match else \"Unknown\"\n\n    def _extract_model_fields(self, chunk):\n        \"\"\"Extract Pydantic model fields.\"\"\"\n        fields = []\n        lines = chunk.content.split('\\n')\n        for line in lines[1:]:  # Skip class definition\n            if ':' in line and not line.strip().startswith('#'):\n                field_match = re.match(r'\\s*(\\w+)\\s*:\\s*([^=]+)', line)\n                if field_match:\n                    fields.append({\n                        'name': field_match.group(1),\n                        'type': field_match.group(2).strip()\n                    })\n        return fields\n\n# Usage\nanalyzer = FastAPIAnalyzer()\nif Path(\"fastapi_project\").exists():\n    api_structure = analyzer.analyze_api_structure(\"fastapi_project\")\n</code></pre>"},{"location":"cookbook/#export-format-recipes","title":"Export Format Recipes","text":""},{"location":"cookbook/#multi-format-export-pipeline","title":"Multi-Format Export Pipeline","text":"<p>Export chunks to multiple formats with transformations.</p> <pre><code>from chunker import chunk_file, chunk_directory_parallel\nfrom chunker.export import JSONExporter, JSONLExporter, SchemaType\nfrom chunker.exporters import ParquetExporter\nfrom pathlib import Path\nimport pandas as pd\nfrom typing import List, Dict, Any\nimport json\n\nclass MultiFormatExporter:\n    \"\"\"Export chunks to multiple formats with transformations.\"\"\"\n\n    def __init__(self):\n        self.exporters = {\n            'json': JSONExporter(schema_type=SchemaType.NESTED),\n            'jsonl': JSONLExporter(),\n            'parquet': ParquetExporter(compression=\"snappy\")\n        }\n\n    def export_with_enrichment(self, chunks: List, base_name: str):\n        \"\"\"Export chunks with additional metadata and transformations.\"\"\"\n        # Enrich chunks with additional metadata\n        enriched_chunks = self._enrich_chunks(chunks)\n\n        # Export to different formats\n        outputs = {}\n\n        # JSON with nested structure\n        json_path = f\"{base_name}_nested.json\"\n        self.exporters['json'].export(enriched_chunks, json_path)\n        outputs['json'] = json_path\n\n        # JSONL for streaming\n        jsonl_path = f\"{base_name}_stream.jsonl\"\n        self.exporters['jsonl'].export(enriched_chunks, jsonl_path)\n        outputs['jsonl'] = jsonl_path\n\n        # Parquet for analytics\n        parquet_path = f\"{base_name}_analytics.parquet\"\n        self.exporters['parquet'].export(enriched_chunks, parquet_path)\n        outputs['parquet'] = parquet_path\n\n        # Custom CSV export\n        csv_path = f\"{base_name}_summary.csv\"\n        self._export_to_csv(enriched_chunks, csv_path)\n        outputs['csv'] = csv_path\n\n        # Generate summary report\n        self._generate_summary_report(enriched_chunks, outputs)\n\n        return outputs\n\n    def _enrich_chunks(self, chunks: List) -&gt; List:\n        \"\"\"Add additional metadata to chunks.\"\"\"\n        import hashlib\n        from datetime import datetime\n\n        enriched = []\n        for i, chunk in enumerate(chunks):\n            # Create enriched chunk (preserve original)\n            enriched_chunk = chunk\n\n            # Add metadata\n            enriched_chunk.metadata = getattr(chunk, 'metadata', {})\n            enriched_chunk.metadata.update({\n                'index': i,\n                'hash': hashlib.md5(chunk.content.encode()).hexdigest(),\n                'size_bytes': len(chunk.content.encode('utf-8')),\n                'line_count': chunk.end_line - chunk.start_line + 1,\n                'complexity_estimate': self._estimate_complexity(chunk),\n                'extracted_at': datetime.now().isoformat(),\n                'has_docstring': self._has_docstring(chunk),\n                'dependencies': self._extract_dependencies(chunk)\n            })\n\n            enriched.append(enriched_chunk)\n\n        return enriched\n\n    def _estimate_complexity(self, chunk) -&gt; int:\n        \"\"\"Estimate chunk complexity.\"\"\"\n        complexity = 1\n        keywords = ['if ', 'for ', 'while ', 'try:', 'except:', 'elif ', 'else:']\n        for keyword in keywords:\n            complexity += chunk.content.count(keyword)\n        return complexity\n\n    def _has_docstring(self, chunk) -&gt; bool:\n        \"\"\"Check if chunk has a docstring.\"\"\"\n        return '\"\"\"' in chunk.content or \"'''\" in chunk.content\n\n    def _extract_dependencies(self, chunk) -&gt; List[str]:\n        \"\"\"Extract imports/dependencies from chunk.\"\"\"\n        dependencies = []\n        if chunk.language == \"python\":\n            import re\n            imports = re.findall(r'(?:from|import)\\s+([\\w\\.]+)', chunk.content)\n            dependencies.extend(imports)\n        return list(set(dependencies))\n\n    def _export_to_csv(self, chunks: List, output_path: str):\n        \"\"\"Export summary to CSV.\"\"\"\n        data = []\n        for chunk in chunks:\n            data.append({\n                'file': Path(chunk.file_path).name,\n                'type': chunk.node_type,\n                'name': chunk.content.split('\\n')[0][:50] + '...',\n                'start_line': chunk.start_line,\n                'end_line': chunk.end_line,\n                'lines': chunk.end_line - chunk.start_line + 1,\n                'complexity': chunk.metadata.get('complexity_estimate', 0),\n                'has_docstring': chunk.metadata.get('has_docstring', False)\n            })\n\n        df = pd.DataFrame(data)\n        df.to_csv(output_path, index=False)\n\n    def _generate_summary_report(self, chunks: List, outputs: Dict[str, str]):\n        \"\"\"Generate a summary report of the export.\"\"\"\n        report = {\n            'export_summary': {\n                'total_chunks': len(chunks),\n                'total_lines': sum(c.end_line - c.start_line + 1 for c in chunks),\n                'languages': list(set(c.language for c in chunks)),\n                'chunk_types': dict(pd.Series([c.node_type for c in chunks]).value_counts()),\n                'files_processed': len(set(c.file_path for c in chunks))\n            },\n            'complexity_stats': {\n                'avg_complexity': sum(c.metadata.get('complexity_estimate', 0) for c in chunks) / len(chunks),\n                'max_complexity': max(c.metadata.get('complexity_estimate', 0) for c in chunks),\n                'complex_chunks': sum(1 for c in chunks if c.metadata.get('complexity_estimate', 0) &gt; 10)\n            },\n            'output_files': outputs,\n            'file_sizes': {}\n        }\n\n        # Add file sizes\n        for format_name, path in outputs.items():\n            if Path(path).exists():\n                size_mb = Path(path).stat().st_size / (1024 * 1024)\n                report['file_sizes'][format_name] = f\"{size_mb:.2f} MB\"\n\n        # Save report\n        report_path = f\"{Path(outputs['json']).stem}_report.json\"\n        with open(report_path, 'w') as f:\n            json.dump(report, f, indent=2)\n\n        print(\"\\nExport Summary:\")\n        print(\"=\" * 50)\n        print(f\"Total chunks: {report['export_summary']['total_chunks']}\")\n        print(f\"Total lines: {report['export_summary']['total_lines']}\")\n        print(f\"Average complexity: {report['complexity_stats']['avg_complexity']:.1f}\")\n        print(\"\\nOutput files:\")\n        for format_name, path in outputs.items():\n            size = report['file_sizes'].get(format_name, 'Unknown')\n            print(f\"  {format_name}: {path} ({size})\")\n        print(f\"\\nReport saved to: {report_path}\")\n\n# Usage example\nclass ProjectExporter:\n    \"\"\"Export entire projects with multiple formats.\"\"\"\n\n    def __init__(self):\n        self.exporter = MultiFormatExporter()\n\n    def export_project(self, project_path: str, output_dir: str):\n        \"\"\"Export all code from a project.\"\"\"\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n\n        # Process project in parallel\n        results = chunk_directory_parallel(\n            project_path,\n            \"python\",  # Adjust for your project\n            pattern=\"**/*.py\",\n            max_workers=8\n        )\n\n        # Flatten results\n        all_chunks = []\n        for file_chunks in results.values():\n            all_chunks.extend(file_chunks)\n\n        # Export with enrichment\n        base_name = str(output_path / \"project_chunks\")\n        outputs = self.exporter.export_with_enrichment(all_chunks, base_name)\n\n        # Create partitioned Parquet for large datasets\n        if len(all_chunks) &gt; 1000:\n            self._create_partitioned_export(all_chunks, output_path)\n\n        return outputs\n\n    def _create_partitioned_export(self, chunks: List, output_dir: Path):\n        \"\"\"Create partitioned Parquet export for large datasets.\"\"\"\n        exporter = ParquetExporter()\n\n        # Export with partitioning by language and node type\n        partitioned_dir = output_dir / \"partitioned_chunks\"\n        exporter.export_partitioned(\n            chunks,\n            str(partitioned_dir),\n            partition_cols=[\"language\", \"node_type\"]\n        )\n\n        print(f\"\\nCreated partitioned export in: {partitioned_dir}\")\n        print(\"Partitions:\")\n        for partition in partitioned_dir.rglob(\"*.parquet\"):\n            rel_path = partition.relative_to(partitioned_dir)\n            print(f\"  {rel_path}\")\n\n# Usage\nexporter = ProjectExporter()\n\n# Export a project\nif Path(\"my_project\").exists():\n    outputs = exporter.export_project(\"my_project\", \"exports\")\n\n    # Read back the Parquet file for analysis\n    df = pd.read_parquet(outputs['parquet'])\n    print(f\"\\nDataFrame shape: {df.shape}\")\n    print(f\"Columns: {df.columns.tolist()}\")\n</code></pre>"},{"location":"cookbook/#custom-export-format-for-documentation","title":"Custom Export Format for Documentation","text":"<p>Create a custom Markdown export for documentation generation.</p> <pre><code>from chunker import chunk_file, CodeChunk\nfrom pathlib import Path\nfrom typing import List, Dict\nimport re\n\nclass MarkdownDocExporter:\n    \"\"\"Export chunks as structured Markdown documentation.\"\"\"\n\n    def __init__(self):\n        self.toc_entries = []\n\n    def export_to_markdown(self, chunks: List[CodeChunk], output_path: str, \n                          project_name: str = \"Project\"):\n        \"\"\"Export chunks as formatted Markdown documentation.\"\"\"\n        # Group chunks by file\n        chunks_by_file = self._group_by_file(chunks)\n\n        # Generate documentation\n        lines = [\n            f\"# {project_name} Code Documentation\\n\",\n            \"*Auto-generated documentation from source code*\\n\",\n            \"## Table of Contents\\n\"\n        ]\n\n        # Generate TOC\n        toc_lines = []\n        for file_path, file_chunks in sorted(chunks_by_file.items()):\n            file_name = Path(file_path).name\n            anchor = self._make_anchor(file_name)\n            toc_lines.append(f\"- [{file_name}](#{anchor})\")\n\n            # Add sub-items\n            for chunk in file_chunks:\n                if chunk.node_type in [\"class_definition\", \"function_definition\"]:\n                    name = self._extract_name(chunk)\n                    if name:\n                        sub_anchor = self._make_anchor(f\"{file_name}-{name}\")\n                        toc_lines.append(f\"  - [{name}](#{sub_anchor})\")\n\n        lines.extend(toc_lines)\n        lines.append(\"\\n---\\n\")\n\n        # Generate documentation for each file\n        for file_path, file_chunks in sorted(chunks_by_file.items()):\n            lines.extend(self._generate_file_doc(file_path, file_chunks))\n\n        # Add index\n        lines.extend(self._generate_index(chunks))\n\n        # Write to file\n        with open(output_path, 'w') as f:\n            f.write('\\n'.join(lines))\n\n        print(f\"Documentation exported to: {output_path}\")\n\n    def _group_by_file(self, chunks: List[CodeChunk]) -&gt; Dict[str, List[CodeChunk]]:\n        \"\"\"Group chunks by source file.\"\"\"\n        grouped = {}\n        for chunk in chunks:\n            if chunk.file_path not in grouped:\n                grouped[chunk.file_path] = []\n            grouped[chunk.file_path].append(chunk)\n        return grouped\n\n    def _generate_file_doc(self, file_path: str, chunks: List[CodeChunk]) -&gt; List[str]:\n        \"\"\"Generate documentation for a single file.\"\"\"\n        lines = []\n        file_name = Path(file_path).name\n\n        # File header\n        lines.append(f\"## {file_name}\")\n        lines.append(f\"*{file_path}*\\n\")\n\n        # File summary\n        summary = self._generate_file_summary(chunks)\n        lines.append(summary)\n        lines.append(\"\")\n\n        # Document each chunk\n        for chunk in chunks:\n            if chunk.node_type in [\"class_definition\", \"function_definition\", \n                                  \"method_definition\"]:\n                lines.extend(self._document_chunk(chunk, file_name))\n\n        lines.append(\"---\\n\")\n        return lines\n\n    def _generate_file_summary(self, chunks: List[CodeChunk]) -&gt; str:\n        \"\"\"Generate a summary of the file contents.\"\"\"\n        counts = {}\n        for chunk in chunks:\n            counts[chunk.node_type] = counts.get(chunk.node_type, 0) + 1\n\n        summary_parts = []\n        if counts.get('class_definition', 0) &gt; 0:\n            summary_parts.append(f\"{counts['class_definition']} classes\")\n        if counts.get('function_definition', 0) &gt; 0:\n            summary_parts.append(f\"{counts['function_definition']} functions\")\n\n        return f\"This file contains {', '.join(summary_parts)}.\" if summary_parts else \"This file contains code chunks.\"\n\n    def _document_chunk(self, chunk: CodeChunk, file_name: str) -&gt; List[str]:\n        \"\"\"Document a single code chunk.\"\"\"\n        lines = []\n        name = self._extract_name(chunk)\n\n        if not name:\n            return lines\n\n        # Section header\n        level = \"###\" if chunk.parent_context else \"###\"\n        anchor = self._make_anchor(f\"{file_name}-{name}\")\n        lines.append(f\"{level} {name}\")\n\n        # Metadata\n        metadata = []\n        if chunk.parent_context:\n            metadata.append(f\"*Parent: {chunk.parent_context}*\")\n        metadata.append(f\"*Lines {chunk.start_line}-{chunk.end_line}*\")\n        if metadata:\n            lines.append(\" | \".join(metadata))\n        lines.append(\"\")\n\n        # Extract docstring\n        docstring = self._extract_docstring(chunk)\n        if docstring:\n            lines.append(f\"&gt; {docstring}\")\n            lines.append(\"\")\n\n        # Function signature\n        signature = self._extract_signature(chunk)\n        if signature:\n            lines.append(\"```\" + chunk.language)\n            lines.append(signature)\n            lines.append(\"```\")\n            lines.append(\"\")\n\n        # Extract parameters (for functions)\n        if chunk.node_type in [\"function_definition\", \"method_definition\"]:\n            params = self._extract_parameters(chunk)\n            if params:\n                lines.append(\"**Parameters:**\")\n                for param in params:\n                    lines.append(f\"- `{param['name']}`: {param.get('type', 'Any')}\")\n                lines.append(\"\")\n\n        # Add any additional metadata\n        if hasattr(chunk, 'metadata') and chunk.metadata:\n            lines.append(\"**Metadata:**\")\n            for key, value in chunk.metadata.items():\n                lines.append(f\"- {key}: {value}\")\n            lines.append(\"\")\n\n        return lines\n\n    def _generate_index(self, chunks: List[CodeChunk]) -&gt; List[str]:\n        \"\"\"Generate an alphabetical index.\"\"\"\n        lines = [\"\\n## Index\\n\"]\n\n        # Collect all named chunks\n        index_entries = []\n        for chunk in chunks:\n            name = self._extract_name(chunk)\n            if name:\n                index_entries.append({\n                    'name': name,\n                    'type': chunk.node_type,\n                    'file': Path(chunk.file_path).name,\n                    'line': chunk.start_line\n                })\n\n        # Sort alphabetically\n        index_entries.sort(key=lambda x: x['name'].lower())\n\n        # Group by first letter\n        current_letter = None\n        for entry in index_entries:\n            first_letter = entry['name'][0].upper()\n            if first_letter != current_letter:\n                current_letter = first_letter\n                lines.append(f\"\\n### {current_letter}\")\n\n            lines.append(f\"- **{entry['name']}** ({entry['type']}) - \"\n                        f\"{entry['file']}:{entry['line']}\")\n\n        return lines\n\n    def _extract_name(self, chunk: CodeChunk) -&gt; str:\n        \"\"\"Extract the name of a function or class.\"\"\"\n        first_line = chunk.content.split('\\n')[0]\n\n        patterns = {\n            'python': r'(?:def|class)\\s+(\\w+)',\n            'javascript': r'(?:function|class|const|let|var)\\s+(\\w+)',\n            'rust': r'(?:fn|struct|impl|trait)\\s+(\\w+)'\n        }\n\n        pattern = patterns.get(chunk.language, r'(\\w+)')\n        match = re.search(pattern, first_line)\n\n        return match.group(1) if match else \"\"\n\n    def _extract_docstring(self, chunk: CodeChunk) -&gt; str:\n        \"\"\"Extract docstring from chunk.\"\"\"\n        lines = chunk.content.split('\\n')\n\n        for i, line in enumerate(lines[1:], 1):\n            if '\"\"\"' in line or \"'''\" in line:\n                # Simple single-line docstring\n                if line.count('\"\"\"') == 2 or line.count(\"'''\") == 2:\n                    return line.strip().strip('\"\"\"').strip(\"'''\")\n                # Multi-line docstring\n                quote = '\"\"\"' if '\"\"\"' in line else \"'''\"\n                docstring_lines = [line.replace(quote, '').strip()]\n                for j in range(i + 1, len(lines)):\n                    if quote in lines[j]:\n                        docstring_lines.append(lines[j].replace(quote, '').strip())\n                        break\n                    docstring_lines.append(lines[j].strip())\n                return ' '.join(docstring_lines)\n\n        return \"\"\n\n    def _extract_signature(self, chunk: CodeChunk) -&gt; str:\n        \"\"\"Extract function/class signature.\"\"\"\n        lines = chunk.content.split('\\n')\n        signature_lines = []\n\n        for line in lines:\n            signature_lines.append(line)\n            if ':' in line and not line.strip().endswith(','):\n                break\n\n        return '\\n'.join(signature_lines)\n\n    def _extract_parameters(self, chunk: CodeChunk) -&gt; List[Dict[str, str]]:\n        \"\"\"Extract function parameters.\"\"\"\n        params = []\n\n        if chunk.language == \"python\":\n            # Extract from function signature\n            match = re.search(r'def\\s+\\w+\\s*\\(([^)]*)\\)', chunk.content)\n            if match:\n                param_str = match.group(1)\n                for param in param_str.split(','):\n                    param = param.strip()\n                    if param and param != 'self':\n                        if ':' in param:\n                            name, type_hint = param.split(':', 1)\n                            params.append({'name': name.strip(), 'type': type_hint.strip()})\n                        else:\n                            params.append({'name': param, 'type': 'Any'})\n\n        return params\n\n    def _make_anchor(self, text: str) -&gt; str:\n        \"\"\"Create a valid markdown anchor from text.\"\"\"\n        return re.sub(r'[^a-zA-Z0-9-]', '-', text.lower())\n\n# Usage\nexporter = MarkdownDocExporter()\n\n# Export documentation for a project\nfrom chunker import chunk_directory_parallel\n\nresults = chunk_directory_parallel(\"src/\", \"python\", pattern=\"**/*.py\")\nall_chunks = []\nfor chunks in results.values():\n    all_chunks.extend(chunks)\n\nexporter.export_to_markdown(\n    all_chunks,\n    \"project_documentation.md\",\n    project_name=\"My Python Project\"\n)\n</code></pre>"},{"location":"cookbook/#aiml-integration","title":"AI/ML Integration","text":""},{"location":"cookbook/#generate-code-embeddings-for-semantic-search","title":"Generate Code Embeddings for Semantic Search","text":"<p>Create vector embeddings for semantic code search.</p> <pre><code>from chunker.chunker import chunk_file\nfrom chunker.parser import list_languages\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport pickle\nfrom pathlib import Path\nfrom typing import List, Dict\nimport json\n\nclass CodeEmbeddingSystem:\n    \"\"\"Semantic code search using embeddings.\"\"\"\n\n    def __init__(self, model_name='microsoft/codebert-base'):\n        # Use a code-specific model if available\n        try:\n            self.model = SentenceTransformer(model_name)\n        except:\n            # Fallback to general model\n            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n\n        self.chunks = []\n        self.embeddings = None\n        self.index = None\n\n    def index_codebase(self, directory):\n        \"\"\"Index all code in directory with embeddings.\"\"\"\n        print(\"Indexing codebase...\")\n\n        # Collect all chunks\n        for lang in ['python', 'javascript', 'rust']:\n            ext_map = {'python': '.py', 'javascript': '.js', 'rust': '.rs'}\n            pattern = f\"*{ext_map[lang]}\"\n\n            for file_path in Path(directory).rglob(pattern):\n                if self._should_skip(file_path):\n                    continue\n\n                try:\n                    chunks = chunk_file(str(file_path), lang)\n                    self.chunks.extend(chunks)\n                except Exception as e:\n                    print(f\"Error processing {file_path}: {e}\")\n\n        print(f\"Found {len(self.chunks)} code chunks\")\n\n        # Generate embeddings\n        print(\"Generating embeddings...\")\n        texts = [self._create_embedding_text(chunk) for chunk in self.chunks]\n        self.embeddings = self.model.encode(texts, show_progress_bar=True)\n\n        # Create FAISS index\n        dimension = self.embeddings.shape[1]\n        self.index = faiss.IndexFlatL2(dimension)\n        self.index.add(self.embeddings.astype('float32'))\n\n        print(f\"Indexed {len(self.chunks)} chunks with {dimension}-dimensional embeddings\")\n\n    def _should_skip(self, file_path):\n        \"\"\"Check if file should be skipped.\"\"\"\n        skip_patterns = ['__pycache__', 'node_modules', '.git', 'venv', 'test_']\n        return any(pattern in str(file_path) for pattern in skip_patterns)\n\n    def _create_embedding_text(self, chunk):\n        \"\"\"Create text representation for embedding.\"\"\"\n        # Combine context with code\n        parts = []\n\n        # Add metadata\n        parts.append(f\"Language: {chunk.language}\")\n        parts.append(f\"Type: {chunk.node_type}\")\n\n        if chunk.parent_context:\n            parts.append(f\"Context: {chunk.parent_context}\")\n\n        # Add function/class name\n        first_line = chunk.content.split('\\n')[0]\n        parts.append(f\"Signature: {first_line}\")\n\n        # Add docstring if present\n        docstring = self._extract_docstring(chunk.content)\n        if docstring:\n            parts.append(f\"Description: {docstring}\")\n\n        # Add simplified code\n        code_summary = self._summarize_code(chunk.content)\n        parts.append(f\"Code: {code_summary}\")\n\n        return '\\n'.join(parts)\n\n    def _extract_docstring(self, code):\n        \"\"\"Extract docstring from code.\"\"\"\n        lines = code.split('\\n')\n        for i, line in enumerate(lines[1:4]):\n            if '\"\"\"' in line or \"'''\" in line:\n                return line.strip().strip('\"\"\"').strip(\"'''\")\n        return None\n\n    def _summarize_code(self, code):\n        \"\"\"Create a summary of the code for embedding.\"\"\"\n        lines = code.split('\\n')\n\n        # Keep important lines (function calls, returns, etc.)\n        important_lines = []\n        keywords = ['return', 'yield', 'raise', 'import', 'from', 'class', 'def']\n\n        for line in lines[:20]:  # Limit to first 20 lines\n            line = line.strip()\n            if any(keyword in line for keyword in keywords):\n                important_lines.append(line)\n\n        return ' '.join(important_lines[:10])  # Limit summary length\n\n    def search(self, query: str, top_k: int = 10) -&gt; List[Dict]:\n        \"\"\"Search for code similar to query.\"\"\"\n        if self.index is None:\n            raise ValueError(\"Index not built. Call index_codebase() first.\")\n\n        # Encode query\n        query_embedding = self.model.encode([query])\n\n        # Search\n        distances, indices = self.index.search(query_embedding.astype('float32'), top_k)\n\n        # Build results\n        results = []\n        for idx, distance in zip(indices[0], distances[0]):\n            if idx &lt; len(self.chunks):\n                chunk = self.chunks[idx]\n                results.append({\n                    'chunk': chunk,\n                    'distance': float(distance),\n                    'similarity': 1 / (1 + float(distance)),  # Convert distance to similarity\n                    'preview': chunk.content.split('\\n')[0]\n                })\n\n        return results\n\n    def find_similar_functions(self, chunk_index: int, top_k: int = 5):\n        \"\"\"Find functions similar to a given chunk.\"\"\"\n        if chunk_index &gt;= len(self.embeddings):\n            raise ValueError(\"Invalid chunk index\")\n\n        # Get embedding for the chunk\n        query_embedding = self.embeddings[chunk_index].reshape(1, -1)\n\n        # Search (excluding the query itself)\n        distances, indices = self.index.search(query_embedding.astype('float32'), top_k + 1)\n\n        # Build results (skip first result which is the query itself)\n        results = []\n        for idx, distance in zip(indices[0][1:], distances[0][1:]):\n            if idx &lt; len(self.chunks):\n                chunk = self.chunks[idx]\n                results.append({\n                    'chunk': chunk,\n                    'distance': float(distance),\n                    'similarity': 1 / (1 + float(distance))\n                })\n\n        return results\n\n    def save(self, path: str):\n        \"\"\"Save the embedding system.\"\"\"\n        save_path = Path(path)\n        save_path.mkdir(exist_ok=True)\n\n        # Save chunks\n        with open(save_path / \"chunks.pkl\", \"wb\") as f:\n            pickle.dump(self.chunks, f)\n\n        # Save embeddings\n        np.save(save_path / \"embeddings.npy\", self.embeddings)\n\n        # Save FAISS index\n        faiss.write_index(self.index, str(save_path / \"index.faiss\"))\n\n        # Save metadata\n        metadata = {\n            'num_chunks': len(self.chunks),\n            'embedding_dim': self.embeddings.shape[1],\n            'model_name': self.model._modules['0'].auto_model.name_or_path\n        }\n        with open(save_path / \"metadata.json\", \"w\") as f:\n            json.dump(metadata, f)\n\n    def load(self, path: str):\n        \"\"\"Load saved embedding system.\"\"\"\n        load_path = Path(path)\n\n        # Load chunks\n        with open(load_path / \"chunks.pkl\", \"rb\") as f:\n            self.chunks = pickle.load(f)\n\n        # Load embeddings\n        self.embeddings = np.load(load_path / \"embeddings.npy\")\n\n        # Load FAISS index\n        self.index = faiss.read_index(str(load_path / \"index.faiss\"))\n\n        print(f\"Loaded {len(self.chunks)} chunks\")\n\n# Usage example\nembedding_system = CodeEmbeddingSystem()\n\n# Index your codebase\nembedding_system.index_codebase(\"./src\")\n\n# Search for code\nresults = embedding_system.search(\"function that handles user authentication\")\n\nprint(\"Search Results:\")\nfor i, result in enumerate(results[:5]):\n    chunk = result['chunk']\n    print(f\"\\n{i+1}. {chunk.file_path}:{chunk.start_line}\")\n    print(f\"   Similarity: {result['similarity']:.3f}\")\n    print(f\"   Type: {chunk.node_type}\")\n    print(f\"   Preview: {result['preview']}\")\n\n# Find similar functions\nchunk_idx = 42  # Example chunk\nsimilar = embedding_system.find_similar_functions(chunk_idx, top_k=3)\nprint(f\"\\nFunctions similar to chunk {chunk_idx}:\")\nfor result in similar:\n    chunk = result['chunk']\n    print(f\"  - {chunk.file_path}:{chunk.start_line} (similarity: {result['similarity']:.3f})\")\n\n# Save for later use\nembedding_system.save(\"./code_embeddings\")\n</code></pre>"},{"location":"cookbook/#generate-training-data-for-code-models","title":"Generate Training Data for Code Models","text":"<p>Create datasets for training code understanding models.</p> <pre><code>from chunker.chunker import chunk_file\nfrom pathlib import Path\nimport json\nimport random\nfrom typing import List, Dict, Tuple\nimport re\n\nclass CodeDatasetGenerator:\n    \"\"\"Generate training datasets from code.\"\"\"\n\n    def __init__(self):\n        self.data = []\n\n    def create_function_naming_dataset(self, directory):\n        \"\"\"Create dataset for function name prediction.\"\"\"\n        dataset = []\n\n        for py_file in Path(directory).rglob(\"*.py\"):\n            if \"__pycache__\" in str(py_file):\n                continue\n\n            try:\n                chunks = chunk_file(str(py_file), \"python\")\n\n                for chunk in chunks:\n                    if chunk.node_type == \"function_definition\":\n                        # Extract function name\n                        name = self._extract_function_name(chunk.content)\n                        if not name or name.startswith('_'):\n                            continue\n\n                        # Remove function name from code\n                        anonymized_code = self._anonymize_function(chunk.content, name)\n\n                        dataset.append({\n                            'input': anonymized_code,\n                            'target': name,\n                            'metadata': {\n                                'file': str(py_file),\n                                'line': chunk.start_line,\n                                'parent': chunk.parent_context\n                            }\n                        })\n            except Exception as e:\n                print(f\"Error processing {py_file}: {e}\")\n\n        return dataset\n\n    def create_docstring_generation_dataset(self, directory):\n        \"\"\"Create dataset for docstring generation.\"\"\"\n        dataset = []\n\n        for py_file in Path(directory).rglob(\"*.py\"):\n            if \"__pycache__\" in str(py_file):\n                continue\n\n            try:\n                chunks = chunk_file(str(py_file), \"python\")\n\n                for chunk in chunks:\n                    if chunk.node_type in [\"function_definition\", \"class_definition\"]:\n                        docstring = self._extract_docstring(chunk.content)\n                        if not docstring:\n                            continue\n\n                        # Remove docstring from code\n                        code_without_docstring = self._remove_docstring(chunk.content)\n\n                        dataset.append({\n                            'input': code_without_docstring,\n                            'target': docstring,\n                            'metadata': {\n                                'file': str(py_file),\n                                'line': chunk.start_line,\n                                'type': chunk.node_type\n                            }\n                        })\n            except Exception as e:\n                print(f\"Error processing {py_file}: {e}\")\n\n        return dataset\n\n    def create_code_completion_dataset(self, directory, context_lines=10):\n        \"\"\"Create dataset for code completion.\"\"\"\n        dataset = []\n\n        for py_file in Path(directory).rglob(\"*.py\"):\n            if \"__pycache__\" in str(py_file):\n                continue\n\n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    lines = f.readlines()\n\n                chunks = chunk_file(str(py_file), \"python\")\n\n                for chunk in chunks:\n                    if chunk.node_type == \"function_definition\":\n                        # Create multiple completion examples from the function\n                        func_lines = chunk.content.split('\\n')\n\n                        for i in range(2, len(func_lines) - 1):\n                            # Get context (previous lines)\n                            start_idx = max(0, i - context_lines)\n                            context = '\\n'.join(func_lines[start_idx:i])\n\n                            # Get completion target (current line)\n                            target = func_lines[i].strip()\n\n                            if target and not target.startswith('#'):\n                                dataset.append({\n                                    'input': context,\n                                    'target': target,\n                                    'metadata': {\n                                        'file': str(py_file),\n                                        'function_line': chunk.start_line,\n                                        'completion_line': chunk.start_line + i\n                                    }\n                                })\n            except Exception as e:\n                print(f\"Error processing {py_file}: {e}\")\n\n        return dataset\n\n    def create_bug_detection_dataset(self, directory):\n        \"\"\"Create synthetic bug detection dataset.\"\"\"\n        dataset = []\n\n        for py_file in Path(directory).rglob(\"*.py\"):\n            if \"__pycache__\" in str(py_file):\n                continue\n\n            try:\n                chunks = chunk_file(str(py_file), \"python\")\n\n                for chunk in chunks:\n                    if chunk.node_type == \"function_definition\":\n                        # Original (correct) code\n                        dataset.append({\n                            'code': chunk.content,\n                            'has_bug': False,\n                            'bug_type': None,\n                            'metadata': {\n                                'file': str(py_file),\n                                'line': chunk.start_line\n                            }\n                        })\n\n                        # Create buggy versions\n                        buggy_versions = self._introduce_bugs(chunk.content)\n                        for bug_type, buggy_code in buggy_versions:\n                            if buggy_code != chunk.content:\n                                dataset.append({\n                                    'code': buggy_code,\n                                    'has_bug': True,\n                                    'bug_type': bug_type,\n                                    'metadata': {\n                                        'file': str(py_file),\n                                        'line': chunk.start_line,\n                                        'original': False\n                                    }\n                                })\n            except Exception as e:\n                print(f\"Error processing {py_file}: {e}\")\n\n        return dataset\n\n    def _extract_function_name(self, code):\n        \"\"\"Extract function name from code.\"\"\"\n        match = re.search(r'def\\s+(\\w+)', code)\n        return match.group(1) if match else None\n\n    def _anonymize_function(self, code, name):\n        \"\"\"Replace function name with placeholder.\"\"\"\n        return re.sub(rf'\\bdef\\s+{name}\\b', 'def &lt;FUNCTION_NAME&gt;', code, count=1)\n\n    def _extract_docstring(self, code):\n        \"\"\"Extract docstring from code.\"\"\"\n        lines = code.split('\\n')\n        in_docstring = False\n        docstring_lines = []\n        quote_style = None\n\n        for line in lines[1:]:  # Skip function/class definition\n            stripped = line.strip()\n\n            if not in_docstring and (stripped.startswith('\"\"\"') or stripped.startswith(\"'''\")):\n                quote_style = '\"\"\"' if stripped.startswith('\"\"\"') else \"'''\"\n                in_docstring = True\n\n                # Check for single-line docstring\n                if stripped.count(quote_style) == 2:\n                    return stripped.strip(quote_style)\n                else:\n                    docstring_lines.append(stripped.replace(quote_style, '', 1))\n            elif in_docstring:\n                if quote_style in stripped:\n                    docstring_lines.append(stripped.replace(quote_style, '', 1))\n                    break\n                else:\n                    docstring_lines.append(stripped)\n\n        return '\\n'.join(docstring_lines).strip() if docstring_lines else None\n\n    def _remove_docstring(self, code):\n        \"\"\"Remove docstring from code.\"\"\"\n        lines = code.split('\\n')\n        result_lines = [lines[0]]  # Keep function/class definition\n\n        in_docstring = False\n        quote_style = None\n\n        for line in lines[1:]:\n            stripped = line.strip()\n\n            if not in_docstring and (stripped.startswith('\"\"\"') or stripped.startswith(\"'''\")):\n                quote_style = '\"\"\"' if stripped.startswith('\"\"\"') else \"'''\"\n                in_docstring = True\n\n                # Check for single-line docstring\n                if stripped.count(quote_style) == 2:\n                    continue\n            elif in_docstring and quote_style in stripped:\n                in_docstring = False\n                continue\n            elif not in_docstring:\n                result_lines.append(line)\n\n        return '\\n'.join(result_lines)\n\n    def _introduce_bugs(self, code):\n        \"\"\"Introduce various types of bugs into code.\"\"\"\n        bugs = []\n\n        # Off-by-one error\n        off_by_one = re.sub(r'range\\((\\w+)\\)', r'range(\\1 + 1)', code)\n        if off_by_one != code:\n            bugs.append(('off_by_one', off_by_one))\n\n        # Wrong operator\n        wrong_op = code.replace('==', '=')  # Dangerous but for synthetic data\n        if wrong_op != code:\n            bugs.append(('wrong_operator', wrong_op))\n\n        # Missing return\n        if 'return' in code:\n            no_return = re.sub(r'return\\s+.*$', 'pass', code, flags=re.MULTILINE)\n            if no_return != code:\n                bugs.append(('missing_return', no_return))\n\n        return bugs\n\n    def save_dataset(self, dataset: List[Dict], output_path: str, split_ratio=0.8):\n        \"\"\"Save dataset with train/val split.\"\"\"\n        # Shuffle\n        random.shuffle(dataset)\n\n        # Split\n        split_idx = int(len(dataset) * split_ratio)\n        train_data = dataset[:split_idx]\n        val_data = dataset[split_idx:]\n\n        # Save\n        base_path = Path(output_path)\n        base_path.mkdir(exist_ok=True)\n\n        with open(base_path / \"train.jsonl\", \"w\") as f:\n            for item in train_data:\n                f.write(json.dumps(item) + \"\\n\")\n\n        with open(base_path / \"val.jsonl\", \"w\") as f:\n            for item in val_data:\n                f.write(json.dumps(item) + \"\\n\")\n\n        # Save statistics\n        stats = {\n            'total_examples': len(dataset),\n            'train_examples': len(train_data),\n            'val_examples': len(val_data),\n            'example_fields': list(dataset[0].keys()) if dataset else []\n        }\n\n        with open(base_path / \"stats.json\", \"w\") as f:\n            json.dump(stats, f, indent=2)\n\n        print(f\"Dataset saved to {output_path}\")\n        print(f\"  Train: {len(train_data)} examples\")\n        print(f\"  Val: {len(val_data)} examples\")\n\n# Usage\ngenerator = CodeDatasetGenerator()\n\n# Generate different datasets\nnaming_dataset = generator.create_function_naming_dataset(\"./src\")\ngenerator.save_dataset(naming_dataset, \"./datasets/function_naming\")\n\ndocstring_dataset = generator.create_docstring_generation_dataset(\"./src\")\ngenerator.save_dataset(docstring_dataset, \"./datasets/docstring_generation\")\n\ncompletion_dataset = generator.create_code_completion_dataset(\"./src\")\ngenerator.save_dataset(completion_dataset, \"./datasets/code_completion\")\n\nbug_dataset = generator.create_bug_detection_dataset(\"./src\")\ngenerator.save_dataset(bug_dataset, \"./datasets/bug_detection\")\n</code></pre>"},{"location":"cookbook/#performance-optimization","title":"Performance Optimization","text":""},{"location":"cookbook/#using-new-parallel-processing-apis","title":"Using New Parallel Processing APIs","text":"<p>Leverage the new parallel processing APIs for maximum performance.</p> <pre><code>from chunker import (\n    chunk_files_parallel,\n    chunk_directory_parallel,\n    ASTCache\n)\nfrom pathlib import Path\nimport time\n\nclass OptimizedBatchProcessor:\n    \"\"\"High-performance batch processor using new APIs.\"\"\"\n\n    def __init__(self):\n        # Configure for maximum performance\n        self.cache = ASTCache(max_size=1000)\n\n    def process_large_project(self, project_root: str):\n        \"\"\"Process a large project efficiently.\"\"\"\n        start_time = time.time()\n\n        # Process by language for better cache utilization\n        languages = {\n            'python': '**/*.py',\n            'javascript': '**/*.js',\n            'rust': '**/*.rs'\n        }\n\n        all_results = {}\n        total_chunks = 0\n\n        for language, pattern in languages.items():\n            print(f\"\\nProcessing {language} files...\")\n\n            # Use the new parallel directory API\n            results = chunk_directory_parallel(\n                project_root,\n                language,\n                pattern=pattern,\n                max_workers=8,  # Adjust based on CPU\n                show_progress=True\n            )\n\n            # Aggregate results\n            language_chunks = sum(len(chunks) for chunks in results.values())\n            total_chunks += language_chunks\n            all_results.update(results)\n\n            print(f\"  Processed {len(results)} {language} files\")\n            print(f\"  Found {language_chunks} chunks\")\n\n            # Show cache performance\n            cache_stats = self.cache.get_stats()\n            print(f\"  Cache hit rate: {cache_stats['hit_rate']:.1%}\")\n\n        # Summary\n        duration = time.time() - start_time\n        print(f\"\\nTotal Processing Summary:\")\n        print(f\"  Files: {len(all_results)}\")\n        print(f\"  Chunks: {total_chunks}\")\n        print(f\"  Duration: {duration:.2f}s\")\n        print(f\"  Speed: {len(all_results)/duration:.1f} files/sec\")\n\n        return all_results\n\n    def compare_sequential_vs_parallel(self, test_files: List[str], language: str):\n        \"\"\"Compare sequential vs parallel performance.\"\"\"\n        from chunker import chunk_file\n\n        # Sequential processing\n        print(\"Sequential processing...\")\n        start = time.time()\n        sequential_results = {}\n        for file in test_files:\n            sequential_results[file] = chunk_file(file, language)\n        sequential_time = time.time() - start\n\n        # Clear cache for fair comparison\n        self.cache.clear()\n\n        # Parallel processing\n        print(\"Parallel processing...\")\n        start = time.time()\n        parallel_results = chunk_files_parallel(\n            test_files,\n            language,\n            max_workers=8,\n            show_progress=False\n        )\n        parallel_time = time.time() - start\n\n        # Results\n        print(f\"\\nPerformance Comparison:\")\n        print(f\"  Sequential: {sequential_time:.2f}s\")\n        print(f\"  Parallel: {parallel_time:.2f}s\")\n        print(f\"  Speedup: {sequential_time/parallel_time:.2f}x\")\n        print(f\"  Files processed: {len(test_files)}\")\n\n        return {\n            'sequential_time': sequential_time,\n            'parallel_time': parallel_time,\n            'speedup': sequential_time/parallel_time\n        }\n\n# Usage\nprocessor = OptimizedBatchProcessor()\n\n# Process entire project\nresults = processor.process_large_project(\"./large_project\")\n\n# Compare performance\ntest_files = list(Path(\"src/\").glob(\"*.py\"))[:20]\nif test_files:\n    comparison = processor.compare_sequential_vs_parallel(\n        [str(f) for f in test_files],\n        \"python\"\n    )\n</code></pre>"},{"location":"cookbook/#streaming-processing-for-huge-files","title":"Streaming Processing for Huge Files","text":"<p>Handle files too large for memory using the streaming API.</p> <pre><code>from chunker import chunk_file_streaming\nfrom pathlib import Path\nimport psutil\nimport os\n\nclass MemoryEfficientProcessor:\n    \"\"\"Process huge files without loading them entirely into memory.\"\"\"\n\n    def __init__(self, memory_limit_mb: int = 1000):\n        self.memory_limit_bytes = memory_limit_mb * 1024 * 1024\n\n    def process_huge_file(self, file_path: str, language: str):\n        \"\"\"Process a very large file using streaming.\"\"\"\n        file_size_mb = Path(file_path).stat().st_size / (1024 * 1024)\n        print(f\"Processing {file_path} ({file_size_mb:.1f} MB)...\")\n\n        # Monitor memory usage\n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss\n\n        chunks_processed = 0\n        chunk_types = {}\n\n        # Stream process the file\n        for chunk in chunk_file_streaming(file_path, language):\n            chunks_processed += 1\n\n            # Track chunk types\n            chunk_types[chunk.node_type] = chunk_types.get(chunk.node_type, 0) + 1\n\n            # Process chunk immediately (don't store)\n            self._process_chunk(chunk)\n\n            # Check memory usage periodically\n            if chunks_processed % 100 == 0:\n                current_memory = process.memory_info().rss\n                memory_used = (current_memory - initial_memory) / (1024 * 1024)\n                print(f\"\\rProcessed {chunks_processed} chunks, \"\n                      f\"Memory delta: {memory_used:.1f} MB\", end=\"\")\n\n                # If approaching limit, trigger garbage collection\n                if current_memory &gt; initial_memory + self.memory_limit_bytes:\n                    import gc\n                    gc.collect()\n\n        # Final stats\n        final_memory = process.memory_info().rss\n        total_memory_used = (final_memory - initial_memory) / (1024 * 1024)\n\n        print(f\"\\n\\nStreaming Processing Complete:\")\n        print(f\"  File size: {file_size_mb:.1f} MB\")\n        print(f\"  Chunks processed: {chunks_processed}\")\n        print(f\"  Memory used: {total_memory_used:.1f} MB\")\n        print(f\"  Memory efficiency: {total_memory_used/file_size_mb:.1%}\")\n        print(f\"\\nChunk type distribution:\")\n        for node_type, count in sorted(chunk_types.items(), key=lambda x: x[1], reverse=True):\n            print(f\"    {node_type}: {count}\")\n\n    def _process_chunk(self, chunk):\n        \"\"\"Process a single chunk without storing it.\"\"\"\n        # Example: Write to database, send to API, etc.\n        # This avoids keeping all chunks in memory\n        pass\n\n    def batch_process_large_files(self, directory: str, size_threshold_mb: int = 10):\n        \"\"\"Process all large files in a directory using streaming.\"\"\"\n        large_files = []\n\n        for file_path in Path(directory).rglob(\"*\"):\n            if file_path.is_file():\n                size_mb = file_path.stat().st_size / (1024 * 1024)\n                if size_mb &gt; size_threshold_mb:\n                    ext = file_path.suffix.lower()\n                    if ext in ['.py', '.js', '.rs', '.c', '.cpp']:\n                        large_files.append((file_path, size_mb))\n\n        print(f\"Found {len(large_files)} large files (&gt;{size_threshold_mb} MB)\\n\")\n\n        for file_path, size_mb in sorted(large_files, key=lambda x: x[1], reverse=True):\n            language = self._detect_language(file_path)\n            if language:\n                self.process_huge_file(str(file_path), language)\n                print(\"\\n\" + \"=\"*60 + \"\\n\")\n\n    def _detect_language(self, file_path: Path) -&gt; str:\n        \"\"\"Detect language from file extension.\"\"\"\n        ext_map = {\n            '.py': 'python',\n            '.js': 'javascript',\n            '.rs': 'rust',\n            '.c': 'c',\n            '.cpp': 'cpp',\n            '.cc': 'cpp'\n        }\n        return ext_map.get(file_path.suffix.lower())\n\n# Usage\nprocessor = MemoryEfficientProcessor(memory_limit_mb=500)\n\n# Process a single huge file\nif Path(\"huge_codebase.py\").exists():\n    processor.process_huge_file(\"huge_codebase.py\", \"python\")\n\n# Process all large files in a directory\nprocessor.batch_process_large_files(\"./large_project\", size_threshold_mb=10)\n</code></pre>"},{"location":"cookbook/#configuration-recipes","title":"Configuration Recipes","text":""},{"location":"cookbook/#dynamic-configuration-based-on-project-type","title":"Dynamic Configuration Based on Project Type","text":"<p>Automatically configure based on detected project type.</p> <pre><code>from chunker import ChunkerConfig, chunk_file\nfrom pathlib import Path\nimport toml\nimport yaml\nimport json\n\nclass SmartConfigurator:\n    \"\"\"Automatically configure chunker based on project characteristics.\"\"\"\n\n    def __init__(self):\n        self.project_patterns = {\n            'django': {\n                'files': ['manage.py', 'settings.py'],\n                'config': {\n                    'languages': {\n                        'python': {\n                            'chunk_types': [\n                                'function_definition',\n                                'class_definition',\n                                'decorated_definition'\n                            ],\n                            'custom_options': {\n                                'include_views': True,\n                                'include_models': True,\n                                'include_serializers': True\n                            }\n                        }\n                    }\n                }\n            },\n            'react': {\n                'files': ['package.json', 'src/App.js'],\n                'config': {\n                    'languages': {\n                        'javascript': {\n                            'chunk_types': [\n                                'function_declaration',\n                                'arrow_function',\n                                'class_declaration',\n                                'jsx_element'\n                            ],\n                            'custom_options': {\n                                'include_jsx': True,\n                                'include_hooks': True\n                            }\n                        }\n                    }\n                }\n            },\n            'rust': {\n                'files': ['Cargo.toml'],\n                'config': {\n                    'languages': {\n                        'rust': {\n                            'chunk_types': [\n                                'function_item',\n                                'impl_item',\n                                'trait_item',\n                                'struct_item',\n                                'enum_item'\n                            ],\n                            'custom_options': {\n                                'include_tests': False,\n                                'include_macros': True\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n    def detect_project_type(self, project_root: str) -&gt; str:\n        \"\"\"Detect the type of project.\"\"\"\n        root = Path(project_root)\n\n        for project_type, pattern in self.project_patterns.items():\n            if all(any(root.rglob(f)) for f in pattern['files']):\n                return project_type\n\n        return 'generic'\n\n    def generate_config(self, project_root: str) -&gt; ChunkerConfig:\n        \"\"\"Generate optimal configuration for the project.\"\"\"\n        project_type = self.detect_project_type(project_root)\n        print(f\"Detected project type: {project_type}\")\n\n        if project_type == 'generic':\n            # Analyze project to create custom config\n            config_dict = self._analyze_project(project_root)\n        else:\n            config_dict = self.project_patterns[project_type]['config']\n\n        # Add common settings\n        config_dict.update({\n            'cache_enabled': True,\n            'cache_size': 200,\n            'parallel_workers': 4,\n            'exclude_patterns': [\n                '*test*', '__pycache__', 'node_modules',\n                '.git', '.venv', 'build', 'dist'\n            ]\n        })\n\n        # Save configuration\n        config_path = Path(project_root) / '.chunkerrc'\n        self._save_config(config_dict, config_path)\n\n        print(f\"Generated configuration saved to: {config_path}\")\n        return ChunkerConfig(str(config_path))\n\n    def _analyze_project(self, project_root: str) -&gt; dict:\n        \"\"\"Analyze project structure to generate config.\"\"\"\n        root = Path(project_root)\n        config = {\n            'languages': {},\n            'chunk_types': []\n        }\n\n        # Count files by extension\n        file_counts = {}\n        for file in root.rglob(\"*\"):\n            if file.is_file():\n                ext = file.suffix.lower()\n                if ext in ['.py', '.js', '.rs', '.c', '.cpp']:\n                    file_counts[ext] = file_counts.get(ext, 0) + 1\n\n        # Configure based on predominant languages\n        for ext, count in sorted(file_counts.items(), key=lambda x: x[1], reverse=True):\n            if ext == '.py':\n                config['languages']['python'] = {\n                    'enabled': True,\n                    'chunk_types': [\n                        'function_definition',\n                        'class_definition',\n                        'async_function_definition'\n                    ],\n                    'min_chunk_size': 3,\n                    'max_chunk_size': 300\n                }\n            elif ext in ['.js', '.jsx']:\n                config['languages']['javascript'] = {\n                    'enabled': True,\n                    'chunk_types': [\n                        'function_declaration',\n                        'arrow_function',\n                        'class_declaration'\n                    ]\n                }\n\n        return config\n\n    def _save_config(self, config: dict, path: Path):\n        \"\"\"Save configuration in appropriate format.\"\"\"\n        if path.suffix == '.toml' or path.name == '.chunkerrc':\n            with open(path, 'w') as f:\n                toml.dump(config, f)\n        elif path.suffix in ['.yaml', '.yml']:\n            with open(path, 'w') as f:\n                yaml.dump(config, f, default_flow_style=False)\n        else:\n            with open(path, 'w') as f:\n                json.dump(config, f, indent=2)\n\n    def create_environment_configs(self, project_root: str):\n        \"\"\"Create different configs for different environments.\"\"\"\n        base_config = self.generate_config(project_root)\n\n        # Development config\n        dev_config = {\n            'extends': '.chunkerrc',\n            'cache_size': 500,\n            'log_level': 'DEBUG',\n            'include_tests': True,\n            'languages': {\n                'python': {\n                    'custom_options': {\n                        'include_docstrings': True,\n                        'include_type_hints': True\n                    }\n                }\n            }\n        }\n\n        # Production config\n        prod_config = {\n            'extends': '.chunkerrc',\n            'cache_size': 100,\n            'log_level': 'WARNING',\n            'include_tests': False,\n            'min_chunk_size': 5,\n            'exclude_patterns': [\n                '*test*', '*_test.py', 'test_*.py',\n                '__pycache__', '.pytest_cache'\n            ]\n        }\n\n        # CI config\n        ci_config = {\n            'extends': '.chunkerrc',\n            'parallel_workers': 2,\n            'show_progress': False,\n            'output_format': 'json',\n            'fail_on_error': True\n        }\n\n        # Save environment configs\n        root = Path(project_root)\n        self._save_config(dev_config, root / 'chunker.dev.toml')\n        self._save_config(prod_config, root / 'chunker.prod.toml')\n        self._save_config(ci_config, root / 'chunker.ci.toml')\n\n        print(\"Created environment-specific configurations:\")\n        print(\"  - chunker.dev.toml (development)\")\n        print(\"  - chunker.prod.toml (production)\")\n        print(\"  - chunker.ci.toml (CI/CD)\")\n\n# Usage\nconfigurator = SmartConfigurator()\n\n# Generate config for current project\nconfig = configurator.generate_config(\".\")\n\n# Create environment-specific configs\nconfigurator.create_environment_configs(\".\")\n\n# Use the generated config\nchunks = chunk_file(\"example.py\", \"python\")\nprint(f\"Processed with custom config: {len(chunks)} chunks\")\n</code></pre>"},{"location":"cookbook/#parallel-processing-with-progress-tracking","title":"Parallel Processing with Progress Tracking","text":"<p>Process large codebases efficiently with detailed progress tracking.</p> <pre><code>from chunker.chunker import chunk_file\nfrom chunker.parser import get_parser, return_parser, clear_cache\nfrom pathlib import Path\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\nfrom multiprocessing import cpu_count, Manager\nimport time\nfrom tqdm import tqdm\nfrom typing import List, Dict, Any\nimport psutil\nimport os\n\nclass ParallelProcessor:\n    \"\"\"High-performance parallel code processor.\"\"\"\n\n    def __init__(self, max_workers=None, use_processes=True):\n        self.max_workers = max_workers or cpu_count()\n        self.use_processes = use_processes\n        self.stats = {\n            'files_processed': 0,\n            'chunks_found': 0,\n            'errors': 0,\n            'start_time': None,\n            'end_time': None\n        }\n\n    def process_directory(self, directory: str, file_pattern: str = \"*.py\", \n                         process_func=None) -&gt; List[Dict[str, Any]]:\n        \"\"\"Process all matching files in directory.\"\"\"\n        files = list(Path(directory).rglob(file_pattern))\n\n        print(f\"Found {len(files)} files to process\")\n        print(f\"Using {self.max_workers} workers ({'processes' if self.use_processes else 'threads'})\")\n\n        self.stats['start_time'] = time.time()\n\n        # Choose executor based on configuration\n        Executor = ProcessPoolExecutor if self.use_processes else ThreadPoolExecutor\n\n        results = []\n        with Executor(max_workers=self.max_workers) as executor:\n            # Submit all tasks\n            future_to_file = {\n                executor.submit(\n                    process_func or self._process_file,\n                    str(file_path)\n                ): file_path\n                for file_path in files\n            }\n\n            # Process with progress bar\n            with tqdm(total=len(files), desc=\"Processing files\") as pbar:\n                for future in as_completed(future_to_file):\n                    file_path = future_to_file[future]\n\n                    try:\n                        result = future.result()\n                        results.append(result)\n                        self.stats['files_processed'] += 1\n                        self.stats['chunks_found'] += result.get('chunks_count', 0)\n\n                        # Update progress bar with info\n                        pbar.set_postfix({\n                            'chunks': self.stats['chunks_found'],\n                            'errors': self.stats['errors']\n                        })\n\n                    except Exception as e:\n                        self.stats['errors'] += 1\n                        results.append({\n                            'file': str(file_path),\n                            'error': str(e),\n                            'chunks': []\n                        })\n\n                    pbar.update(1)\n\n        self.stats['end_time'] = time.time()\n        self._print_summary()\n\n        return results\n\n    def _process_file(self, file_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Default file processor.\"\"\"\n        start_time = time.time()\n\n        # Detect language\n        language = self._detect_language(file_path)\n\n        # Process file\n        chunks = chunk_file(file_path, language)\n\n        # Analyze chunks\n        analysis = {\n            'file': file_path,\n            'language': language,\n            'chunks_count': len(chunks),\n            'processing_time': time.time() - start_time,\n            'chunks': chunks,\n            'stats': self._analyze_chunks(chunks)\n        }\n\n        return analysis\n\n    def _detect_language(self, file_path: str) -&gt; str:\n        \"\"\"Detect language from file extension.\"\"\"\n        ext_map = {\n            '.py': 'python',\n            '.js': 'javascript',\n            '.ts': 'javascript',\n            '.rs': 'rust',\n            '.c': 'c',\n            '.h': 'c',\n            '.cpp': 'cpp',\n            '.cc': 'cpp',\n            '.hpp': 'cpp'\n        }\n\n        ext = Path(file_path).suffix.lower()\n        return ext_map.get(ext, 'python')\n\n    def _analyze_chunks(self, chunks: List) -&gt; Dict[str, Any]:\n        \"\"\"Analyze chunks for statistics.\"\"\"\n        if not chunks:\n            return {}\n\n        stats = {\n            'total_lines': sum(c.end_line - c.start_line + 1 for c in chunks),\n            'by_type': {},\n            'largest_chunk': max(chunks, key=lambda c: c.end_line - c.start_line).node_type,\n            'avg_chunk_size': sum(c.end_line - c.start_line + 1 for c in chunks) / len(chunks)\n        }\n\n        # Count by type\n        for chunk in chunks:\n            stats['by_type'][chunk.node_type] = stats['by_type'].get(chunk.node_type, 0) + 1\n\n        return stats\n\n    def _print_summary(self):\n        \"\"\"Print processing summary.\"\"\"\n        duration = self.stats['end_time'] - self.stats['start_time']\n        files_per_second = self.stats['files_processed'] / duration if duration &gt; 0 else 0\n\n        print(\"\\n\" + \"=\"*60)\n        print(\"Processing Summary\")\n        print(\"=\"*60)\n        print(f\"Files processed: {self.stats['files_processed']}\")\n        print(f\"Total chunks: {self.stats['chunks_found']}\")\n        print(f\"Errors: {self.stats['errors']}\")\n        print(f\"Duration: {duration:.2f} seconds\")\n        print(f\"Speed: {files_per_second:.2f} files/second\")\n        print(f\"Memory usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024:.1f} MB\")\n\nclass OptimizedChunker:\n    \"\"\"Memory-efficient chunker for large files.\"\"\"\n\n    def __init__(self, chunk_size_mb: int = 10):\n        self.chunk_size_bytes = chunk_size_mb * 1024 * 1024\n\n    def chunk_large_file(self, file_path: str, language: str) -&gt; List:\n        \"\"\"Chunk large files in parts to manage memory.\"\"\"\n        file_size = Path(file_path).stat().st_size\n\n        if file_size &lt;= self.chunk_size_bytes:\n            # Small file - process normally\n            return chunk_file(file_path, language)\n\n        # Large file - process in parts\n        print(f\"Large file detected ({file_size / 1024 / 1024:.1f} MB), processing in chunks...\")\n\n        all_chunks = []\n        parser = get_parser(language)\n\n        try:\n            with open(file_path, 'rb') as f:\n                offset = 0\n\n                while offset &lt; file_size:\n                    # Read chunk\n                    f.seek(offset)\n                    data = f.read(self.chunk_size_bytes)\n\n                    # Find good break point (end of line)\n                    if offset + len(data) &lt; file_size:\n                        last_newline = data.rfind(b'\\n')\n                        if last_newline != -1:\n                            data = data[:last_newline + 1]\n\n                    # Parse chunk\n                    tree = parser.parse(data)\n\n                    # Extract chunks (adjust line numbers)\n                    line_offset = data[:offset].count(b'\\n') if offset &gt; 0 else 0\n                    chunks = self._extract_chunks_from_tree(\n                        tree, data, file_path, language, line_offset\n                    )\n                    all_chunks.extend(chunks)\n\n                    # Move to next chunk\n                    offset += len(data)\n\n        finally:\n            return_parser(language, parser)\n\n        return all_chunks\n\n    def _extract_chunks_from_tree(self, tree, data, file_path, language, line_offset):\n        \"\"\"Extract chunks from parse tree.\"\"\"\n        # This is a simplified version - real implementation would\n        # properly walk the tree and extract chunks\n        chunks = []\n\n        # ... chunk extraction logic ...\n\n        return chunks\n\n# Usage examples\nif __name__ == \"__main__\":\n    # Example 1: Parallel processing with custom function\n    def analyze_complexity(file_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Custom processing function.\"\"\"\n        language = Path(file_path).suffix.lstrip('.')\n        language_map = {'py': 'python', 'js': 'javascript', 'rs': 'rust'}\n        language = language_map.get(language, 'python')\n\n        chunks = chunk_file(file_path, language)\n\n        # Calculate complexity metrics\n        complexity_scores = []\n        for chunk in chunks:\n            if chunk.node_type in [\"function_definition\", \"method_definition\"]:\n                lines = chunk.end_line - chunk.start_line + 1\n                # Simple complexity heuristic\n                complexity = lines * 0.1 + chunk.content.count('if ') * 2\n                complexity_scores.append({\n                    'name': chunk.content.split('\\n')[0],\n                    'complexity': complexity,\n                    'lines': lines\n                })\n\n        return {\n            'file': file_path,\n            'chunks': chunks,\n            'complexity_scores': complexity_scores,\n            'avg_complexity': sum(s['complexity'] for s in complexity_scores) / len(complexity_scores) \n                             if complexity_scores else 0\n        }\n\n    # Run parallel processing\n    processor = ParallelProcessor(max_workers=8, use_processes=True)\n    results = processor.process_directory(\n        \"./large_codebase\",\n        file_pattern=\"*.py\",\n        process_func=analyze_complexity\n    )\n\n    # Find most complex functions\n    all_complexities = []\n    for result in results:\n        if 'complexity_scores' in result:\n            for score in result['complexity_scores']:\n                score['file'] = result['file']\n                all_complexities.append(score)\n\n    # Sort by complexity\n    all_complexities.sort(key=lambda x: x['complexity'], reverse=True)\n\n    print(\"\\nTop 10 Most Complex Functions:\")\n    for i, item in enumerate(all_complexities[:10], 1):\n        print(f\"{i}. {Path(item['file']).name}: {item['name']}\")\n        print(f\"   Complexity: {item['complexity']:.2f}, Lines: {item['lines']}\")\n</code></pre>"},{"location":"cookbook/#incremental-processing-with-change-detection","title":"Incremental Processing with Change Detection","text":"<p>Process only changed files for efficiency.</p> <pre><code>from chunker.chunker import chunk_file\nfrom pathlib import Path\nimport hashlib\nimport json\nimport time\nfrom datetime import datetime\nfrom typing import Dict, List, Set\nimport git\n\nclass IncrementalProcessor:\n    \"\"\"Process only changed files since last run.\"\"\"\n\n    def __init__(self, cache_file: str = \".chunker_cache.json\"):\n        self.cache_file = cache_file\n        self.cache = self._load_cache()\n        self.repo = None\n\n        # Try to initialize git repo\n        try:\n            self.repo = git.Repo(search_parent_directories=True)\n        except:\n            print(\"Git repository not found, using file modification times\")\n\n    def _load_cache(self) -&gt; Dict:\n        \"\"\"Load processing cache.\"\"\"\n        if Path(self.cache_file).exists():\n            with open(self.cache_file, 'r') as f:\n                return json.load(f)\n        return {\n            'files': {},\n            'last_run': None,\n            'stats': {}\n        }\n\n    def _save_cache(self):\n        \"\"\"Save processing cache.\"\"\"\n        self.cache['last_run'] = datetime.now().isoformat()\n        with open(self.cache_file, 'w') as f:\n            json.dump(self.cache, f, indent=2)\n\n    def _get_file_hash(self, file_path: str) -&gt; str:\n        \"\"\"Calculate file hash.\"\"\"\n        hasher = hashlib.sha256()\n        with open(file_path, 'rb') as f:\n            while chunk := f.read(8192):\n                hasher.update(chunk)\n        return hasher.hexdigest()\n\n    def _get_changed_files(self, directory: str, pattern: str = \"*.py\") -&gt; Set[Path]:\n        \"\"\"Get files that have changed since last run.\"\"\"\n        changed_files = set()\n        all_files = set()\n\n        # Get all matching files\n        for file_path in Path(directory).rglob(pattern):\n            if \"__pycache__\" not in str(file_path):\n                all_files.add(file_path)\n\n        # Check using git if available\n        if self.repo and self.cache.get('last_run'):\n            try:\n                # Get changed files since last run\n                last_run = datetime.fromisoformat(self.cache['last_run'])\n                diff = self.repo.index.diff(None)  # Unstaged changes\n\n                for item in diff:\n                    file_path = Path(self.repo.working_dir) / item.a_path\n                    if file_path in all_files:\n                        changed_files.add(file_path)\n\n                # Also check committed changes\n                if self.cache.get('last_commit'):\n                    commits = list(self.repo.iter_commits(\n                        f\"{self.cache['last_commit']}..HEAD\"\n                    ))\n                    for commit in commits:\n                        for item in commit.diff(commit.parents[0] if commit.parents else None):\n                            file_path = Path(self.repo.working_dir) / item.a_path\n                            if file_path in all_files:\n                                changed_files.add(file_path)\n\n                # Update last commit\n                self.cache['last_commit'] = self.repo.head.commit.hexsha\n\n            except Exception as e:\n                print(f\"Git diff failed: {e}, falling back to hash comparison\")\n                changed_files = self._check_by_hash(all_files)\n        else:\n            # Fall back to hash comparison\n            changed_files = self._check_by_hash(all_files)\n\n        # Check for new files\n        cached_files = set(Path(f) for f in self.cache['files'].keys())\n        new_files = all_files - cached_files\n        changed_files.update(new_files)\n\n        # Check for deleted files\n        deleted_files = cached_files - all_files\n        for file_path in deleted_files:\n            del self.cache['files'][str(file_path)]\n\n        return changed_files\n\n    def _check_by_hash(self, files: Set[Path]) -&gt; Set[Path]:\n        \"\"\"Check which files changed by comparing hashes.\"\"\"\n        changed = set()\n\n        for file_path in files:\n            file_str = str(file_path)\n            current_hash = self._get_file_hash(file_str)\n\n            if file_str not in self.cache['files'] or \\\n               self.cache['files'][file_str].get('hash') != current_hash:\n                changed.add(file_path)\n\n        return changed\n\n    def process_incrementally(self, directory: str, language: str = \"python\",\n                            force_all: bool = False) -&gt; Dict[str, Any]:\n        \"\"\"Process only changed files.\"\"\"\n        start_time = time.time()\n\n        # Get changed files\n        if force_all:\n            pattern = {'python': '*.py', 'javascript': '*.js', 'rust': '*.rs'}.get(language, '*')\n            changed_files = set(Path(directory).rglob(pattern))\n        else:\n            changed_files = self._get_changed_files(directory)\n\n        print(f\"Found {len(changed_files)} changed files\")\n\n        # Process changed files\n        results = {\n            'processed_files': [],\n            'total_chunks': 0,\n            'errors': [],\n            'processing_time': 0\n        }\n\n        for file_path in changed_files:\n            try:\n                # Process file\n                chunks = chunk_file(str(file_path), language)\n\n                # Update cache\n                file_info = {\n                    'hash': self._get_file_hash(str(file_path)),\n                    'last_processed': datetime.now().isoformat(),\n                    'chunks_count': len(chunks),\n                    'chunk_types': list(set(c.node_type for c in chunks))\n                }\n                self.cache['files'][str(file_path)] = file_info\n\n                # Add to results\n                results['processed_files'].append({\n                    'file': str(file_path),\n                    'chunks': len(chunks),\n                    'types': file_info['chunk_types']\n                })\n                results['total_chunks'] += len(chunks)\n\n            except Exception as e:\n                results['errors'].append({\n                    'file': str(file_path),\n                    'error': str(e)\n                })\n\n        # Update statistics\n        results['processing_time'] = time.time() - start_time\n        self._update_stats(results)\n\n        # Save cache\n        self._save_cache()\n\n        # Print summary\n        self._print_summary(results)\n\n        return results\n\n    def _update_stats(self, results: Dict[str, Any]):\n        \"\"\"Update cumulative statistics.\"\"\"\n        if 'cumulative' not in self.cache['stats']:\n            self.cache['stats']['cumulative'] = {\n                'total_runs': 0,\n                'total_files_processed': 0,\n                'total_chunks_found': 0,\n                'total_errors': 0,\n                'total_time': 0\n            }\n\n        stats = self.cache['stats']['cumulative']\n        stats['total_runs'] += 1\n        stats['total_files_processed'] += len(results['processed_files'])\n        stats['total_chunks_found'] += results['total_chunks']\n        stats['total_errors'] += len(results['errors'])\n        stats['total_time'] += results['processing_time']\n\n    def _print_summary(self, results: Dict[str, Any]):\n        \"\"\"Print processing summary.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"Incremental Processing Summary\")\n        print(\"=\"*60)\n        print(f\"Files processed: {len(results['processed_files'])}\")\n        print(f\"Total chunks: {results['total_chunks']}\")\n        print(f\"Errors: {len(results['errors'])}\")\n        print(f\"Processing time: {results['processing_time']:.2f} seconds\")\n\n        if results['errors']:\n            print(\"\\nErrors:\")\n            for error in results['errors'][:5]:\n                print(f\"  - {error['file']}: {error['error']}\")\n\n        # Show cumulative stats\n        if 'cumulative' in self.cache['stats']:\n            stats = self.cache['stats']['cumulative']\n            print(\"\\nCumulative Statistics:\")\n            print(f\"  Total runs: {stats['total_runs']}\")\n            print(f\"  Total files processed: {stats['total_files_processed']}\")\n            print(f\"  Total chunks found: {stats['total_chunks_found']}\")\n            print(f\"  Average files per run: {stats['total_files_processed'] / stats['total_runs']:.1f}\")\n\n    def get_file_history(self, file_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Get processing history for a file.\"\"\"\n        file_str = str(file_path)\n        if file_str in self.cache['files']:\n            return self.cache['files'][file_str]\n        return None\n\n# Usage\nprocessor = IncrementalProcessor()\n\n# First run - processes all files\nresults = processor.process_incrementally(\"./src\", language=\"python\")\n\n# Make some changes to files...\n\n# Second run - only processes changed files\nresults = processor.process_incrementally(\"./src\", language=\"python\")\n\n# Force reprocess all files\nresults = processor.process_incrementally(\"./src\", language=\"python\", force_all=True)\n\n# Check file history\nhistory = processor.get_file_history(\"./src/main.py\")\nif history:\n    print(f\"Last processed: {history['last_processed']}\")\n    print(f\"Chunks found: {history['chunks_count']}\")\n</code></pre>"},{"location":"cookbook/#language-specific-recipes","title":"Language-Specific Recipes","text":""},{"location":"cookbook/#python-extract-type-hints-and-decorators","title":"Python: Extract Type Hints and Decorators","text":"<pre><code>import ast\nfrom chunker.chunker import chunk_file\nfrom typing import List, Dict, Any\n\nclass PythonAnalyzer:\n    \"\"\"Python-specific code analysis.\"\"\"\n\n    def analyze_type_hints(self, file_path: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"Extract and analyze type hints.\"\"\"\n        chunks = chunk_file(file_path, \"python\")\n        results = []\n\n        for chunk in chunks:\n            if chunk.node_type in [\"function_definition\", \"method_definition\"]:\n                try:\n                    tree = ast.parse(chunk.content)\n                    func = tree.body[0]\n\n                    analysis = {\n                        'function': func.name,\n                        'location': f\"{file_path}:{chunk.start_line}\",\n                        'parameters': self._extract_param_types(func),\n                        'return_type': ast.unparse(func.returns) if func.returns else None,\n                        'has_complete_hints': self._check_complete_hints(func),\n                        'decorators': [ast.unparse(d) for d in func.decorator_list]\n                    }\n\n                    results.append(analysis)\n                except:\n                    continue\n\n        return results\n\n    def _extract_param_types(self, func_node):\n        \"\"\"Extract parameter type hints.\"\"\"\n        params = []\n        for arg in func_node.args.args:\n            params.append({\n                'name': arg.arg,\n                'type': ast.unparse(arg.annotation) if arg.annotation else None\n            })\n        return params\n\n    def _check_complete_hints(self, func_node):\n        \"\"\"Check if function has complete type hints.\"\"\"\n        # Check all parameters\n        for arg in func_node.args.args:\n            if arg.arg != 'self' and not arg.annotation:\n                return False\n\n        # Check return type (except for __init__)\n        if func_node.name != '__init__' and not func_node.returns:\n            return False\n\n        return True\n</code></pre>"},{"location":"cookbook/#javascripttypescript-extract-exports-and-imports","title":"JavaScript/TypeScript: Extract Exports and Imports","text":"<pre><code>from chunker.chunker import chunk_file\nimport re\n\nclass JavaScriptAnalyzer:\n    \"\"\"JavaScript/TypeScript-specific analysis.\"\"\"\n\n    def analyze_module_structure(self, file_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Analyze module imports and exports.\"\"\"\n        chunks = chunk_file(file_path, \"javascript\")\n\n        with open(file_path, 'r') as f:\n            content = f.read()\n\n        analysis = {\n            'imports': self._extract_imports(content),\n            'exports': self._extract_exports(chunks, content),\n            'components': self._find_react_components(chunks),\n            'async_functions': self._find_async_functions(chunks)\n        }\n\n        return analysis\n\n    def _extract_imports(self, content: str) -&gt; List[Dict[str, str]]:\n        \"\"\"Extract import statements.\"\"\"\n        imports = []\n\n        # ES6 imports\n        import_pattern = r'import\\s+(?:{([^}]+)}|(\\w+))\\s+from\\s+[\"\\']([^\"\\']+)[\"\\']'\n        for match in re.finditer(import_pattern, content):\n            imports.append({\n                'type': 'named' if match.group(1) else 'default',\n                'names': match.group(1) or match.group(2),\n                'source': match.group(3)\n            })\n\n        return imports\n\n    def _extract_exports(self, chunks, content: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"Extract export statements.\"\"\"\n        exports = []\n\n        for chunk in chunks:\n            if 'export' in chunk.content.split('\\n')[0]:\n                exports.append({\n                    'type': chunk.node_type,\n                    'line': chunk.start_line,\n                    'is_default': 'export default' in chunk.content,\n                    'name': self._extract_export_name(chunk.content)\n                })\n\n        return exports\n\n    def _find_react_components(self, chunks) -&gt; List[str]:\n        \"\"\"Find React components.\"\"\"\n        components = []\n\n        for chunk in chunks:\n            # Check for React component patterns\n            if chunk.node_type == \"class_declaration\":\n                if \"extends Component\" in chunk.content or \"extends React.Component\" in chunk.content:\n                    components.append(chunk.content.split('\\n')[0])\n            elif chunk.node_type == \"function_declaration\":\n                # Check for JSX return\n                if \"return &lt;\" in chunk.content or \"return (\" in chunk.content:\n                    components.append(chunk.content.split('\\n')[0])\n\n        return components\n</code></pre>"},{"location":"cookbook/#rust-analyze-traits-and-implementations","title":"Rust: Analyze Traits and Implementations","text":"<pre><code>from chunker.chunker import chunk_file\nimport re\n\nclass RustAnalyzer:\n    \"\"\"Rust-specific code analysis.\"\"\"\n\n    def analyze_rust_code(self, file_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Analyze Rust code structure.\"\"\"\n        chunks = chunk_file(file_path, \"rust\")\n\n        analysis = {\n            'structs': [],\n            'traits': [],\n            'impls': [],\n            'functions': [],\n            'unsafe_blocks': 0,\n            'lifetimes': set()\n        }\n\n        for chunk in chunks:\n            if chunk.node_type == \"struct_item\":\n                analysis['structs'].append(self._analyze_struct(chunk))\n            elif chunk.node_type == \"trait_item\":\n                analysis['traits'].append(self._analyze_trait(chunk))\n            elif chunk.node_type == \"impl_item\":\n                analysis['impls'].append(self._analyze_impl(chunk))\n            elif chunk.node_type == \"function_item\":\n                analysis['functions'].append(self._analyze_function(chunk))\n\n            # Count unsafe blocks\n            analysis['unsafe_blocks'] += chunk.content.count('unsafe {')\n\n            # Extract lifetimes\n            lifetimes = re.findall(r\"'(\\w+)\", chunk.content)\n            analysis['lifetimes'].update(lifetimes)\n\n        analysis['lifetimes'] = list(analysis['lifetimes'])\n        return analysis\n\n    def _analyze_struct(self, chunk):\n        \"\"\"Analyze struct definition.\"\"\"\n        first_line = chunk.content.split('\\n')[0]\n        return {\n            'name': re.search(r'struct\\s+(\\w+)', first_line).group(1),\n            'is_public': first_line.startswith('pub '),\n            'has_generics': '&lt;' in first_line,\n            'line': chunk.start_line\n        }\n</code></pre>"},{"location":"cookbook/#build-tool-integration","title":"Build Tool Integration","text":""},{"location":"cookbook/#pre-commit-hook","title":"Pre-commit Hook","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nPre-commit hook to check code quality using tree-sitter-chunker.\nSave as .git/hooks/pre-commit and make executable.\n\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom chunker.chunker import chunk_file\nfrom chunker.exceptions import ChunkerError\n\n# Configuration\nMAX_FUNCTION_LENGTH = 50\nMAX_COMPLEXITY = 10\nMAX_FILE_LENGTH = 500\n\ndef get_staged_files():\n    \"\"\"Get list of staged files.\"\"\"\n    result = subprocess.run(\n        ['git', 'diff', '--cached', '--name-only', '--diff-filter=ACM'],\n        capture_output=True,\n        text=True\n    )\n    return [f.strip() for f in result.stdout.split('\\n') if f.strip()]\n\ndef check_file(file_path):\n    \"\"\"Check a single file for issues.\"\"\"\n    issues = []\n\n    # Determine language\n    ext_map = {'.py': 'python', '.js': 'javascript', '.rs': 'rust'}\n    ext = Path(file_path).suffix\n    if ext not in ext_map:\n        return []\n\n    language = ext_map[ext]\n\n    try:\n        chunks = chunk_file(file_path, language)\n\n        # Check function length\n        for chunk in chunks:\n            if chunk.node_type in [\"function_definition\", \"method_definition\"]:\n                length = chunk.end_line - chunk.start_line + 1\n                if length &gt; MAX_FUNCTION_LENGTH:\n                    issues.append({\n                        'file': file_path,\n                        'line': chunk.start_line,\n                        'type': 'long_function',\n                        'message': f'Function is {length} lines (max: {MAX_FUNCTION_LENGTH})'\n                    })\n\n        # Check file length\n        if chunks:\n            total_lines = max(c.end_line for c in chunks)\n            if total_lines &gt; MAX_FILE_LENGTH:\n                issues.append({\n                    'file': file_path,\n                    'line': 1,\n                    'type': 'long_file',\n                    'message': f'File is {total_lines} lines (max: {MAX_FILE_LENGTH})'\n                })\n\n    except ChunkerError as e:\n        # Syntax error - let other tools handle this\n        pass\n\n    return issues\n\ndef main():\n    \"\"\"Run pre-commit checks.\"\"\"\n    staged_files = get_staged_files()\n    all_issues = []\n\n    for file_path in staged_files:\n        if Path(file_path).exists():\n            issues = check_file(file_path)\n            all_issues.extend(issues)\n\n    if all_issues:\n        print(\"Pre-commit check failed:\")\n        print(\"-\" * 60)\n\n        for issue in all_issues:\n            print(f\"{issue['file']}:{issue['line']} - {issue['type']}\")\n            print(f\"  {issue['message']}\")\n\n        print(f\"\\nTotal issues: {len(all_issues)}\")\n        return 1\n\n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre>"},{"location":"cookbook/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<pre><code># .github/workflows/code-quality.yml\nname: Code Quality Check\n\non: [push, pull_request]\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.10'\n\n    - name: Install dependencies\n      run: |\n        pip install uv\n        uv pip install -e .\n        uv pip install git+https://github.com/tree-sitter/py-tree-sitter.git\n        python scripts/fetch_grammars.py\n        python scripts/build_lib.py\n\n    - name: Run code analysis\n      run: |\n        python .github/scripts/analyze_code.py\n\n    - name: Upload results\n      if: always()\n      uses: actions/upload-artifact@v3\n      with:\n        name: code-analysis-results\n        path: |\n          code-quality-report.html\n          code-quality-report.json\n</code></pre>"},{"location":"cookbook/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"cookbook/#custom-chunk-types","title":"Custom Chunk Types","text":"<pre><code>from chunker.parser import get_parser\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n@dataclass\nclass CustomChunk:\n    \"\"\"Extended chunk with custom attributes.\"\"\"\n    type: str\n    name: str\n    content: str\n    start_line: int\n    end_line: int\n    complexity: int\n    dependencies: List[str]\n    test_coverage: Optional[float] = None\n\nclass CustomChunker:\n    \"\"\"Extract custom chunk types.\"\"\"\n\n    def __init__(self):\n        self.custom_extractors = {\n            'python': {\n                'test_function': self._extract_python_tests,\n                'dataclass': self._extract_dataclasses,\n                'api_endpoint': self._extract_api_endpoints\n            }\n        }\n\n    def extract_custom_chunks(self, file_path: str, language: str) -&gt; List[CustomChunk]:\n        \"\"\"Extract custom chunk types.\"\"\"\n        parser = get_parser(language)\n\n        with open(file_path, 'rb') as f:\n            content = f.read()\n\n        tree = parser.parse(content)\n        chunks = []\n\n        if language in self.custom_extractors:\n            for chunk_type, extractor in self.custom_extractors[language].items():\n                chunks.extend(extractor(tree, content, file_path))\n\n        return chunks\n\n    def _extract_python_tests(self, tree, content, file_path):\n        \"\"\"Extract test functions.\"\"\"\n        chunks = []\n\n        # Walk tree and find test functions\n        def walk(node):\n            if node.type == \"function_definition\":\n                func_content = content[node.start_byte:node.end_byte].decode('utf-8')\n                if func_content.startswith('def test_') or '@pytest.mark' in func_content:\n                    chunks.append(CustomChunk(\n                        type='test_function',\n                        name=self._get_function_name(func_content),\n                        content=func_content,\n                        start_line=node.start_point[0] + 1,\n                        end_line=node.end_point[0] + 1,\n                        complexity=self._calculate_complexity(func_content),\n                        dependencies=self._extract_dependencies(func_content)\n                    ))\n\n            for child in node.children:\n                walk(child)\n\n        walk(tree.root_node)\n        return chunks\n</code></pre>"},{"location":"cookbook/#see-also","title":"See Also","text":"<ul> <li>API Reference - Complete API documentation</li> <li>User Guide - Comprehensive usage guide</li> <li>Getting Started - Quick start tutorial</li> <li>Architecture - System design and internals</li> </ul> <p>Happy coding! \ud83d\ude80</p>"},{"location":"cross-language-usage/","title":"Cross-Language Usage Guide","text":"<p>Tree-sitter Chunker can be used from any programming language through multiple integration methods.</p>"},{"location":"cross-language-usage/#integration-methods","title":"Integration Methods","text":""},{"location":"cross-language-usage/#1-python-package-native","title":"1. Python Package (Native)","text":"<p>For Python projects, use the package directly:</p> <pre><code>pip install treesitter-chunker\n\nfrom chunker import chunk_file, chunk_text, chunk_directory\n\n# Chunk a file\nchunks = chunk_file(\"example.py\", language=\"python\")\n\n# Chunk text directly\nchunks = chunk_text(code_string, language=\"javascript\")\n\n# Chunk entire directory\nresults = chunk_directory(\"src/\", language=\"python\")\n</code></pre>"},{"location":"cross-language-usage/#2-command-line-interface-any-language","title":"2. Command-Line Interface (Any Language)","text":"<p>The CLI can be called from any language via subprocess/exec:</p> <pre><code># Output as JSON for easy parsing\ntreesitter-chunker chunk file.py --lang python --output-format json\n\n# Read from stdin\necho \"def hello(): pass\" | treesitter-chunker chunk --stdin --lang python --json\n\n# Batch process with quiet mode\ntreesitter-chunker batch src/ --pattern \"*.js\" --output-format jsonl --quiet\n\n# Minimal output format for easy parsing\ntreesitter-chunker chunk file.py --output-format minimal\n# Output: file.py:1-3:function_definition\n</code></pre> <p>CLI Output Formats: - <code>json</code> - Pretty-printed JSON - <code>jsonl</code> - JSON Lines (one object per line) - <code>minimal</code> - Simple format: <code>file:start-end:type</code> - <code>csv</code> - CSV with headers - <code>table</code> - Rich table (default, human-readable)</p> <p>Example from Node.js:</p> <pre><code>const { exec } = require('child_process');\nconst util = require('util');\nconst execPromise = util.promisify(exec);\n\nasync function chunkFile(filePath, language) {\n    const { stdout } = await execPromise(\n        `treesitter-chunker chunk \"${filePath}\" --lang ${language} --json`\n    );\n    return JSON.parse(stdout);\n}\n</code></pre> <p>Example from Go:</p> <pre><code>import (\n    \"os/exec\"\n    \"encoding/json\"\n)\n\nfunc chunkFile(filePath, language string) ([]Chunk, error) {\n    cmd := exec.Command(\"treesitter-chunker\", \"chunk\", filePath, \n                       \"--lang\", language, \"--json\")\n    output, err := cmd.Output()\n    if err != nil {\n        return nil, err\n    }\n\n    var chunks []Chunk\n    err = json.Unmarshal(output, &amp;chunks)\n    return chunks, err\n}\n</code></pre>"},{"location":"cross-language-usage/#3-rest-api-http","title":"3. REST API (HTTP)","text":"<p>Run the API server:</p> <pre><code># Install with API dependencies\npip install \"treesitter-chunker[api]\"\n\n# Start the server\npython -m api.server\n# Or: uvicorn api.server:app --reload\n</code></pre> <p>The API provides these endpoints: - <code>GET /health</code> - Health check - <code>GET /languages</code> - List supported languages - <code>POST /chunk/text</code> - Chunk source code text - <code>POST /chunk/file</code> - Chunk a file</p> <p>Example requests:</p> <pre><code># Chunk text\ncurl -X POST http://localhost:8000/chunk/text \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"content\": \"def hello():\\n    print(\\\"Hello!\\\")\",\n    \"language\": \"python\"\n  }'\n\n# Chunk file\ncurl -X POST http://localhost:8000/chunk/file \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"file_path\": \"/path/to/file.js\",\n    \"language\": \"javascript\"\n  }'\n</code></pre> <p>See <code>/api/examples/</code> for client examples in Python, JavaScript, and Go.</p>"},{"location":"cross-language-usage/#4-docker-container","title":"4. Docker Container","text":"<p>Use the Docker image for isolated execution:</p> <pre><code># Pull the image\ndocker pull ghcr.io/consiliency/treesitter-chunker:latest\n\n# Run as CLI\ndocker run --rm -v $(pwd):/workspace \\\n  treesitter-chunker chunk /workspace/file.py -l python --json\n\n# Run as API server\ndocker run -p 8000:8000 \\\n  treesitter-chunker python -m api.server\n</code></pre>"},{"location":"cross-language-usage/#5-language-specific-bindings-future","title":"5. Language-Specific Bindings (Future)","text":"<p>Planned native bindings: - JavaScript/TypeScript: npm package using N-API - Go: Module using CGO or exec wrapper - Rust: Crate using PyO3 or native tree-sitter - Java: JAR using JNI or ProcessBuilder</p>"},{"location":"cross-language-usage/#api-response-format","title":"API Response Format","text":"<p>All methods return chunks with this structure:</p> <pre><code>{\n  \"chunks\": [\n    {\n      \"node_type\": \"function_definition\",\n      \"start_line\": 1,\n      \"end_line\": 5,\n      \"content\": \"def hello(name):\\n    ...\",\n      \"parent_context\": \"ClassName\",\n      \"size\": 5\n    }\n  ],\n  \"total_chunks\": 1,\n  \"language\": \"python\"\n}\n</code></pre>"},{"location":"cross-language-usage/#filtering-options","title":"Filtering Options","text":"<p>All methods support these filters: - <code>min_chunk_size</code> - Minimum lines per chunk - <code>max_chunk_size</code> - Maximum lines per chunk - <code>chunk_types</code> - List of node types to include</p>"},{"location":"cross-language-usage/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>CLI: Has startup overhead, best for batch operations</li> <li>API: Keep server running for multiple requests</li> <li>Docker: Additional container overhead, but good isolation</li> <li>Native bindings: Best performance (when available)</li> </ol>"},{"location":"cross-language-usage/#error-handling","title":"Error Handling","text":"<p>All methods return appropriate error codes: - CLI: Non-zero exit code on error - API: HTTP status codes (400 for bad request, 404 for not found) - Subprocess: Check return code and stderr</p>"},{"location":"cross-language-usage/#examples-repository","title":"Examples Repository","text":"<p>See <code>/api/examples/</code> for complete working examples: - <code>client.py</code> - Python API client - <code>client.js</code> - Node.js API client - <code>client.go</code> - Go API client - <code>curl_examples.sh</code> - Shell/curl examples</p>"},{"location":"cross-language-usage/#supported-languages","title":"Supported Languages","text":"<p>Run <code>treesitter-chunker list-languages</code> or <code>GET /languages</code> to see all supported languages.</p> <p>Common languages include: - Python, JavaScript, TypeScript, Go, Rust - Java, C, C++, C#, Ruby, PHP - Swift, Kotlin, Scala, Haskell - And 30+ more...</p>"},{"location":"cross-language-usage/#configuration","title":"Configuration","text":"<p>All methods respect <code>.chunkerrc</code> configuration files in TOML/YAML/JSON format:</p> <pre><code># .chunkerrc\nmin_chunk_size = 5\nmax_chunk_size = 100\n\n[languages.python]\nchunk_types = [\"function_definition\", \"class_definition\"]\n</code></pre>"},{"location":"environment_variables/","title":"Environment Variables Configuration","text":"<p>The Tree-sitter Chunker supports environment variables for configuration in two ways:</p> <ol> <li>Variable expansion in config files - Use <code>${VAR}</code> syntax in configuration files</li> <li>Override configuration values - Use <code>CHUNKER_*</code> environment variables</li> </ol>"},{"location":"environment_variables/#variable-expansion-in-config-files","title":"Variable Expansion in Config Files","text":"<p>You can use environment variables directly in your configuration files using the <code>${VAR}</code> or <code>${VAR:default}</code> syntax.</p>"},{"location":"environment_variables/#basic-syntax","title":"Basic Syntax","text":"<pre><code>[chunker]\nplugin_dirs = [\"${HOME}/.chunker/plugins\", \"${CUSTOM_PLUGIN_DIR}\"]\n</code></pre>"},{"location":"environment_variables/#with-default-values","title":"With Default Values","text":"<pre><code>[chunker.default_plugin_config]\nmin_chunk_size = \"${MIN_CHUNK_SIZE:3}\"  # Uses 3 if MIN_CHUNK_SIZE not set\n</code></pre>"},{"location":"environment_variables/#examples","title":"Examples","text":"<pre><code># YAML example\nchunker:\n  plugin_dirs:\n    - ${HOME}/.chunker/plugins\n    - ${CHUNKER_PLUGINS:/opt/chunker/plugins}\n\nlanguages:\n  python:\n    max_chunk_size: ${PYTHON_MAX_SIZE:500}\n</code></pre> <pre><code>// JSON example\n{\n  \"chunker\": {\n    \"plugin_dirs\": [\"${HOME}/.chunker/plugins\", \"${WORK_DIR}/plugins\"]\n  }\n}\n</code></pre>"},{"location":"environment_variables/#configuration-overrides","title":"Configuration Overrides","text":"<p>Environment variables with the <code>CHUNKER_</code> prefix can override configuration values at runtime.</p>"},{"location":"environment_variables/#global-settings","title":"Global Settings","text":"Environment Variable Description Example <code>CHUNKER_ENABLED_LANGUAGES</code> Comma-separated list of enabled languages <code>python,rust,javascript</code> <code>CHUNKER_PLUGIN_DIRS</code> Comma-separated list of plugin directories <code>/path/one,/path/two</code>"},{"location":"environment_variables/#default-plugin-configuration","title":"Default Plugin Configuration","text":"Environment Variable Description Example <code>CHUNKER_DEFAULT_PLUGIN_CONFIG_MIN_CHUNK_SIZE</code> Default minimum chunk size <code>5</code> <code>CHUNKER_DEFAULT_PLUGIN_CONFIG_MAX_CHUNK_SIZE</code> Default maximum chunk size <code>1000</code>"},{"location":"environment_variables/#language-specific-settings","title":"Language-Specific Settings","text":"<p>For any language, you can set:</p> Environment Variable Pattern Description Example <code>CHUNKER_LANGUAGES_&lt;LANG&gt;_ENABLED</code> Enable/disable language <code>CHUNKER_LANGUAGES_PYTHON_ENABLED=true</code> <code>CHUNKER_LANGUAGES_&lt;LANG&gt;_MIN_CHUNK_SIZE</code> Minimum chunk size <code>CHUNKER_LANGUAGES_RUST_MIN_CHUNK_SIZE=10</code> <code>CHUNKER_LANGUAGES_&lt;LANG&gt;_MAX_CHUNK_SIZE</code> Maximum chunk size <code>CHUNKER_LANGUAGES_RUST_MAX_CHUNK_SIZE=500</code> <code>CHUNKER_LANGUAGES_&lt;LANG&gt;_CHUNK_TYPES</code> Comma-separated chunk types <code>CHUNKER_LANGUAGES_PYTHON_CHUNK_TYPES=function_definition,class_definition</code>"},{"location":"environment_variables/#custom-language-options","title":"Custom Language Options","text":"<p>Any custom option for a language can be set:</p> <pre><code># Python custom options\nexport CHUNKER_LANGUAGES_PYTHON_INCLUDE_DOCSTRINGS=true\n\n# JavaScript custom options  \nexport CHUNKER_LANGUAGES_JAVASCRIPT_INCLUDE_JSX=false\n</code></pre>"},{"location":"environment_variables/#usage-examples","title":"Usage Examples","text":""},{"location":"environment_variables/#example-1-development-vs-production","title":"Example 1: Development vs Production","text":"<pre><code># Development environment\nexport CHUNKER_ENABLED_LANGUAGES=python,rust,javascript,go,java\nexport CHUNKER_DEFAULT_PLUGIN_CONFIG_MIN_CHUNK_SIZE=1\n\n# Production environment\nexport CHUNKER_ENABLED_LANGUAGES=python,rust\nexport CHUNKER_DEFAULT_PLUGIN_CONFIG_MIN_CHUNK_SIZE=5\nexport CHUNKER_LANGUAGES_PYTHON_MAX_CHUNK_SIZE=1000\n</code></pre>"},{"location":"environment_variables/#example-2-cicd-pipeline","title":"Example 2: CI/CD Pipeline","text":"<pre><code># GitHub Actions example\nenv:\n  CHUNKER_ENABLED_LANGUAGES: python,javascript\n  CHUNKER_PLUGIN_DIRS: ${{ github.workspace }}/plugins\n  CHUNKER_LANGUAGES_PYTHON_MIN_CHUNK_SIZE: 10\n</code></pre>"},{"location":"environment_variables/#example-3-docker","title":"Example 3: Docker","text":"<pre><code># Dockerfile\nENV CHUNKER_ENABLED_LANGUAGES=python,rust\nENV CHUNKER_DEFAULT_PLUGIN_CONFIG_MAX_CHUNK_SIZE=2000\n\n# Or in docker-compose.yml\nservices:\n  chunker:\n    environment:\n      - CHUNKER_ENABLED_LANGUAGES=python,rust,go\n      - CHUNKER_PLUGIN_DIRS=/app/plugins,/opt/plugins\n</code></pre>"},{"location":"environment_variables/#example-4-shell-script","title":"Example 4: Shell Script","text":"<pre><code>#!/bin/bash\n\n# Configure chunker for large files\nexport CHUNKER_DEFAULT_PLUGIN_CONFIG_MAX_CHUNK_SIZE=5000\nexport CHUNKER_LANGUAGES_PYTHON_MAX_CHUNK_SIZE=3000\n\n# Run with custom plugin directory\nexport CUSTOM_PLUGINS=/opt/custom-chunker-plugins\n\n# Your chunker command here\npython -m chunker.cli chunk large_file.py\n</code></pre>"},{"location":"environment_variables/#precedence-order","title":"Precedence Order","text":"<p>When multiple configuration sources are present, they are applied in this order:</p> <ol> <li>Default values in code</li> <li>Configuration file values</li> <li>Environment variable expansion in config files (<code>${VAR}</code>)</li> <li>Environment variable overrides (<code>CHUNKER_*</code>)</li> </ol> <p>This means environment variable overrides have the highest precedence.</p>"},{"location":"environment_variables/#disabling-environment-variables","title":"Disabling Environment Variables","text":"<p>If you need to disable environment variable support (e.g., for security reasons), you can do so programmatically:</p> <pre><code>from chunker.chunker_config import ChunkerConfig\n\n# Load config without environment variable support\nconfig = ChunkerConfig(config_path, use_env_vars=False)\n</code></pre>"},{"location":"environment_variables/#debugging","title":"Debugging","text":"<p>To see which environment variables are being used:</p> <pre><code>from chunker.chunker_config import ChunkerConfig\n\n# Get information about supported environment variables\nenv_info = ChunkerConfig.get_env_var_info()\nfor var, description in env_info.items():\n    print(f\"{var}: {description}\")\n</code></pre>"},{"location":"environment_variables/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use descriptive variable names: When using custom environment variables in config files, use clear names like <code>${CHUNKER_WORKSPACE}</code> instead of <code>${DIR}</code></p> </li> <li> <p>Provide defaults: Always provide sensible defaults using the <code>${VAR:default}</code> syntax</p> </li> <li> <p>Document your variables: If you're using custom environment variables, document them in your project</p> </li> <li> <p>Validate values: Environment variables are strings, so numeric values are converted. Make sure to handle potential conversion errors</p> </li> <li> <p>Security: Be cautious about accepting environment variables from untrusted sources, especially in production environments</p> </li> </ol>"},{"location":"export-formats/","title":"Export Formats Guide","text":"<p>Tree-sitter Chunker supports multiple export formats to integrate with different workflows and tools. This guide covers all available formats, their use cases, and advanced configuration options.</p>"},{"location":"export-formats/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>JSON Export</li> <li>JSONL Export</li> <li>Parquet Export</li> <li>GraphML Walkthrough</li> <li>Neo4j Walkthrough</li> <li>Format Comparison</li> <li>Schema Types</li> <li>Compression Options</li> <li>Streaming Export</li> <li>Custom Export Formats</li> <li>Integration Examples</li> </ol>"},{"location":"export-formats/#overview","title":"Overview","text":"<p>Tree-sitter Chunker provides three main export formats:</p> <ul> <li>JSON: Human-readable, supports nested structures, ideal for small to medium datasets</li> <li>JSONL: Line-delimited JSON, perfect for streaming and log processing</li> <li>Parquet: Columnar format, excellent for analytics and big data workflows</li> </ul> <p>Each format supports different schema types and compression options.</p>"},{"location":"export-formats/#graphml-walkthrough","title":"GraphML Walkthrough","text":"<p>GraphML is ideal for visualizing code structure. See <code>docs/graphml_export.md</code> for full details.</p> <pre><code>from chunker.core import chunk_file\nfrom chunker.export.graphml_exporter import GraphMLExporter\n\nchunks = chunk_file(\"example.py\", \"python\")\nexporter = GraphMLExporter()\nexporter.add_chunks(chunks)\nexporter.extract_relationships(chunks)\nexporter.export(\"chunks.graphml\")\n</code></pre> <p>Open <code>chunks.graphml</code> in yEd/Gephi to explore the call/import graph.</p>"},{"location":"export-formats/#neo4j-walkthrough","title":"Neo4j Walkthrough","text":"<p>Export relationships to Neo4j for graph queries:</p> <pre><code>from chunker.core import chunk_file\nfrom chunker.export.formats.graph import Neo4jExporter\n\nchunks = chunk_file(\"example.py\", \"python\")\nexporter = Neo4jExporter(uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"pass\")\nexporter.add_chunks(chunks)\nexporter.extract_relationships(chunks)\nexporter.flush()  # Push nodes/edges to Neo4j\n</code></pre> <p>Example Cypher queries:</p> <pre><code>// Functions calling function named 'process'\nMATCH (a:Chunk)-[:CALLS]-&gt;(b:Chunk {name: 'process'}) RETURN a, b\n\n// Modules importing others\nMATCH (a:Chunk)-[:IMPORTS]-&gt;(b:Chunk) RETURN a.file_path, b.file_path LIMIT 25\n</code></pre>"},{"location":"export-formats/#json-export","title":"JSON Export","text":"<p>JSON export provides a flexible, human-readable format with support for different schema types.</p>"},{"location":"export-formats/#basic-usage","title":"Basic Usage","text":"<pre><code>from chunker import chunk_file\nfrom chunker.export import JSONExporter, SchemaType\n\n# Get chunks\nchunks = chunk_file(\"example.py\", \"python\")\n\n# Export with default settings\nexporter = JSONExporter()\nexporter.export(chunks, \"output.json\")\n\n# Export with pretty printing\nexporter.export(chunks, \"output.json\", indent=2)\n\n# Export with compression\nexporter.export(chunks, \"output.json.gz\", compress=True)\n</code></pre>"},{"location":"export-formats/#schema-types","title":"Schema Types","text":""},{"location":"export-formats/#flat-schema-default","title":"Flat Schema (Default)","text":"<p>Simple, denormalized structure with all chunk data in a flat array:</p> <pre><code>exporter = JSONExporter(schema_type=SchemaType.FLAT)\nexporter.export(chunks, \"flat.json\")\n</code></pre> <p>Output structure:</p> <pre><code>{\n  \"chunks\": [\n    {\n      \"chunk_id\": \"abc123\",\n      \"language\": \"python\",\n      \"file_path\": \"/path/to/file.py\",\n      \"node_type\": \"function_definition\",\n      \"start_line\": 10,\n      \"end_line\": 20,\n      \"parent_context\": \"class:MyClass\",\n      \"content\": \"def my_function():\\n    pass\"\n    }\n  ],\n  \"metadata\": {\n    \"total_chunks\": 42,\n    \"export_time\": \"2024-01-13T10:30:00Z\",\n    \"chunker_version\": \"1.0.0\"\n  }\n}\n</code></pre>"},{"location":"export-formats/#nested-schema","title":"Nested Schema","text":"<p>Preserves hierarchical relationships between chunks:</p> <pre><code>exporter = JSONExporter(schema_type=SchemaType.NESTED)\nexporter.export(chunks, \"nested.json\")\n</code></pre> <p>Output structure:</p> <pre><code>{\n  \"files\": {\n    \"/path/to/file.py\": {\n      \"language\": \"python\",\n      \"chunks\": [\n        {\n          \"chunk_id\": \"abc123\",\n          \"node_type\": \"class_definition\",\n          \"start_line\": 5,\n          \"end_line\": 50,\n          \"children\": [\n            {\n              \"chunk_id\": \"def456\",\n              \"node_type\": \"function_definition\",\n              \"start_line\": 10,\n              \"end_line\": 20,\n              \"content\": \"def method(self):\\n    pass\"\n            }\n          ]\n        }\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"export-formats/#relational-schema","title":"Relational Schema","text":"<p>Normalized structure suitable for relational databases:</p> <pre><code>exporter = JSONExporter(schema_type=SchemaType.RELATIONAL)\nexporter.export(chunks, \"relational.json\")\n</code></pre> <p>Output structure:</p> <pre><code>{\n  \"chunks\": [\n    {\n      \"chunk_id\": \"abc123\",\n      \"file_id\": \"file_001\",\n      \"parent_chunk_id\": null,\n      \"node_type\": \"class_definition\",\n      \"start_line\": 5,\n      \"end_line\": 50\n    }\n  ],\n  \"files\": [\n    {\n      \"file_id\": \"file_001\",\n      \"file_path\": \"/path/to/file.py\",\n      \"language\": \"python\"\n    }\n  ],\n  \"relationships\": [\n    {\n      \"parent_id\": \"abc123\",\n      \"child_id\": \"def456\",\n      \"relationship_type\": \"contains\"\n    }\n  ]\n}\n</code></pre>"},{"location":"export-formats/#advanced-json-export","title":"Advanced JSON Export","text":"<pre><code>from chunker.export import JSONExporter, SchemaType\nimport json\n\nclass CustomJSONExporter(JSONExporter):\n    \"\"\"Extended JSON exporter with custom features.\"\"\"\n\n    def export_with_metadata(self, chunks, output_path, custom_metadata=None):\n        \"\"\"Export with additional metadata.\"\"\"\n        data = self._prepare_data(chunks)\n\n        # Add custom metadata\n        if custom_metadata:\n            data['metadata'].update(custom_metadata)\n\n        # Add statistics\n        data['statistics'] = {\n            'total_chunks': len(chunks),\n            'chunks_by_type': self._count_by_type(chunks),\n            'avg_chunk_size': self._avg_chunk_size(chunks),\n            'languages': list(set(c.language for c in chunks))\n        }\n\n        # Export with custom encoder\n        with open(output_path, 'w') as f:\n            json.dump(data, f, cls=self.CustomEncoder, indent=2)\n\n    def _count_by_type(self, chunks):\n        counts = {}\n        for chunk in chunks:\n            counts[chunk.node_type] = counts.get(chunk.node_type, 0) + 1\n        return counts\n\n    def _avg_chunk_size(self, chunks):\n        if not chunks:\n            return 0\n        total_lines = sum(c.end_line - c.start_line + 1 for c in chunks)\n        return total_lines / len(chunks)\n\n    class CustomEncoder(json.JSONEncoder):\n        \"\"\"Handle special types.\"\"\"\n        def default(self, obj):\n            if hasattr(obj, 'isoformat'):\n                return obj.isoformat()\n            return super().default(obj)\n</code></pre>"},{"location":"export-formats/#jsonl-export","title":"JSONL Export","text":"<p>JSON Lines format is ideal for streaming, logging, and processing large datasets.</p>"},{"location":"export-formats/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from chunker.export import JSONLExporter\n\n# Export chunks to JSONL\nexporter = JSONLExporter()\nexporter.export(chunks, \"output.jsonl\")\n\n# Export with compression\nexporter.export(chunks, \"output.jsonl.gz\", compress=True)\n</code></pre>"},{"location":"export-formats/#streaming-export","title":"Streaming Export","text":"<p>Process and export large datasets without loading everything into memory:</p> <pre><code>from chunker import chunk_file_streaming\nfrom chunker.export import JSONLExporter\n\ndef export_large_codebase(directory, language):\n    \"\"\"Export large codebase using streaming.\"\"\"\n    exporter = JSONLExporter()\n\n    # Create generator for all chunks\n    def chunk_generator():\n        from pathlib import Path\n        for file_path in Path(directory).rglob(f\"*.{language[:2]}\"):\n            for chunk in chunk_file_streaming(file_path, language):\n                yield chunk\n\n    # Stream export\n    exporter.export_streaming(\n        chunk_generator(),\n        \"large_export.jsonl\",\n        compress=True\n    )\n\n# Use it\nexport_large_codebase(\"src/\", \"python\")\n</code></pre>"},{"location":"export-formats/#jsonl-with-filtering","title":"JSONL with Filtering","text":"<pre><code>class FilteredJSONLExporter(JSONLExporter):\n    \"\"\"Export only chunks matching criteria.\"\"\"\n\n    def export_filtered(self, chunks, output_path, filter_func):\n        \"\"\"Export only chunks that pass filter.\"\"\"\n        filtered = (chunk for chunk in chunks if filter_func(chunk))\n        self.export_streaming(filtered, output_path)\n\n# Example: Export only large functions\nexporter = FilteredJSONLExporter()\nexporter.export_filtered(\n    chunks,\n    \"large_functions.jsonl\",\n    lambda c: c.node_type == \"function_definition\" and \n              (c.end_line - c.start_line) &gt; 50\n)\n</code></pre>"},{"location":"export-formats/#processing-jsonl-files","title":"Processing JSONL Files","text":"<pre><code>import json\n\ndef process_jsonl_file(file_path):\n    \"\"\"Read and process JSONL file.\"\"\"\n    chunks = []\n\n    with open(file_path, 'r') as f:\n        for line in f:\n            if line.strip():\n                chunk_data = json.loads(line)\n                chunks.append(chunk_data)\n\n    return chunks\n\n# Streaming processing\ndef stream_process_jsonl(file_path, processor_func):\n    \"\"\"Process JSONL file line by line.\"\"\"\n    with open(file_path, 'r') as f:\n        for line in f:\n            if line.strip():\n                chunk_data = json.loads(line)\n                processor_func(chunk_data)\n</code></pre>"},{"location":"export-formats/#parquet-export","title":"Parquet Export","text":"<p>Parquet is a columnar storage format that's highly efficient for analytics workloads.</p>"},{"location":"export-formats/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from chunker.exporters import ParquetExporter\n\n# Basic export\nexporter = ParquetExporter()\nexporter.export(chunks, \"output.parquet\")\n\n# Export with specific columns\nexporter = ParquetExporter(\n    columns=[\"language\", \"file_path\", \"node_type\", \"content\", \"start_line\", \"end_line\"]\n)\nexporter.export(chunks, \"output.parquet\")\n</code></pre>"},{"location":"export-formats/#compression-options","title":"Compression Options","text":"<p>Parquet supports multiple compression algorithms:</p> <pre><code># Snappy (default) - Fast compression\nexporter = ParquetExporter(compression=\"snappy\")\n\n# Gzip - Higher compression ratio\nexporter = ParquetExporter(compression=\"gzip\")\n\n# Brotli - Best compression ratio\nexporter = ParquetExporter(compression=\"brotli\")\n\n# LZ4 - Fastest compression\nexporter = ParquetExporter(compression=\"lz4\")\n\n# Zstd - Good balance\nexporter = ParquetExporter(compression=\"zstd\")\n\n# No compression\nexporter = ParquetExporter(compression=None)\n</code></pre>"},{"location":"export-formats/#partitioned-export","title":"Partitioned Export","text":"<p>Partition data for efficient querying:</p> <pre><code># Partition by language and node type\nexporter = ParquetExporter()\nexporter.export_partitioned(\n    chunks,\n    \"output_dir/\",\n    partition_cols=[\"language\", \"node_type\"]\n)\n\n# Creates directory structure:\n# output_dir/\n#   language=python/\n#     node_type=function_definition/\n#       part-0.parquet\n#     node_type=class_definition/\n#       part-0.parquet\n#   language=javascript/\n#     node_type=function_declaration/\n#       part-0.parquet\n</code></pre>"},{"location":"export-formats/#advanced-parquet-features","title":"Advanced Parquet Features","text":"<pre><code>import pyarrow as pa\nimport pyarrow.parquet as pq\nfrom chunker.exporters import ParquetExporter\n\nclass AdvancedParquetExporter(ParquetExporter):\n    \"\"\"Extended Parquet exporter with advanced features.\"\"\"\n\n    def export_with_schema(self, chunks, output_path, custom_schema=None):\n        \"\"\"Export with custom schema.\"\"\"\n        # Convert chunks to records\n        records = [self._chunk_to_record(chunk) for chunk in chunks]\n\n        # Define schema if not provided\n        if custom_schema is None:\n            custom_schema = pa.schema([\n                ('chunk_id', pa.string()),\n                ('language', pa.string()),\n                ('file_path', pa.string()),\n                ('node_type', pa.string()),\n                ('start_line', pa.int32()),\n                ('end_line', pa.int32()),\n                ('byte_start', pa.int64()),\n                ('byte_end', pa.int64()),\n                ('content', pa.string()),\n                ('parent_context', pa.string()),\n                ('metadata', pa.string()),  # JSON string\n            ])\n\n        # Create table with schema\n        table = pa.Table.from_pylist(records, schema=custom_schema)\n\n        # Write with options\n        pq.write_table(\n            table,\n            output_path,\n            compression=self.compression,\n            use_dictionary=['language', 'node_type'],  # Dictionary encoding\n            row_group_size=5000,\n            data_page_size=1024*1024,  # 1MB pages\n            version='2.6'  # Latest format\n        )\n\n    def export_with_statistics(self, chunks, output_path):\n        \"\"\"Export with column statistics for query optimization.\"\"\"\n        records = [self._chunk_to_record(chunk) for chunk in chunks]\n        table = pa.Table.from_pylist(records)\n\n        # Write with statistics\n        pq.write_table(\n            table,\n            output_path,\n            compression=self.compression,\n            write_statistics=True,\n            column_encoding={'content': 'PLAIN'},  # No dictionary for content\n        )\n</code></pre>"},{"location":"export-formats/#reading-parquet-files","title":"Reading Parquet Files","text":"<pre><code>import pyarrow.parquet as pq\nimport pandas as pd\n\n# Read with PyArrow\ndef read_parquet_pyarrow(file_path):\n    \"\"\"Read Parquet file using PyArrow.\"\"\"\n    table = pq.read_table(file_path)\n\n    # Filter example\n    filtered = table.filter(\n        (table['node_type'] == 'function_definition') &amp; \n        (table['end_line'] - table['start_line'] &gt; 50)\n    )\n\n    return filtered.to_pylist()\n\n# Read with Pandas\ndef read_parquet_pandas(file_path):\n    \"\"\"Read Parquet file using Pandas.\"\"\"\n    df = pd.read_parquet(\n        file_path,\n        columns=['file_path', 'node_type', 'start_line', 'end_line']\n    )\n\n    # Analysis example\n    print(df['node_type'].value_counts())\n    return df\n\n# Read partitioned dataset\ndef read_partitioned_dataset(directory):\n    \"\"\"Read partitioned Parquet dataset.\"\"\"\n    dataset = pq.ParquetDataset(directory)\n\n    # Read with filters\n    table = dataset.read(\n        filters=[\n            ('language', '=', 'python'),\n            ('node_type', 'in', ['function_definition', 'class_definition'])\n        ]\n    )\n\n    return table.to_pandas()\n</code></pre>"},{"location":"export-formats/#format-comparison","title":"Format Comparison","text":""},{"location":"export-formats/#performance-comparison","title":"Performance Comparison","text":"Format Write Speed Read Speed Compression File Size JSON Medium Medium Optional Large JSONL Fast Fast Optional Large Parquet Slow Very Fast Built-in Small"},{"location":"export-formats/#feature-comparison","title":"Feature Comparison","text":"Feature JSON JSONL Parquet Human Readable \u2705 \u2705 \u274c Streaming \u274c \u2705 \u2705 Schema Evolution \u2705 \u2705 Limited Query Performance Poor Medium Excellent Compression External External Built-in Partial Reads \u274c \u2705 \u2705 Column Selection \u274c \u274c \u2705"},{"location":"export-formats/#use-case-recommendations","title":"Use Case Recommendations","text":"<p>Use JSON when: - Human readability is important - Working with small to medium datasets - Integrating with web APIs - Need flexible, nested structures</p> <p>Use JSONL when: - Processing streaming data - Working with log files - Need line-by-line processing - Building data pipelines</p> <p>Use Parquet when: - Working with large datasets - Performing analytics queries - Need efficient storage - Using data science tools</p>"},{"location":"export-formats/#schema-types_1","title":"Schema Types","text":""},{"location":"export-formats/#flat-schema","title":"Flat Schema","text":"<p>Best for simple use cases and direct database imports:</p> <pre><code># Flat schema example\n{\n    \"chunks\": [\n        {\n            \"chunk_id\": \"...\",\n            \"all_fields\": \"...\"\n        }\n    ]\n}\n</code></pre>"},{"location":"export-formats/#nested-schema_1","title":"Nested Schema","text":"<p>Preserves relationships and hierarchy:</p> <pre><code># Nested schema example\n{\n    \"files\": {\n        \"path\": {\n            \"chunks\": [\n                {\n                    \"chunk\": \"...\",\n                    \"children\": [...]\n                }\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"export-formats/#relational-schema_1","title":"Relational Schema","text":"<p>Normalized for relational databases:</p> <pre><code># Relational schema example\n{\n    \"chunks\": [...],\n    \"files\": [...],\n    \"relationships\": [...]\n}\n</code></pre>"},{"location":"export-formats/#compression-options_1","title":"Compression Options","text":""},{"location":"export-formats/#compression-comparison","title":"Compression Comparison","text":"Algorithm Compression Ratio Speed Use Case None 1:1 Fastest Local processing Snappy 2-4:1 Very Fast Default choice LZ4 2-4:1 Fastest Speed critical Gzip 5-10:1 Slow Network transfer Zstd 5-15:1 Medium Balanced Brotli 10-20:1 Very Slow Storage critical"},{"location":"export-formats/#compression-examples","title":"Compression Examples","text":"<pre><code># JSON with gzip\nfrom chunker.export import JSONExporter\nexporter = JSONExporter()\nexporter.export(chunks, \"output.json.gz\", compress=True)\n\n# JSONL with custom compression\nimport gzip\nimport json\n\ndef export_jsonl_compressed(chunks, output_path):\n    with gzip.open(output_path, 'wt', encoding='utf-8') as f:\n        for chunk in chunks:\n            json.dump(chunk.__dict__, f)\n            f.write('\\n')\n\n# Parquet with zstd\nfrom chunker.exporters import ParquetExporter\nexporter = ParquetExporter(compression=\"zstd\")\nexporter.export(chunks, \"output.parquet\")\n</code></pre>"},{"location":"export-formats/#streaming-export_1","title":"Streaming Export","text":""},{"location":"export-formats/#memory-efficient-export","title":"Memory-Efficient Export","text":"<pre><code>from chunker import chunk_file_streaming\nfrom chunker.export import JSONLExporter\nfrom chunker.exporters import ParquetExporter\n\nclass StreamingExporter:\n    \"\"\"Memory-efficient export for large codebases.\"\"\"\n\n    def export_directory_streaming(self, directory, language, output_format=\"jsonl\"):\n        \"\"\"Export entire directory with minimal memory usage.\"\"\"\n        from pathlib import Path\n\n        def chunk_generator():\n            for file_path in Path(directory).rglob(f\"*.{language[:2]}\"):\n                print(f\"Processing {file_path}\")\n                for chunk in chunk_file_streaming(file_path, language):\n                    yield chunk\n\n        if output_format == \"jsonl\":\n            exporter = JSONLExporter()\n            exporter.export_streaming(\n                chunk_generator(),\n                f\"export.{output_format}\",\n                compress=True\n            )\n        elif output_format == \"parquet\":\n            exporter = ParquetExporter()\n            exporter.export_streaming(\n                chunk_generator(),\n                f\"export.{output_format}\",\n                batch_size=10000  # Write in batches\n            )\n</code></pre>"},{"location":"export-formats/#parallel-streaming-export","title":"Parallel Streaming Export","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\nimport queue\nimport threading\n\nclass ParallelStreamingExporter:\n    \"\"\"Export with parallel processing and streaming.\"\"\"\n\n    def __init__(self, num_workers=4):\n        self.num_workers = num_workers\n        self.chunk_queue = queue.Queue(maxsize=1000)\n        self.done = threading.Event()\n\n    def export_parallel(self, files, language, output_path):\n        \"\"\"Process files in parallel, export in streaming fashion.\"\"\"\n        # Start export thread\n        export_thread = threading.Thread(\n            target=self._export_worker,\n            args=(output_path,)\n        )\n        export_thread.start()\n\n        # Process files in parallel\n        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n            futures = []\n            for file_path in files:\n                future = executor.submit(self._process_file, file_path, language)\n                futures.append(future)\n\n            # Wait for all processing to complete\n            for future in futures:\n                future.result()\n\n        # Signal completion and wait for export to finish\n        self.done.set()\n        export_thread.join()\n\n    def _process_file(self, file_path, language):\n        \"\"\"Process a single file and queue chunks.\"\"\"\n        for chunk in chunk_file_streaming(file_path, language):\n            self.chunk_queue.put(chunk)\n\n    def _export_worker(self, output_path):\n        \"\"\"Export chunks from queue.\"\"\"\n        from chunker.export import JSONLExporter\n        exporter = JSONLExporter()\n\n        with open(output_path, 'w') as f:\n            while not self.done.is_set() or not self.chunk_queue.empty():\n                try:\n                    chunk = self.chunk_queue.get(timeout=0.1)\n                    exporter._write_chunk(chunk, f)\n                except queue.Empty:\n                    continue\n</code></pre>"},{"location":"export-formats/#custom-export-formats","title":"Custom Export Formats","text":""},{"location":"export-formats/#csv-export","title":"CSV Export","text":"<pre><code>import csv\nfrom typing import List\nfrom chunker import CodeChunk\n\nclass CSVExporter:\n    \"\"\"Export chunks to CSV format.\"\"\"\n\n    def export(self, chunks: List[CodeChunk], output_path: str):\n        \"\"\"Export chunks to CSV.\"\"\"\n        with open(output_path, 'w', newline='', encoding='utf-8') as f:\n            fieldnames = [\n                'chunk_id', 'language', 'file_path', 'node_type',\n                'start_line', 'end_line', 'parent_context'\n            ]\n\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n            writer.writeheader()\n\n            for chunk in chunks:\n                writer.writerow({\n                    'chunk_id': chunk.chunk_id,\n                    'language': chunk.language,\n                    'file_path': chunk.file_path,\n                    'node_type': chunk.node_type,\n                    'start_line': chunk.start_line,\n                    'end_line': chunk.end_line,\n                    'parent_context': chunk.parent_context or ''\n                })\n</code></pre>"},{"location":"export-formats/#xml-export","title":"XML Export","text":"<pre><code>import xml.etree.ElementTree as ET\nfrom xml.dom import minidom\n\nclass XMLExporter:\n    \"\"\"Export chunks to XML format.\"\"\"\n\n    def export(self, chunks: List[CodeChunk], output_path: str):\n        \"\"\"Export chunks to XML.\"\"\"\n        root = ET.Element('chunks')\n\n        for chunk in chunks:\n            chunk_elem = ET.SubElement(root, 'chunk')\n            chunk_elem.set('id', chunk.chunk_id)\n\n            # Add child elements\n            ET.SubElement(chunk_elem, 'language').text = chunk.language\n            ET.SubElement(chunk_elem, 'file_path').text = chunk.file_path\n            ET.SubElement(chunk_elem, 'node_type').text = chunk.node_type\n            ET.SubElement(chunk_elem, 'start_line').text = str(chunk.start_line)\n            ET.SubElement(chunk_elem, 'end_line').text = str(chunk.end_line)\n\n            # Add content as CDATA\n            content_elem = ET.SubElement(chunk_elem, 'content')\n            content_elem.text = chunk.content\n\n        # Pretty print\n        xml_str = minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"  \")\n\n        with open(output_path, 'w', encoding='utf-8') as f:\n            f.write(xml_str)\n</code></pre>"},{"location":"export-formats/#sqlite-export","title":"SQLite Export","text":"<pre><code>import sqlite3\nfrom typing import List\nfrom chunker import CodeChunk\n\nclass SQLiteExporter:\n    \"\"\"Export chunks to SQLite database.\"\"\"\n\n    def export(self, chunks: List[CodeChunk], db_path: str):\n        \"\"\"Export chunks to SQLite.\"\"\"\n        conn = sqlite3.connect(db_path)\n        cursor = conn.cursor()\n\n        # Create tables\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS chunks (\n                chunk_id TEXT PRIMARY KEY,\n                language TEXT,\n                file_path TEXT,\n                node_type TEXT,\n                start_line INTEGER,\n                end_line INTEGER,\n                byte_start INTEGER,\n                byte_end INTEGER,\n                parent_context TEXT,\n                content TEXT\n            )\n        ''')\n\n        cursor.execute('''\n            CREATE INDEX IF NOT EXISTS idx_file_path ON chunks(file_path)\n        ''')\n\n        cursor.execute('''\n            CREATE INDEX IF NOT EXISTS idx_node_type ON chunks(node_type)\n        ''')\n\n        # Insert chunks\n        for chunk in chunks:\n            cursor.execute('''\n                INSERT OR REPLACE INTO chunks VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            ''', (\n                chunk.chunk_id,\n                chunk.language,\n                chunk.file_path,\n                chunk.node_type,\n                chunk.start_line,\n                chunk.end_line,\n                chunk.byte_start,\n                chunk.byte_end,\n                chunk.parent_context,\n                chunk.content\n            ))\n\n        conn.commit()\n        conn.close()\n</code></pre>"},{"location":"export-formats/#integration-examples","title":"Integration Examples","text":""},{"location":"export-formats/#elasticsearch-integration","title":"Elasticsearch Integration","text":"<pre><code>from elasticsearch import Elasticsearch, helpers\nfrom chunker import chunk_directory_parallel\n\ndef index_to_elasticsearch(directory, language, es_host=\"localhost:9200\"):\n    \"\"\"Index chunks to Elasticsearch.\"\"\"\n    es = Elasticsearch([es_host])\n\n    # Create index with mapping\n    es.indices.create(\n        index=\"code_chunks\",\n        body={\n            \"mappings\": {\n                \"properties\": {\n                    \"chunk_id\": {\"type\": \"keyword\"},\n                    \"language\": {\"type\": \"keyword\"},\n                    \"file_path\": {\"type\": \"keyword\"},\n                    \"node_type\": {\"type\": \"keyword\"},\n                    \"content\": {\"type\": \"text\"},\n                    \"start_line\": {\"type\": \"integer\"},\n                    \"end_line\": {\"type\": \"integer\"}\n                }\n            }\n        },\n        ignore=400  # Ignore if exists\n    )\n\n    # Process chunks\n    results = chunk_directory_parallel(directory, language)\n\n    # Prepare bulk actions\n    actions = []\n    for file_path, chunks in results.items():\n        for chunk in chunks:\n            actions.append({\n                \"_index\": \"code_chunks\",\n                \"_id\": chunk.chunk_id,\n                \"_source\": {\n                    \"chunk_id\": chunk.chunk_id,\n                    \"language\": chunk.language,\n                    \"file_path\": chunk.file_path,\n                    \"node_type\": chunk.node_type,\n                    \"content\": chunk.content,\n                    \"start_line\": chunk.start_line,\n                    \"end_line\": chunk.end_line\n                }\n            })\n\n    # Bulk index\n    helpers.bulk(es, actions)\n    print(f\"Indexed {len(actions)} chunks\")\n</code></pre>"},{"location":"export-formats/#vector-database-integration","title":"Vector Database Integration","text":"<pre><code>import chromadb\nfrom chunker import chunk_file\n\ndef export_to_chroma(chunks, collection_name=\"code_chunks\"):\n    \"\"\"Export chunks to ChromaDB for semantic search.\"\"\"\n    # Initialize ChromaDB\n    client = chromadb.Client()\n    collection = client.create_collection(\n        name=collection_name,\n        metadata={\"hnsw:space\": \"cosine\"}\n    )\n\n    # Prepare data\n    documents = []\n    metadatas = []\n    ids = []\n\n    for chunk in chunks:\n        # Create searchable document\n        doc = f\"{chunk.node_type} in {chunk.file_path}\\n{chunk.content}\"\n        documents.append(doc)\n\n        # Metadata for filtering\n        metadatas.append({\n            \"language\": chunk.language,\n            \"file_path\": chunk.file_path,\n            \"node_type\": chunk.node_type,\n            \"start_line\": chunk.start_line,\n            \"end_line\": chunk.end_line\n        })\n\n        ids.append(chunk.chunk_id)\n\n    # Add to collection\n    collection.add(\n        documents=documents,\n        metadatas=metadatas,\n        ids=ids\n    )\n\n    return collection\n</code></pre>"},{"location":"export-formats/#data-pipeline-integration","title":"Data Pipeline Integration","text":"<pre><code>from chunker import chunk_file_streaming\nfrom chunker.export import JSONLExporter\nimport apache_beam as beam\n\nclass ChunkToDict(beam.DoFn):\n    \"\"\"Convert chunk to dictionary for Beam.\"\"\"\n    def process(self, chunk):\n        yield {\n            'chunk_id': chunk.chunk_id,\n            'language': chunk.language,\n            'file_path': chunk.file_path,\n            'node_type': chunk.node_type,\n            'content': chunk.content,\n            'metrics': {\n                'lines': chunk.end_line - chunk.start_line + 1,\n                'bytes': chunk.byte_end - chunk.byte_start\n            }\n        }\n\ndef create_beam_pipeline(input_files, language):\n    \"\"\"Create Apache Beam pipeline for chunk processing.\"\"\"\n    with beam.Pipeline() as p:\n        chunks = (\n            p\n            | 'Read Files' &gt;&gt; beam.Create(input_files)\n            | 'Extract Chunks' &gt;&gt; beam.FlatMap(\n                lambda f: chunk_file_streaming(f, language)\n            )\n            | 'Convert to Dict' &gt;&gt; beam.ParDo(ChunkToDict())\n            | 'Write to Parquet' &gt;&gt; beam.io.WriteToParquet(\n                'output/chunks',\n                schema=beam.io.parquetio.FastAvroSchema({\n                    'chunk_id': 'string',\n                    'language': 'string',\n                    'file_path': 'string',\n                    'node_type': 'string',\n                    'content': 'string',\n                    'metrics': {\n                        'lines': 'int',\n                        'bytes': 'long'\n                    }\n                })\n            )\n        )\n</code></pre>"},{"location":"export-formats/#see-also","title":"See Also","text":"<ul> <li>API Reference - Export API documentation</li> <li>User Guide - Basic export examples</li> <li>Performance Guide - Export performance tips</li> <li>Configuration - Export configuration options</li> </ul>"},{"location":"getting-started/","title":"Getting Started with Tree-sitter Chunker","text":"<p>Welcome to Tree-sitter Chunker! This tutorial will guide you through installation, basic usage, and your first code chunking project. By the end, you'll be confidently chunking code and building useful tools.</p>"},{"location":"getting-started/#what-is-tree-sitter-chunker","title":"What is Tree-sitter Chunker?","text":"<p>Tree-sitter Chunker intelligently splits source code into semantic chunks like functions, classes, and methods. Unlike simple line-based splitting, it understands code structure, making it perfect for:</p> <ul> <li>Building code search systems</li> <li>Creating embeddings for AI/ML applications</li> <li>Generating documentation</li> <li>Analyzing code structure and complexity</li> </ul>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>Basic command line familiarity</li> <li>A C compiler (for building grammars)</li> <li>Git (for fetching grammar repositories)</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#step-1-set-up-environment","title":"Step 1: Set Up Environment","text":"<p>We recommend using <code>uv</code> for package management:</p> <pre><code># Install uv (if not already installed)\npip install uv\n\n# Create a new project directory\nmkdir my-chunker-project\ncd my-chunker-project\n\n# Create virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/#step-2-install-tree-sitter-chunker","title":"Step 2: Install Tree-sitter Chunker","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourusername/treesitter-chunker.git\ncd treesitter-chunker\n\n# Install in development mode\nuv pip install -e \".[dev]\"\n\n# Install py-tree-sitter with ABI 15 support\nuv pip install git+https://github.com/tree-sitter/py-tree-sitter.git\n</code></pre>"},{"location":"getting-started/#step-3-build-language-support","title":"Step 3: Build Language Support","text":"<pre><code># Fetch grammar repositories\npython scripts/fetch_grammars.py\n\n# Compile grammars (this takes a minute)\npython scripts/build_lib.py\n\n# Verify installation\npython -c \"from chunker.parser import list_languages; print(list_languages())\"\n# Should output: ['c', 'cpp', 'javascript', 'python', 'rust']\n</code></pre>"},{"location":"getting-started/#your-first-chunking-project","title":"Your First Chunking Project","text":""},{"location":"getting-started/#step-1-create-sample-code","title":"Step 1: Create Sample Code","text":"<p>Let's create a Python file to analyze:</p> <pre><code># save as example.py\n\"\"\"Example module for demonstrating code chunking.\"\"\"\n\nclass DataProcessor:\n    \"\"\"Process data with various transformations.\"\"\"\n\n    def __init__(self, data):\n        self.data = data\n        self.results = []\n\n    def clean_data(self):\n        \"\"\"Remove invalid entries from data.\"\"\"\n        self.data = [item for item in self.data if self.validate(item)]\n        return self.data\n\n    def validate(self, item):\n        \"\"\"Check if an item is valid.\"\"\"\n        return item is not None and len(str(item)) &gt; 0\n\n    def transform(self, func):\n        \"\"\"Apply a transformation function to all data.\"\"\"\n        self.results = [func(item) for item in self.data]\n        return self.results\n\ndef process_numbers(numbers):\n    \"\"\"Process a list of numbers.\"\"\"\n    processor = DataProcessor(numbers)\n    processor.clean_data()\n    return processor.transform(lambda x: x * 2)\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    test_data = [1, 2, None, 4, 5, \"\"]\n    result = process_numbers(test_data)\n    print(f\"Result: {result}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"getting-started/#step-2-chunk-with-cli","title":"Step 2: Chunk with CLI","text":"<pre><code># Basic chunking - see the structure\npython cli/main.py chunk example.py -l python\n\n# Output:\n# \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n# \u2503 Chunk# \u2503 Node Type            \u2503 Lines    \u2503 Parent Context       \u2503\n# \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n# \u2502 1      \u2502 class_definition     \u2502 3-21     \u2502                      \u2502\n# \u2502 2      \u2502 function_definition  \u2502 6-8      \u2502 class:DataProcessor  \u2502\n# \u2502 3      \u2502 function_definition  \u2502 10-13    \u2502 class:DataProcessor  \u2502\n# \u2502 4      \u2502 function_definition  \u2502 15-17    \u2502 class:DataProcessor  \u2502\n# \u2502 5      \u2502 function_definition  \u2502 19-22    \u2502 class:DataProcessor  \u2502\n# \u2502 6      \u2502 function_definition  \u2502 24-28    \u2502                      \u2502\n# \u2502 7      \u2502 function_definition  \u2502 30-34    \u2502                      \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Get JSON output for programmatic use\npython cli/main.py chunk example.py -l python --json &gt; chunks.json\n</code></pre>"},{"location":"getting-started/#step-3-use-the-python-api","title":"Step 3: Use the Python API","text":"<p>Create a Python script to analyze the chunks:</p> <pre><code># save as analyze_chunks.py\nfrom chunker.chunker import chunk_file\nfrom chunker.parser import list_languages, get_language_info\n\n# Check available languages\nprint(\"Available languages:\", list_languages())\n\n# Get language info\ninfo = get_language_info(\"python\")\nprint(f\"\\nPython language info:\")\nprint(f\"  ABI Version: {info.version}\")\nprint(f\"  Node types: {info.node_types_count}\")\nprint(f\"  Has scanner: {info.has_scanner}\")\n\n# Chunk the file\nchunks = chunk_file(\"example.py\", \"python\")\n\nprint(f\"\\nFound {len(chunks)} chunks:\")\nprint(\"-\" * 50)\n\nfor i, chunk in enumerate(chunks, 1):\n    print(f\"\\nChunk {i}: {chunk.node_type}\")\n    print(f\"  Location: lines {chunk.start_line}-{chunk.end_line}\")\n    print(f\"  Parent: {chunk.parent_context or 'module level'}\")\n\n    # Show first line of content\n    first_line = chunk.content.split('\\n')[0]\n    print(f\"  Signature: {first_line}\")\n\n    # Extract docstring if present\n    lines = chunk.content.split('\\n')\n    for line in lines[1:3]:\n        if '\"\"\"' in line:\n            print(f\"  Docstring: {line.strip()}\")\n            break\n</code></pre> <p>Run it:</p> <pre><code>python analyze_chunks.py\n</code></pre>"},{"location":"getting-started/#building-useful-tools","title":"Building Useful Tools","text":""},{"location":"getting-started/#tool-1-function-extractor","title":"Tool 1: Function Extractor","text":"<p>Extract all functions with their metadata:</p> <pre><code># save as extract_functions.py\nfrom chunker.chunker import chunk_file\nfrom pathlib import Path\nimport json\n\ndef extract_functions(file_path, language):\n    \"\"\"Extract all functions from a source file.\"\"\"\n    chunks = chunk_file(file_path, language)\n\n    functions = []\n    for chunk in chunks:\n        if \"function\" in chunk.node_type or \"method\" in chunk.node_type:\n            # Extract function name from first line\n            first_line = chunk.content.split('\\n')[0]\n\n            # Extract docstring\n            docstring = None\n            lines = chunk.content.split('\\n')\n            for i, line in enumerate(lines[1:], 1):\n                if '\"\"\"' in line or \"'''\" in line:\n                    # Simple single-line docstring\n                    if line.count('\"\"\"') == 2 or line.count(\"'''\") == 2:\n                        docstring = line.strip().strip('\"\"\"').strip(\"'''\")\n                    break\n\n            functions.append({\n                \"name\": first_line.strip(),\n                \"type\": chunk.node_type,\n                \"file\": chunk.file_path,\n                \"lines\": [chunk.start_line, chunk.end_line],\n                \"parent\": chunk.parent_context,\n                \"docstring\": docstring,\n                \"size\": chunk.end_line - chunk.start_line + 1\n            })\n\n    return functions\n\n# Extract from our example\nfunctions = extract_functions(\"example.py\", \"python\")\n\nprint(\"Functions found:\")\nprint(json.dumps(functions, indent=2))\n\n# Find complex functions\ncomplex_functions = [f for f in functions if f[\"size\"] &gt; 10]\nif complex_functions:\n    print(\"\\nComplex functions (&gt;10 lines):\")\n    for f in complex_functions:\n        print(f\"  - {f['name']} ({f['size']} lines)\")\n</code></pre>"},{"location":"getting-started/#tool-2-code-structure-analyzer","title":"Tool 2: Code Structure Analyzer","text":"<p>Analyze the structure of your codebase:</p> <pre><code># save as analyze_structure.py\nfrom chunker.chunker import chunk_file\nfrom collections import defaultdict\nimport statistics\n\ndef analyze_file_structure(file_path, language):\n    \"\"\"Analyze code structure and complexity.\"\"\"\n    chunks = chunk_file(file_path, language)\n\n    # Collect metrics\n    metrics = {\n        \"total_chunks\": len(chunks),\n        \"by_type\": defaultdict(int),\n        \"sizes\": [],\n        \"nesting_levels\": defaultdict(list),\n        \"top_level\": 0,\n        \"nested\": 0\n    }\n\n    for chunk in chunks:\n        # Count by type\n        metrics[\"by_type\"][chunk.node_type] += 1\n\n        # Track sizes\n        size = chunk.end_line - chunk.start_line + 1\n        metrics[\"sizes\"].append(size)\n\n        # Track nesting\n        if chunk.parent_context:\n            metrics[\"nested\"] += 1\n            parent_type = chunk.parent_context.split(':')[0]\n            metrics[\"nesting_levels\"][parent_type].append(chunk.node_type)\n        else:\n            metrics[\"top_level\"] += 1\n\n    # Calculate statistics\n    if metrics[\"sizes\"]:\n        metrics[\"avg_size\"] = statistics.mean(metrics[\"sizes\"])\n        metrics[\"median_size\"] = statistics.median(metrics[\"sizes\"])\n        metrics[\"max_size\"] = max(metrics[\"sizes\"])\n        metrics[\"min_size\"] = min(metrics[\"sizes\"])\n\n    return metrics\n\n# Analyze our example\nmetrics = analyze_file_structure(\"example.py\", \"python\")\n\nprint(\"Code Structure Analysis\")\nprint(\"=\" * 40)\nprint(f\"Total chunks: {metrics['total_chunks']}\")\nprint(f\"Top-level: {metrics['top_level']}\")\nprint(f\"Nested: {metrics['nested']}\")\n\nprint(\"\\nChunk types:\")\nfor chunk_type, count in metrics[\"by_type\"].items():\n    print(f\"  {chunk_type}: {count}\")\n\nprint(\"\\nSize statistics:\")\nprint(f\"  Average: {metrics.get('avg_size', 0):.1f} lines\")\nprint(f\"  Median: {metrics.get('median_size', 0):.1f} lines\")\nprint(f\"  Range: {metrics.get('min_size', 0)}-{metrics.get('max_size', 0)} lines\")\n\nprint(\"\\nNesting:\")\nfor parent, children in metrics[\"nesting_levels\"].items():\n    print(f\"  {parent} contains: {len(children)} nested chunks\")\n</code></pre>"},{"location":"getting-started/#tool-3-multi-file-code-index","title":"Tool 3: Multi-File Code Index","text":"<p>Build a searchable index across multiple files:</p> <pre><code># save as build_index.py\nfrom chunker.chunker import chunk_file\nfrom chunker.exceptions import LanguageNotFoundError\nfrom pathlib import Path\nimport json\n\nclass CodeIndex:\n    \"\"\"Build and search a code index.\"\"\"\n\n    def __init__(self):\n        self.index = []\n        self.language_map = {\n            '.py': 'python',\n            '.js': 'javascript',\n            '.rs': 'rust',\n            '.c': 'c',\n            '.cpp': 'cpp',\n            '.cc': 'cpp',\n            '.h': 'c',\n            '.hpp': 'cpp'\n        }\n\n    def add_file(self, file_path):\n        \"\"\"Add a file to the index.\"\"\"\n        path = Path(file_path)\n        ext = path.suffix.lower()\n\n        if ext not in self.language_map:\n            return False\n\n        language = self.language_map[ext]\n\n        try:\n            chunks = chunk_file(str(path), language)\n\n            for chunk in chunks:\n                # Extract identifier from first line\n                first_line = chunk.content.split('\\n')[0]\n\n                entry = {\n                    'file': str(path),\n                    'language': language,\n                    'type': chunk.node_type,\n                    'identifier': self._extract_identifier(first_line, language),\n                    'signature': first_line.strip(),\n                    'lines': [chunk.start_line, chunk.end_line],\n                    'parent': chunk.parent_context,\n                    'size': chunk.end_line - chunk.start_line + 1\n                }\n                self.index.append(entry)\n\n            return True\n\n        except LanguageNotFoundError:\n            print(f\"Language not supported for {file_path}\")\n            return False\n        except Exception as e:\n            print(f\"Error processing {file_path}: {e}\")\n            return False\n\n    def _extract_identifier(self, line, language):\n        \"\"\"Extract function/class name from first line.\"\"\"\n        import re\n\n        patterns = {\n            'python': r'(?:def|class)\\s+(\\w+)',\n            'javascript': r'(?:function|class|const|let|var)\\s+(\\w+)',\n            'rust': r'(?:fn|struct|impl|trait)\\s+(\\w+)',\n            'c': r'(?:\\w+\\s+)?(\\w+)\\s*\\(',\n            'cpp': r'(?:class|struct|(?:\\w+\\s+)?(\\w+)\\s*\\()'\n        }\n\n        pattern = patterns.get(language, r'(\\w+)')\n        match = re.search(pattern, line)\n        return match.group(1) if match else line.split()[0]\n\n    def add_directory(self, directory, recursive=True):\n        \"\"\"Add all supported files in a directory.\"\"\"\n        path = Path(directory)\n        pattern = '**/*' if recursive else '*'\n\n        files_added = 0\n        for file_path in path.glob(pattern):\n            if file_path.is_file() and self.add_file(file_path):\n                files_added += 1\n\n        return files_added\n\n    def search(self, query, search_in='identifier'):\n        \"\"\"Search the index.\"\"\"\n        results = []\n        query_lower = query.lower()\n\n        for entry in self.index:\n            if query_lower in entry[search_in].lower():\n                results.append(entry)\n\n        return results\n\n    def save(self, filename):\n        \"\"\"Save index to JSON file.\"\"\"\n        with open(filename, 'w') as f:\n            json.dump(self.index, f, indent=2)\n\n    def load(self, filename):\n        \"\"\"Load index from JSON file.\"\"\"\n        with open(filename, 'r') as f:\n            self.index = json.load(f)\n\n# Build index\nindex = CodeIndex()\n\n# Add current directory\nfiles_added = index.add_directory(\".\", recursive=False)\nprint(f\"Added {files_added} files to index\")\nprint(f\"Total chunks indexed: {len(index.index)}\")\n\n# Search examples\nprint(\"\\nSearch results for 'process':\")\nresults = index.search(\"process\")\nfor r in results:\n    print(f\"  {r['file']}:{r['lines'][0]} - {r['identifier']} ({r['type']})\")\n\nprint(\"\\nSearch results for 'data':\")\nresults = index.search(\"data\")\nfor r in results:\n    print(f\"  {r['file']}:{r['lines'][0]} - {r['identifier']} ({r['type']})\")\n\n# Save index\nindex.save(\"code_index.json\")\nprint(\"\\nIndex saved to code_index.json\")\n</code></pre>"},{"location":"getting-started/#using-new-features","title":"Using New Features","text":""},{"location":"getting-started/#plugin-system","title":"Plugin System","text":"<p>Tree-sitter Chunker now includes a plugin system for language support:</p> <pre><code># save as use_plugins.py\nfrom chunker.plugin_manager import get_plugin_manager\nfrom chunker.core import chunk_file\n\n# Load built-in plugins\nmanager = get_plugin_manager()\nmanager.load_built_in_plugins()\n\n# List available plugins\nprint(\"Available plugins:\", manager.list_plugins())\n# Output: ['python', 'javascript', 'rust', 'c', 'cpp']\n\n# Chunk with plugins loaded\nchunks = chunk_file(\"example.py\", \"python\")\nprint(f\"Chunked {len(chunks)} items with plugin support\")\n</code></pre>"},{"location":"getting-started/#parallel-processing","title":"Parallel Processing","text":"<p>Process multiple files in parallel for better performance:</p> <pre><code># save as parallel_processing.py\nfrom chunker.parallel import chunk_files_parallel, chunk_directory_parallel\nfrom pathlib import Path\n\n# Create some test files\ntest_files = [\"example.py\", \"analyze_chunks.py\", \"extract_functions.py\"]\n\n# Process files in parallel\nresults = chunk_files_parallel(\n    test_files,\n    \"python\",\n    max_workers=4,\n    show_progress=True\n)\n\nprint(f\"\\nProcessed {len(results)} files:\")\nfor file_path, chunks in results.items():\n    print(f\"  {file_path}: {len(chunks)} chunks\")\n\n# Process entire directory\nif Path(\"src\").exists():\n    dir_results = chunk_directory_parallel(\n        \"src/\",\n        \"python\",\n        pattern=\"**/*.py\",\n        max_workers=4\n    )\n    print(f\"\\nDirectory processing: {len(dir_results)} files\")\n</code></pre>"},{"location":"getting-started/#export-formats","title":"Export Formats","text":"<p>Export chunks in different formats:</p> <pre><code># save as export_chunks.py\nfrom chunker.core import chunk_file\nfrom chunker.export.json_export import JSONExporter, JSONLExporter\nfrom chunker.export.formatters import SchemaType\nfrom chunker.exporters import ParquetExporter\n\n# Get chunks\nchunks = chunk_file(\"example.py\", \"python\")\n\n# Export to JSON with nested schema\njson_exporter = JSONExporter(schema_type=SchemaType.NESTED)\njson_exporter.export(chunks, \"chunks_nested.json\", indent=2)\n\n# Export to JSONL for streaming\njsonl_exporter = JSONLExporter()\njsonl_exporter.export(chunks, \"chunks.jsonl\")\n\n# Export to Parquet for analytics\nparquet_exporter = ParquetExporter(compression=\"snappy\")\nparquet_exporter.export(chunks, \"chunks.parquet\")\n\nprint(\"Exported to:\")\nprint(\"  - chunks_nested.json (nested hierarchy)\")\nprint(\"  - chunks.jsonl (streaming format)\")\nprint(\"  - chunks.parquet (columnar format)\")\n</code></pre>"},{"location":"getting-started/#configuration-files","title":"Configuration Files","text":"<p>Use configuration files to customize behavior:</p> <pre><code># Create a configuration file\ncat &gt; .chunkerrc &lt;&lt; 'EOF'\n# Tree-sitter Chunker Configuration\n\n# Global settings\nmin_chunk_size = 3\nmax_chunk_size = 300\n\n# Plugin directories\nplugin_dirs = [\"./plugins\"]\n\n# Language-specific settings\n[languages.python]\nchunk_types = [\"function_definition\", \"class_definition\", \"async_function_definition\"]\nmin_chunk_size = 5\n\n[languages.javascript]\nchunk_types = [\"function_declaration\", \"arrow_function\", \"class_declaration\"]\ninclude_jsx = true\nEOF\n\n# Use with CLI\npython cli/main.py chunk example.py -l python --config .chunkerrc\n</code></pre>"},{"location":"getting-started/#working-with-different-languages","title":"Working with Different Languages","text":"<p>Let's create examples in multiple languages:</p> <pre><code># Create a JavaScript example\ncat &gt; example.js &lt;&lt; 'EOF'\nclass Calculator {\n    constructor() {\n        this.result = 0;\n    }\n\n    add(a, b) {\n        return a + b;\n    }\n\n    multiply(a, b) {\n        return a * b;\n    }\n}\n\nconst calculate = (operation, a, b) =&gt; {\n    const calc = new Calculator();\n    switch(operation) {\n        case 'add': return calc.add(a, b);\n        case 'multiply': return calc.multiply(a, b);\n        default: throw new Error('Unknown operation');\n    }\n};\nEOF\n\n# Create a Rust example\ncat &gt; example.rs &lt;&lt; 'EOF'\nstruct Calculator {\n    result: f64,\n}\n\nimpl Calculator {\n    fn new() -&gt; Self {\n        Calculator { result: 0.0 }\n    }\n\n    fn add(&amp;self, a: f64, b: f64) -&gt; f64 {\n        a + b\n    }\n\n    fn multiply(&amp;self, a: f64, b: f64) -&gt; f64 {\n        a * b\n    }\n}\n\nfn calculate(operation: &amp;str, a: f64, b: f64) -&gt; Result&lt;f64, String&gt; {\n    let calc = Calculator::new();\n    match operation {\n        \"add\" =&gt; Ok(calc.add(a, b)),\n        \"multiply\" =&gt; Ok(calc.multiply(a, b)),\n        _ =&gt; Err(\"Unknown operation\".to_string()),\n    }\n}\nEOF\n</code></pre> <p>Now analyze all three languages:</p> <pre><code># save as compare_languages.py\nfrom chunker.chunker import chunk_file\n\nfiles = [\n    (\"example.py\", \"python\"),\n    (\"example.js\", \"javascript\"),\n    (\"example.rs\", \"rust\")\n]\n\nfor file_path, language in files:\n    print(f\"\\n{'='*50}\")\n    print(f\"Analyzing {file_path} ({language})\")\n    print('='*50)\n\n    chunks = chunk_file(file_path, language)\n\n    # Group by type\n    by_type = {}\n    for chunk in chunks:\n        by_type.setdefault(chunk.node_type, []).append(chunk)\n\n    # Display summary\n    print(f\"Total chunks: {len(chunks)}\")\n    for node_type, items in sorted(by_type.items()):\n        print(f\"\\n{node_type} ({len(items)}):\")\n        for item in items:\n            first_line = item.content.split('\\n')[0].strip()\n            if item.parent_context:\n                print(f\"  - {first_line} [in {item.parent_context}]\")\n            else:\n                print(f\"  - {first_line}\")\n</code></pre>"},{"location":"getting-started/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/#1-handle-errors-gracefully","title":"1. Handle Errors Gracefully","text":"<pre><code>from chunker.chunker import chunk_file\nfrom chunker.exceptions import LanguageNotFoundError, ChunkerError\n\ndef safe_chunk_file(file_path, language):\n    \"\"\"Safely chunk a file with error handling.\"\"\"\n    try:\n        return chunk_file(file_path, language)\n    except LanguageNotFoundError as e:\n        print(f\"Language '{language}' not supported. Available: {e}\")\n        return []\n    except FileNotFoundError:\n        print(f\"File not found: {file_path}\")\n        return []\n    except ChunkerError as e:\n        print(f\"Chunking error: {e}\")\n        return []\n</code></pre>"},{"location":"getting-started/#2-use-type-hints","title":"2. Use Type Hints","text":"<pre><code>from typing import List, Dict, Optional\nfrom chunker.chunker import CodeChunk\n\ndef analyze_chunks(chunks: List[CodeChunk]) -&gt; Dict[str, int]:\n    \"\"\"Analyze chunks and return statistics.\"\"\"\n    stats: Dict[str, int] = {}\n    for chunk in chunks:\n        stats[chunk.node_type] = stats.get(chunk.node_type, 0) + 1\n    return stats\n</code></pre>"},{"location":"getting-started/#3-process-large-codebases-efficiently","title":"3. Process Large Codebases Efficiently","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\n\ndef process_codebase(root_dir: str, max_workers: int = 4):\n    \"\"\"Process a large codebase in parallel.\"\"\"\n    from chunker.chunker import chunk_file\n\n    # Collect all Python files\n    py_files = list(Path(root_dir).rglob(\"*.py\"))\n\n    def process_file(file_path):\n        try:\n            return chunk_file(str(file_path), \"python\")\n        except Exception as e:\n            print(f\"Error in {file_path}: {e}\")\n            return []\n\n    # Process in parallel\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        results = list(executor.map(process_file, py_files))\n\n    # Flatten results\n    all_chunks = []\n    for chunks in results:\n        all_chunks.extend(chunks)\n\n    return all_chunks\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>You've now learned the basics of Tree-sitter Chunker! Here's what to explore next:</p> <ol> <li>User Guide - Comprehensive documentation including plugins and performance</li> <li>API Reference - Detailed documentation of all 27 exported APIs</li> <li>Plugin Development - Create custom language plugins</li> <li>Configuration - Advanced configuration options</li> <li>Performance Guide - Optimization strategies</li> <li>Export Formats - Working with different output formats</li> <li>Cookbook - Advanced recipes and examples</li> <li>Architecture - Understanding the internals</li> </ol>"},{"location":"getting-started/#quick-reference-card","title":"Quick Reference Card","text":"<pre><code># Import\nfrom chunker.core import chunk_file\nfrom chunker.parallel import chunk_files_parallel, chunk_directory_parallel\nfrom chunker.streaming import chunk_file_streaming\nfrom chunker.plugin_manager import get_plugin_manager\nfrom chunker.cache import ASTCache\nfrom chunker.chunker_config import ChunkerConfig\nfrom chunker.types import CodeChunk\nfrom chunker.parser import get_parser, list_languages\nfrom chunker.export.json_export import JSONExporter, JSONLExporter\nfrom chunker.export.formatters import SchemaType\nfrom chunker.exporters.parquet import ParquetExporter\n\n# Basic chunking\nchunks = chunk_file(\"file.py\", \"python\")\n\n# Parallel processing\nresults = chunk_files_parallel([\"file1.py\", \"file2.py\"], \"python\")\ndir_results = chunk_directory_parallel(\"src/\", \"python\", pattern=\"**/*.py\")\n\n# Streaming for large files\nfor chunk in chunk_file_streaming(\"huge_file.py\", \"python\"):\n    process(chunk)\n\n# Plugin management\nmanager = get_plugin_manager()\nmanager.load_built_in_plugins()\n\n# Export formats\nexporter = JSONExporter(schema_type=SchemaType.NESTED)\nexporter.export(chunks, \"output.json\")\n\nparquet = ParquetExporter(compression=\"snappy\")\nparquet.export(chunks, \"output.parquet\")\n\n# Configuration\nconfig = ChunkerConfig(\".chunkerrc\")\n\n# Available languages\nlanguages = list_languages()  # ['c', 'cpp', 'javascript', 'python', 'rust']\n\n# Chunk properties\nchunk.language       # Programming language\nchunk.file_path      # Source file path  \nchunk.node_type      # e.g., \"function_definition\"\nchunk.start_line     # Starting line (1-indexed)\nchunk.end_line       # Ending line\nchunk.byte_start     # Starting byte offset\nchunk.byte_end       # Ending byte offset\nchunk.parent_context # e.g., \"class:MyClass\"\nchunk.content        # Actual source code\nchunk.chunk_id       # Unique identifier\n\n# CLI usage\n# python cli/main.py chunk &lt;file&gt; -l &lt;language&gt; [options]\n# Options: --json, --jsonl, --config, --parallel, --progress\n\n# Common patterns\nfunctions = [c for c in chunks if \"function\" in c.node_type]\nclasses = [c for c in chunks if c.node_type == \"class_definition\"]\nmethods = [c for c in chunks if c.parent_context]\nlarge_functions = [c for c in chunks if c.end_line - c.start_line &gt; 50]\n</code></pre> <p>Happy chunking! \ud83d\ude80</p>"},{"location":"grammar_discovery/","title":"Grammar Discovery Service","text":"<p>The Grammar Discovery Service is a core component of Phase 14 - Universal Language Support. It provides automatic discovery of tree-sitter grammars from GitHub and manages metadata about available grammars.</p>"},{"location":"grammar_discovery/#overview","title":"Overview","text":"<p>The service implements the <code>GrammarDiscoveryContract</code> and provides:</p> <ul> <li>GitHub API Integration: Automatically discovers grammars from the tree-sitter organization</li> <li>Caching: Results are cached locally to avoid rate limiting and improve performance</li> <li>Search: Find grammars by name or description</li> <li>Version Tracking: Check for updates to installed grammars</li> <li>Compatibility Info: Get compatibility requirements for specific grammar versions</li> </ul>"},{"location":"grammar_discovery/#architecture","title":"Architecture","text":"<pre><code>GrammarDiscoveryService\n\u251c\u2500\u2500 GitHub API Client (requests)\n\u251c\u2500\u2500 Cache Manager (JSON file-based)\n\u251c\u2500\u2500 Version Comparator\n\u2514\u2500\u2500 Metadata Extractor\n</code></pre>"},{"location":"grammar_discovery/#usage","title":"Usage","text":"<pre><code>from chunker.grammar.discovery import GrammarDiscoveryService\n\n# Create service instance\ndiscovery = GrammarDiscoveryService()\n\n# List all official grammars\ngrammars = discovery.list_available_grammars()\n\n# Include community grammars\nall_grammars = discovery.list_available_grammars(include_community=True)\n\n# Get info for a specific language\npython_info = discovery.get_grammar_info(\"python\")\nif python_info:\n    print(f\"Python grammar: {python_info.url}\")\n    print(f\"Version: {python_info.version}\")\n    print(f\"Stars: {python_info.stars}\")\n\n# Search for grammars\nrust_grammars = discovery.search_grammars(\"rust\")\n\n# Check for updates\ninstalled = {\"python\": \"0.19.0\", \"rust\": \"0.20.0\"}\nupdates = discovery.check_grammar_updates(installed)\nfor lang, (current, latest) in updates.items():\n    print(f\"{lang}: {current} -&gt; {latest}\")\n\n# Get compatibility info\ncompat = discovery.get_grammar_compatibility(\"python\", \"0.20.0\")\nprint(f\"ABI Version: {compat.abi_version}\")\nprint(f\"Tested Python versions: {compat.tested_python_versions}\")\n\n# Refresh cache manually\nsuccess = discovery.refresh_cache()\n</code></pre>"},{"location":"grammar_discovery/#caching","title":"Caching","text":"<p>The service implements intelligent caching to minimize API calls:</p> <ul> <li>Location: <code>~/.cache/treesitter-chunker/discovery_cache.json</code></li> <li>Duration: 24 hours by default</li> <li>Format: JSON with timestamp and grammar metadata</li> <li>Automatic refresh: When cache expires or on manual refresh</li> </ul>"},{"location":"grammar_discovery/#rate-limiting","title":"Rate Limiting","text":"<p>The service respects GitHub API rate limits:</p> <ul> <li>Unauthenticated: 60 requests/hour</li> <li>Graceful handling: Stops fetching when limit reached</li> <li>Cache fallback: Uses cached data when API unavailable</li> </ul>"},{"location":"grammar_discovery/#grammar-metadata","title":"Grammar Metadata","text":"<p>Each grammar includes:</p> <ul> <li>name: Language identifier (e.g., \"python\", \"rust\")</li> <li>url: GitHub repository URL</li> <li>version: Current version (semantic versioning)</li> <li>last_updated: Last modification timestamp</li> <li>stars: GitHub star count</li> <li>description: Grammar description</li> <li>supported_extensions: File extensions (e.g., [\".py\", \".pyw\"])</li> <li>official: Whether from tree-sitter organization</li> </ul>"},{"location":"grammar_discovery/#implementation-details","title":"Implementation Details","text":""},{"location":"grammar_discovery/#language-name-resolution","title":"Language Name Resolution","text":"<p>The service handles multiple naming conventions: - Exact match: <code>get_grammar_info(\"python\")</code> - With prefix: <code>get_grammar_info(\"rust\")</code> finds \"tree-sitter-rust\"</p>"},{"location":"grammar_discovery/#version-comparison","title":"Version Comparison","text":"<p>Simple semantic versioning comparison: - Major.Minor.Patch format - Handles missing patch versions - Falls back safely on parse errors</p>"},{"location":"grammar_discovery/#file-extension-mapping","title":"File Extension Mapping","text":"<p>Common languages have predefined extension mappings: - Python: [\".py\", \".pyw\"] - JavaScript: [\".js\", \".mjs\", \".cjs\"] - TypeScript: [\".ts\", \".tsx\"] - And many more...</p>"},{"location":"grammar_discovery/#error-handling","title":"Error Handling","text":"<p>The service handles errors gracefully:</p> <ul> <li>Network errors: Falls back to cache</li> <li>Rate limiting: Stops fetching, returns partial results</li> <li>Invalid responses: Logs errors, continues with valid data</li> <li>Cache corruption: Recreates cache on next successful fetch</li> </ul>"},{"location":"grammar_discovery/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements:</p> <ol> <li>Authentication: Support GitHub tokens for higher rate limits</li> <li>Community grammars: Discover grammars outside tree-sitter org</li> <li>Release tracking: Fetch actual release versions instead of defaults</li> <li>Parallel fetching: Speed up discovery with concurrent requests</li> <li>Grammar quality metrics: Include test coverage, commit activity</li> </ol>"},{"location":"grammar_discovery/#testing","title":"Testing","text":"<p>The service includes comprehensive tests:</p> <ul> <li>Unit tests: Mock GitHub API responses</li> <li>Integration tests: Work with other Phase 14 components</li> <li>Cache tests: Verify caching behavior</li> <li>Error tests: Ensure graceful failure handling</li> </ul> <p>Run tests:</p> <pre><code>python -m pytest tests/test_grammar_discovery.py -xvs\n</code></pre>"},{"location":"graphml_export/","title":"GraphML Export Documentation","text":""},{"location":"graphml_export/#overview","title":"Overview","text":"<p>The GraphML exporter converts code chunks and their relationships into GraphML format, a standard XML-based format for representing graphs. This enables visualization and analysis in tools like yEd, Gephi, Cytoscape, and other graph visualization software.</p>"},{"location":"graphml_export/#features","title":"Features","text":""},{"location":"graphml_export/#core-features","title":"Core Features","text":"<ul> <li>Valid GraphML 1.0 output - Generates standards-compliant GraphML files</li> <li>Full metadata support - Exports all chunk properties as node/edge attributes</li> <li>Relationship preservation - Maintains all code relationships (calls, imports, contains)</li> <li>XML safety - Properly escapes special characters in code content</li> <li>UTF-8 support - Handles international characters correctly</li> </ul>"},{"location":"graphml_export/#visualization-features","title":"Visualization Features","text":"<ul> <li>Customizable node colors - Map chunk types to specific colors</li> <li>Customizable node shapes - Map chunk types to shapes (rectangle, ellipse, etc.)</li> <li>Edge styling - Color relationships by type</li> <li>Automatic attribute discovery - Dynamically creates GraphML keys for all properties</li> </ul>"},{"location":"graphml_export/#extended-features-yed-support","title":"Extended Features (yEd Support)","text":"<ul> <li>yEd-specific extensions - Enhanced visualization in yEd graph editor</li> <li>Advanced node styling - Gradients, borders, and text formatting</li> <li>Edge routing hints - Better automatic layout support</li> </ul>"},{"location":"graphml_export/#usage","title":"Usage","text":""},{"location":"graphml_export/#basic-usage","title":"Basic Usage","text":"<pre><code>from chunker.export.graphml_exporter import GraphMLExporter\nfrom chunker.types import CodeChunk\n\n# Create exporter\nexporter = GraphMLExporter()\n\n# Add chunks\nchunks = [chunk1, chunk2, chunk3]  # Your CodeChunk objects\nexporter.add_chunks(chunks)\n\n# Add relationships\nexporter.add_relationship(chunk1, chunk2, \"CALLS\", {\"line\": 42})\n\n# Export to file\nfrom pathlib import Path\nexporter.export(Path(\"output.graphml\"))\n</code></pre>"},{"location":"graphml_export/#with-visualization-hints","title":"With Visualization Hints","text":"<pre><code># Add visualization hints for better rendering\nexporter.add_visualization_hints(\n    node_colors={\n        \"function\": \"#4287f5\",\n        \"class\": \"#42f554\",\n        \"method\": \"#f5a442\"\n    },\n    edge_colors={\n        \"CALLS\": \"#ff0000\",\n        \"IMPORTS\": \"#0000ff\",\n        \"CONTAINS\": \"#00ff00\"\n    },\n    node_shapes={\n        \"function\": \"ellipse\",\n        \"class\": \"rectangle\",\n        \"method\": \"roundrectangle\"\n    }\n)\n\n# Export with pretty printing\ngraphml_str = exporter.export_string(pretty_print=True)\n</code></pre>"},{"location":"graphml_export/#automatic-relationship-extraction","title":"Automatic Relationship Extraction","text":"<pre><code># Automatically extract relationships from chunk metadata\nexporter.extract_relationships(chunks)\n# This will create:\n# - CONTAINS edges for parent-child relationships\n# - IMPORTS edges for import dependencies\n# - CALLS edges for function calls\n</code></pre>"},{"location":"graphml_export/#graphml-structure","title":"GraphML Structure","text":""},{"location":"graphml_export/#generated-keys","title":"Generated Keys","text":"<p>The exporter automatically generates GraphML key definitions for all unique attributes found in nodes and edges:</p> <pre><code>&lt;key id=\"n_label\" for=\"node\" attr.name=\"label\" attr.type=\"string\"/&gt;\n&lt;key id=\"n_file_path\" for=\"node\" attr.name=\"file_path\" attr.type=\"string\"/&gt;\n&lt;key id=\"n_start_line\" for=\"node\" attr.name=\"start_line\" attr.type=\"int\"/&gt;\n&lt;key id=\"n_chunk_type\" for=\"node\" attr.name=\"chunk_type\" attr.type=\"string\"/&gt;\n&lt;!-- Additional keys for all metadata properties --&gt;\n</code></pre>"},{"location":"graphml_export/#node-structure","title":"Node Structure","text":"<p>Each code chunk becomes a node with: - Unique ID based on file path and line numbers - Label showing the chunk type - All properties from the chunk metadata</p> <pre><code>&lt;node id=\"src/main.py:1:10\"&gt;\n  &lt;data key=\"n_label\"&gt;function&lt;/data&gt;\n  &lt;data key=\"n_file_path\"&gt;src/main.py&lt;/data&gt;\n  &lt;data key=\"n_start_line\"&gt;1&lt;/data&gt;\n  &lt;data key=\"n_end_line\"&gt;10&lt;/data&gt;\n  &lt;data key=\"n_chunk_type\"&gt;function&lt;/data&gt;\n  &lt;data key=\"n_name\"&gt;main&lt;/data&gt;\n  &lt;!-- Additional metadata --&gt;\n&lt;/node&gt;\n</code></pre>"},{"location":"graphml_export/#edge-structure","title":"Edge Structure","text":"<p>Relationships become directed edges with: - Source and target node IDs - Relationship type as label - Additional properties from relationship metadata</p> <pre><code>&lt;edge id=\"e0\" source=\"src/main.py:1:10\" target=\"src/utils.py:5:15\"&gt;\n  &lt;data key=\"e_label\"&gt;CALLS&lt;/data&gt;\n  &lt;data key=\"e_line\"&gt;3&lt;/data&gt;\n&lt;/edge&gt;\n</code></pre>"},{"location":"graphml_export/#type-inference","title":"Type Inference","text":"<p>The exporter automatically infers GraphML data types from Python values: - <code>bool</code> \u2192 <code>boolean</code> - <code>int</code> \u2192 <code>int</code> - <code>float</code> \u2192 <code>double</code> - Everything else \u2192 <code>string</code></p>"},{"location":"graphml_export/#special-character-handling","title":"Special Character Handling","text":"<p>All XML special characters in code content and metadata are properly escaped: - <code>&amp;</code> \u2192 <code>&amp;amp;</code> - <code>&lt;</code> \u2192 <code>&amp;lt;</code> - <code>&gt;</code> \u2192 <code>&amp;gt;</code> - <code>\"</code> \u2192 <code>&amp;quot;</code> (in attributes)</p>"},{"location":"graphml_export/#compatibility","title":"Compatibility","text":"<p>The generated GraphML files are compatible with: - yEd Graph Editor - Gephi - Cytoscape - NetworkX - igraph - Most other graph analysis tools</p>"},{"location":"graphml_export/#advanced-usage","title":"Advanced Usage","text":""},{"location":"graphml_export/#custom-graph-attributes","title":"Custom Graph Attributes","text":"<pre><code># Customize graph-level attributes\nexporter.graph_attrs[\"id\"] = \"MyCodeGraph\"\nexporter.graph_attrs[\"description\"] = \"Code analysis results\"\n</code></pre>"},{"location":"graphml_export/#filtering-nodes","title":"Filtering Nodes","text":"<pre><code># Add only specific chunk types\nfiltered_chunks = [c for c in chunks if c.metadata.get(\"chunk_type\") == \"function\"]\nexporter.add_chunks(filtered_chunks)\n</code></pre>"},{"location":"graphml_export/#post-processing","title":"Post-Processing","text":"<p>The generated GraphML is standard XML and can be further processed:</p> <pre><code>import xml.etree.ElementTree as ET\n\n# Export and parse\ngraphml_str = exporter.export_string()\nroot = ET.fromstring(graphml_str)\n\n# Add custom elements\ncomment = ET.Comment(\"Generated by TreeSitter Chunker\")\nroot.insert(0, comment)\n\n# Re-serialize\nmodified_xml = ET.tostring(root, encoding='unicode')\n</code></pre>"},{"location":"intelligent_fallback/","title":"Intelligent Fallback System","text":"<p>The IntelligentFallbackChunker provides smart, automatic selection between tree-sitter parsing and sliding window chunking based on file characteristics, language support, and token limits.</p>"},{"location":"intelligent_fallback/#overview","title":"Overview","text":"<p>The intelligent fallback system automatically chooses the best chunking method by analyzing: - Tree-sitter language support availability - Parse success/failure - Token limits and chunk sizes - File type and content characteristics - Specialized processor availability</p>"},{"location":"intelligent_fallback/#quick-start","title":"Quick Start","text":"<pre><code>from chunker import IntelligentFallbackChunker\n\n# Create with token limit\nfallback = IntelligentFallbackChunker(\n    token_limit=1000,  # Max tokens per chunk\n    model=\"gpt-4\"      # Tokenizer model\n)\n\n# Chunk any file - it will automatically choose the best method\nchunks = fallback.chunk_text(content, \"example.py\")\n\n# Each chunk includes decision metadata\nfor chunk in chunks:\n    print(f\"Method: {chunk.metadata['chunking_decision']}\")\n    print(f\"Reason: {chunk.metadata['chunking_reason']}\")\n</code></pre>"},{"location":"intelligent_fallback/#decision-logic","title":"Decision Logic","text":""},{"location":"intelligent_fallback/#1-tree-sitter-primary","title":"1. Tree-sitter (Primary)","text":"<p>Used when: - Language is supported by tree-sitter - Parsing succeeds - All chunks fit within token limits</p> <pre><code># Automatically uses tree-sitter for supported languages\nchunks = fallback.chunk_text(python_code, \"script.py\")\n# Decision: TREE_SITTER\n</code></pre>"},{"location":"intelligent_fallback/#2-tree-sitter-with-splitting","title":"2. Tree-sitter with Splitting","text":"<p>Used when: - Language is supported by tree-sitter - Parsing succeeds - Some chunks exceed token limits</p> <pre><code># Large functions/classes are automatically split\nfallback = IntelligentFallbackChunker(token_limit=500)\nchunks = fallback.chunk_text(large_class, \"module.py\")\n# Decision: TREE_SITTER_WITH_SPLIT\n</code></pre>"},{"location":"intelligent_fallback/#3-specialized-processor","title":"3. Specialized Processor","text":"<p>Used when: - File type has a specialized processor (markdown, logs, config) - File is not a code file</p> <pre><code># Automatically uses markdown processor\nchunks = fallback.chunk_text(markdown_content, \"README.md\")\n# Decision: SPECIALIZED_PROCESSOR\n</code></pre>"},{"location":"intelligent_fallback/#4-sliding-window-fallback","title":"4. Sliding Window (Fallback)","text":"<p>Used when: - No tree-sitter support for language - Parse fails or produces no chunks - Unknown file type</p> <pre><code># Falls back to sliding window for unknown types\nchunks = fallback.chunk_text(text, \"data.xyz\")\n# Decision: SLIDING_WINDOW\n</code></pre>"},{"location":"intelligent_fallback/#features","title":"Features","text":""},{"location":"intelligent_fallback/#automatic-language-detection","title":"Automatic Language Detection","text":"<pre><code># Extension-based detection\nfallback.chunk_text(content, \"script.py\")    # Detects Python\nfallback.chunk_text(content, \"app.js\")       # Detects JavaScript\n\n# Shebang detection\ncontent = \"#!/usr/bin/env python3\\n...\"\nfallback.chunk_text(content, \"script\")       # Detects Python from shebang\n</code></pre>"},{"location":"intelligent_fallback/#token-limit-enforcement","title":"Token Limit Enforcement","text":"<pre><code># Set token limit for LLM compatibility\nfallback = IntelligentFallbackChunker(\n    token_limit=4000,      # GPT-4 safe limit\n    model=\"gpt-4\"          # Use correct tokenizer\n)\n\n# Large chunks are automatically split\nchunks = fallback.chunk_text(large_file, \"big.py\")\n# All chunks guaranteed &lt;= 4000 tokens\n</code></pre>"},{"location":"intelligent_fallback/#decision-transparency","title":"Decision Transparency","text":"<pre><code># Get detailed decision information\ninfo = fallback.get_decision_info(\"file.py\", content)\n\nprint(info['decision'])           # e.g., \"tree_sitter_with_split\"\nprint(info['reason'])             # e.g., \"Tree-sitter with splitting (largest chunk: 1234 tokens)\"\nprint(info['metrics'])            # Detailed analysis metrics\n</code></pre>"},{"location":"intelligent_fallback/#configuration","title":"Configuration","text":""},{"location":"intelligent_fallback/#basic-configuration","title":"Basic Configuration","text":"<pre><code># Default configuration\nfallback = IntelligentFallbackChunker()\n\n# With token limits\nfallback = IntelligentFallbackChunker(\n    token_limit=1000,\n    model=\"claude\"\n)\n\n# With sliding window config\nfallback = IntelligentFallbackChunker(\n    sliding_window_config={\n        'window_size': 2000,\n        'overlap': 200\n    }\n)\n</code></pre>"},{"location":"intelligent_fallback/#supported-languages","title":"Supported Languages","text":"<p>The system automatically detects these languages:</p> <pre><code># Common languages\n.py    \u2192 python\n.js    \u2192 javascript\n.ts    \u2192 typescript\n.java  \u2192 java\n.cpp   \u2192 cpp\n.rs    \u2192 rust\n.go    \u2192 go\n.rb    \u2192 ruby\n.php   \u2192 php\n.cs    \u2192 csharp\n# ... and many more\n</code></pre>"},{"location":"intelligent_fallback/#decision-metrics","title":"Decision Metrics","text":"<p>The system analyzes multiple metrics:</p> <pre><code>metrics = {\n    'has_tree_sitter_support': True,    # Language supported?\n    'parse_success': True,              # Parse succeeded?\n    'largest_chunk_tokens': 1234,       # Biggest chunk size\n    'average_chunk_tokens': 567,        # Average size\n    'total_tokens': 5678,               # Total tokens\n    'is_code_file': True,               # Code vs text?\n    'token_limit_exceeded': False,      # Any oversized chunks?\n}\n</code></pre>"},{"location":"intelligent_fallback/#examples","title":"Examples","text":""},{"location":"intelligent_fallback/#multi-language-project","title":"Multi-Language Project","text":"<pre><code>fallback = IntelligentFallbackChunker(token_limit=1000)\n\n# Automatically handles different file types\nfor file_path in project_files:\n    with open(file_path) as f:\n        content = f.read()\n\n    chunks = fallback.chunk_text(content, file_path)\n\n    # Each file uses optimal method\n    print(f\"{file_path}: {chunks[0].metadata['chunking_decision']}\")\n</code></pre>"},{"location":"intelligent_fallback/#llm-processing-pipeline","title":"LLM Processing Pipeline","text":"<pre><code># Configure for specific LLM\nfallback = IntelligentFallbackChunker(\n    token_limit=3500,      # Leave room for prompts\n    model=\"gpt-3.5-turbo\"\n)\n\n# Process repository\nfor file_path in repo.get_files():\n    chunks = fallback.chunk_text(\n        repo.read_file(file_path), \n        file_path\n    )\n\n    # All chunks fit in context window\n    for chunk in chunks:\n        response = llm.process(chunk.content)\n</code></pre>"},{"location":"intelligent_fallback/#fallback-handling","title":"Fallback Handling","text":"<pre><code># System gracefully handles edge cases\nfallback = IntelligentFallbackChunker()\n\n# Empty file - uses sliding window\nchunks = fallback.chunk_text(\"\", \"empty.py\")\n\n# Binary file - detects and handles\nchunks = fallback.chunk_text(binary_content, \"image.png\")\n\n# Corrupted code - falls back gracefully\nchunks = fallback.chunk_text(corrupted, \"broken.py\")\n</code></pre>"},{"location":"intelligent_fallback/#integration-with-phase-11-components","title":"Integration with Phase 11 Components","text":"<p>The intelligent fallback integrates with all Phase 11 processors:</p> <pre><code># Automatically uses specialized processors\nchunks = fallback.chunk_text(markdown, \"README.md\")    # MarkdownProcessor\nchunks = fallback.chunk_text(logs, \"app.log\")          # LogProcessor\nchunks = fallback.chunk_text(config, \"config.yaml\")    # ConfigProcessor\n\n# Falls back to sliding window when needed\nchunks = fallback.chunk_text(text, \"notes.txt\")        # SlidingWindowEngine\n</code></pre>"},{"location":"intelligent_fallback/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Set Appropriate Token Limits <code>python    # Account for prompt overhead    fallback = IntelligentFallbackChunker(        token_limit=6000  # For 8k context window    )</code></p> </li> <li> <p>Use Correct Tokenizer Model <code>python    # Match tokenizer to LLM    fallback = IntelligentFallbackChunker(        model=\"claude\"  # For Claude models    )</code></p> </li> <li> <p>Handle Decision Metadata    ```python    chunks = fallback.chunk_text(content, file_path)</p> </li> </ol> <p># Log decisions for debugging    for chunk in chunks:        logger.info(f\"Chunked with {chunk.metadata['chunking_decision']}\")    ```</p> <ol> <li>Monitor Performance    ```python    # Check decision distribution    decisions = {}    for chunk in all_chunks:        decision = chunk.metadata['chunking_decision']        decisions[decision] = decisions.get(decision, 0) + 1</li> </ol> <p>print(\"Chunking methods used:\", decisions)    ```</p>"},{"location":"intelligent_fallback/#limitations","title":"Limitations","text":"<ul> <li>Language detection is heuristic-based</li> <li>Token counting requires the tiktoken library</li> <li>Specialized processors must be available in the environment</li> <li>Binary files are detected but not chunked</li> </ul>"},{"location":"internal-api/","title":"Internal API Reference","text":"<p>\u26a0\ufe0f Warning: The APIs documented here are internal implementation details and are not part of the public API. They may change without notice between versions. Use at your own risk.</p>"},{"location":"internal-api/#internal-modules","title":"Internal Modules","text":"<p>The following modules have been moved to <code>chunker._internal</code> as they are implementation details:</p> <ul> <li><code>registry</code> - Language registry for discovering and loading tree-sitter languages</li> <li><code>factory</code> - Parser factory for creating and managing parser instances</li> <li><code>cache</code> - AST caching implementation (note: <code>ASTCache</code> is still exported publicly)</li> <li><code>gc_tuning</code> - Garbage collection optimization utilities</li> <li><code>vfs</code> - Virtual file system implementations</li> <li><code>file_utils</code> - File metadata and hashing utilities</li> </ul>"},{"location":"internal-api/#migration-guide","title":"Migration Guide","text":"<p>If you were previously using these internal modules directly:</p> <pre><code># Old way (no longer supported)\nfrom chunker.registry import LanguageRegistry\nfrom chunker.factory import ParserFactory, ParserConfig\n\n# New way (not recommended - internal use only)\nfrom chunker._internal.registry import LanguageRegistry\nfrom chunker._internal.factory import ParserFactory, ParserConfig\n</code></pre> <p>Recommended approach: Use the public API instead:</p> <pre><code># Public API - stable and supported\nfrom chunker import chunk_file, chunk_text, list_languages\nfrom chunker.parser import get_parser, get_language_info\n</code></pre>"},{"location":"internal-api/#why-this-change","title":"Why This Change?","text":"<p>Moving these modules to <code>_internal</code> helps:</p> <ol> <li>Clarify the public API - Users know which APIs are stable</li> <li>Enable internal refactoring - We can improve internals without breaking changes</li> <li>Reduce API surface - Simpler, cleaner public interface</li> <li>Better encapsulation - Implementation details are hidden</li> </ol>"},{"location":"internal-api/#advanced-use-cases","title":"Advanced Use Cases","text":"<p>If you need advanced functionality that was previously available through these modules, please:</p> <ol> <li>Check if the public API already provides what you need</li> <li>Open an issue describing your use case</li> <li>Consider contributing a PR to expose the functionality properly</li> </ol>"},{"location":"internal-api/#internal-module-documentation","title":"Internal Module Documentation","text":""},{"location":"internal-api/#registry-module","title":"Registry Module","text":"<p>The registry module discovers available languages from compiled tree-sitter libraries:</p> <pre><code># Internal use only!\nfrom chunker._internal.registry import LanguageRegistry\nregistry = LanguageRegistry(library_path)\nlanguages = registry.list_languages()\n</code></pre>"},{"location":"internal-api/#factory-module","title":"Factory Module","text":"<p>The factory creates and manages parser instances with pooling:</p> <pre><code># Internal use only!\nfrom chunker._internal.factory import ParserFactory, ParserConfig\nconfig = ParserConfig(timeout_ms=1000)\nfactory = ParserFactory(registry)\nparser = factory.get_parser(\"python\", config)\n</code></pre>"},{"location":"internal-api/#cache-module","title":"Cache Module","text":"<p>The cache module provides AST caching with SQLite:</p> <pre><code># Public API available!\nfrom chunker import ASTCache  # This is still public\ncache = ASTCache(cache_dir=\"./cache\")\n</code></pre> <p>Remember: These internal APIs may change or be removed in any version without notice!</p>"},{"location":"log_processor/","title":"Log Processor Documentation","text":"<p>The <code>LogProcessor</code> is a specialized text processor designed to handle various log file formats with intelligent chunking capabilities. It supports timestamp-based chunking, session detection, log level grouping, and error context extraction.</p>"},{"location":"log_processor/#features","title":"Features","text":"<ul> <li>Multi-format Support: Automatically detects and parses various log formats (syslog, Apache, ISO timestamps, Log4j, etc.)</li> <li>Flexible Chunking Strategies: Time-based, line-based, session-based, or log level-based chunking</li> <li>Session Detection: Identifies session boundaries for user activity tracking</li> <li>Error Context Extraction: Groups error messages with surrounding context lines</li> <li>Streaming Support: Process large log files efficiently with streaming API</li> <li>Multi-line Entry Handling: Correctly handles stack traces and multi-line log entries</li> <li>Timezone Awareness: Parses timestamps with various timezone formats</li> <li>Custom Pattern Support: Add your own log format patterns</li> </ul>"},{"location":"log_processor/#installation","title":"Installation","text":"<p>The LogProcessor is part of the treesitter-chunker package:</p> <pre><code>pip install treesitter-chunker\n</code></pre>"},{"location":"log_processor/#basic-usage","title":"Basic Usage","text":"<pre><code>from chunker.processors.logs import LogProcessor\n\n# Create a processor with default settings\nprocessor = LogProcessor()\n\n# Process a log file\nwith open('application.log', 'r') as f:\n    content = f.read()\n\nchunks = processor.process(content)\n\n# Each chunk contains:\nfor chunk in chunks:\n    print(f\"Lines {chunk.start_line}-{chunk.end_line}\")\n    print(f\"Formats detected: {chunk.metadata['formats']}\")\n    print(f\"Log levels: {chunk.metadata['levels']}\")\n</code></pre>"},{"location":"log_processor/#configuration-options","title":"Configuration Options","text":""},{"location":"log_processor/#chunking-strategies","title":"Chunking Strategies","text":"<pre><code># Time-based chunking (default)\nprocessor = LogProcessor(config={\n    'chunk_by': 'time',\n    'time_window': 300  # 5 minutes\n})\n\n# Line-based chunking\nprocessor = LogProcessor(config={\n    'chunk_by': 'lines',\n    'max_chunk_lines': 1000\n})\n\n# Session-based chunking\nprocessor = LogProcessor(config={\n    'chunk_by': 'session',\n    'detect_sessions': True\n})\n\n# Log level-based chunking\nprocessor = LogProcessor(config={\n    'chunk_by': 'level'\n})\n</code></pre>"},{"location":"log_processor/#error-context-grouping","title":"Error Context Grouping","text":"<pre><code>processor = LogProcessor(config={\n    'group_errors': True,\n    'context_lines': 5  # Include 5 lines before/after errors\n})\n</code></pre>"},{"location":"log_processor/#custom-patterns","title":"Custom Patterns","text":"<pre><code># Add custom log format patterns\nprocessor = LogProcessor(config={\n    'patterns': {\n        'custom_app': r'^(?P&lt;timestamp&gt;\\d{2}:\\d{2}:\\d{2})\\s+\\[(?P&lt;level&gt;\\w+)\\]\\s+(?P&lt;message&gt;.*)'\n    }\n})\n</code></pre>"},{"location":"log_processor/#supported-log-formats","title":"Supported Log Formats","text":""},{"location":"log_processor/#built-in-formats","title":"Built-in Formats","text":"<ol> <li> <p>Syslog Format <code>Jan  1 00:00:00 hostname process[pid]: message</code></p> </li> <li> <p>Apache Common/Combined Log Format <code>192.168.1.1 - - [01/Jan/2024:00:00:00 +0000] \"GET / HTTP/1.1\" 200 1234</code></p> </li> <li> <p>ISO Timestamp Format <code>2024-01-01 12:00:00,000 [INFO] message    2024-01-01T12:00:00.000Z [DEBUG] message</code></p> </li> <li> <p>Java/Log4j Format <code>2024-01-01 12:00:00,000 INFO [Thread-1] com.example.Class - message</code></p> </li> </ol>"},{"location":"log_processor/#log-level-detection","title":"Log Level Detection","text":"<p>The processor recognizes standard log levels: - CRITICAL/FATAL/EMERGENCY - ERROR/ERR/SEVERE - WARNING/WARN - INFO/INFORMATION/NOTICE - DEBUG/TRACE/VERBOSE</p>"},{"location":"log_processor/#advanced-usage","title":"Advanced Usage","text":""},{"location":"log_processor/#streaming-large-files","title":"Streaming Large Files","text":"<pre><code>def process_large_log(file_path):\n    processor = LogProcessor(config={\n        'chunk_by': 'time',\n        'time_window': 60\n    })\n\n    def line_generator():\n        with open(file_path, 'r') as f:\n            for line in f:\n                yield line\n\n    for chunk in processor.process_stream(line_generator()):\n        # Process each chunk as it's generated\n        process_chunk(chunk)\n</code></pre>"},{"location":"log_processor/#session-tracking","title":"Session Tracking","text":"<pre><code>processor = LogProcessor(config={\n    'chunk_by': 'session',\n    'detect_sessions': True\n})\n\nchunks = processor.process(log_content)\n\n# Group chunks by session\nsessions = {}\nfor chunk in chunks:\n    session_id = chunk.metadata.get('session_id')\n    if session_id:\n        sessions.setdefault(session_id, []).append(chunk)\n</code></pre>"},{"location":"log_processor/#error-analysis","title":"Error Analysis","text":"<pre><code>processor = LogProcessor(config={\n    'group_errors': True,\n    'context_lines': 10\n})\n\nchunks = processor.process(log_content)\n\n# Find chunks with errors\nerror_chunks = [c for c in chunks if c.metadata.get('has_errors')]\n\nfor chunk in error_chunks:\n    print(f\"Found {chunk.metadata['error_count']} errors\")\n    # Analyze error patterns, extract stack traces, etc.\n</code></pre>"},{"location":"log_processor/#chunk-metadata","title":"Chunk Metadata","text":"<p>Each chunk includes metadata about its contents:</p> <pre><code>{\n    'entry_count': 42,              # Number of log entries\n    'formats': ['syslog', 'iso'],  # Detected log formats\n    'levels': ['INFO', 'ERROR'],    # Log levels present\n    'start_time': '2024-01-01T00:00:00',  # First timestamp\n    'end_time': '2024-01-01T00:05:00',    # Last timestamp\n    'session_id': 'session_1',      # Session identifier (if applicable)\n    'has_errors': True,             # Contains error messages\n    'error_count': 3,               # Number of errors\n    'log_level': 'ERROR'            # Primary level (for level-based chunking)\n}\n</code></pre>"},{"location":"log_processor/#performance-considerations","title":"Performance Considerations","text":"<ol> <li> <p>Memory Usage: The processor buffers log entries for chunking. For very large files, use streaming mode.</p> </li> <li> <p>Timestamp Parsing: Complex timestamp formats may impact performance. Consider using simpler formats or custom patterns for better performance.</p> </li> <li> <p>Pattern Matching: Each line is matched against multiple patterns. Reduce the number of patterns for better performance.</p> </li> <li> <p>Chunk Size: Smaller chunks (shorter time windows or fewer lines) create more chunks but use less memory.</p> </li> </ol>"},{"location":"log_processor/#best-practices","title":"Best Practices","text":"<ol> <li>Choose Appropriate Chunking Strategy:</li> <li>Use time-based for time-series analysis</li> <li>Use session-based for user activity tracking</li> <li>Use level-based for error analysis</li> <li> <p>Use line-based for simple splitting</p> </li> <li> <p>Configure Time Windows: Match your time windows to your log rotation schedule or analysis needs.</p> </li> <li> <p>Handle Multi-line Entries: The processor automatically handles stack traces and multi-line entries. Ensure your logs use consistent formatting.</p> </li> <li> <p>Test Format Detection: Verify that your log formats are correctly detected. Add custom patterns if needed.</p> </li> <li> <p>Monitor Memory Usage: For production use with large files, implement monitoring and use streaming mode when appropriate.</p> </li> </ol>"},{"location":"log_processor/#examples","title":"Examples","text":"<p>See the <code>examples/logs/</code> directory for sample log files and <code>examples/demo_log_processor.py</code> for comprehensive usage examples.</p>"},{"location":"markdown_processor/","title":"Markdown Processor","text":"<p>The Markdown Processor is a specialized component of the tree-sitter-chunker that intelligently handles Markdown files while preserving document structure and formatting.</p>"},{"location":"markdown_processor/#overview","title":"Overview","text":"<p>The MarkdownProcessor provides intelligent chunking for Markdown documents by recognizing headers, code blocks, lists, tables, and other Markdown elements. It ensures that the chunked output maintains the document's logical structure and readability.</p>"},{"location":"markdown_processor/#features","title":"Features","text":""},{"location":"markdown_processor/#header-aware-chunking","title":"Header-Aware Chunking","text":"<ul> <li>Recognizes header hierarchy (# through ######)</li> <li>Groups content under headers</li> <li>Preserves document outline structure</li> <li>Supports both ATX and Setext style headers</li> </ul>"},{"location":"markdown_processor/#code-block-preservation","title":"Code Block Preservation","text":"<ul> <li>Keeps fenced code blocks intact</li> <li>Preserves language specifications</li> <li>Maintains indented code blocks</li> <li>Handles nested code in lists</li> </ul>"},{"location":"markdown_processor/#list-continuity","title":"List Continuity","text":"<ul> <li>Groups list items intelligently</li> <li>Preserves nested list structures</li> <li>Maintains numbering in ordered lists</li> <li>Handles mixed list types</li> </ul>"},{"location":"markdown_processor/#table-integrity","title":"Table Integrity","text":"<ul> <li>Keeps tables as single chunks</li> <li>Preserves alignment specifications</li> <li>Handles multi-line cells</li> <li>Maintains header rows</li> </ul>"},{"location":"markdown_processor/#special-element-handling","title":"Special Element Handling","text":"<ul> <li>Front matter (YAML/TOML) preservation</li> <li>Blockquote grouping</li> <li>Link reference definitions</li> <li>Footnote continuity</li> <li>HTML block handling</li> </ul>"},{"location":"markdown_processor/#usage","title":"Usage","text":""},{"location":"markdown_processor/#basic-usage","title":"Basic Usage","text":"<pre><code>from chunker.processors.markdown import MarkdownProcessor\n\nprocessor = MarkdownProcessor()\nchunks = processor.process_file(\"README.md\")\n</code></pre>"},{"location":"markdown_processor/#with-custom-configuration","title":"With Custom Configuration","text":"<pre><code>from chunker.processors.markdown import MarkdownProcessor, ProcessorConfig\n\nconfig = ProcessorConfig(\n    chunk_size=100,           # Target lines per chunk\n    preserve_headers=True,    # Keep headers with content\n    group_sections=True,      # Group by header sections\n    preserve_code_blocks=True # Keep code blocks intact\n)\n\nprocessor = MarkdownProcessor(config)\nchunks = processor.process_file(\"documentation.md\")\n</code></pre>"},{"location":"markdown_processor/#integration-with-main-chunker","title":"Integration with Main Chunker","text":"<p>The MarkdownProcessor is automatically used by the intelligent fallback system:</p> <pre><code>from chunker import IntelligentFallbackChunker\n\nchunker = IntelligentFallbackChunker()\nchunks = chunker.chunk_text(markdown_content, \"document.md\")\n</code></pre>"},{"location":"markdown_processor/#chunking-strategy","title":"Chunking Strategy","text":""},{"location":"markdown_processor/#section-based-chunking","title":"Section-Based Chunking","text":"<p>The processor uses headers as natural chunk boundaries:</p> <pre><code># Main Section\nContent under main section...\n\n## Subsection 1\nDetails for subsection 1...\n\n## Subsection 2\nDetails for subsection 2...\n</code></pre> <p>Each section becomes a logical chunk, with subsections grouped under their parent headers when appropriate.</p>"},{"location":"markdown_processor/#code-block-handling","title":"Code Block Handling","text":"<p>Code blocks are never split:</p> <pre><code>## Implementation\n\nHere's how to use the API:\n\n```python\ndef process_data(input_data):\n    # This entire code block stays together\n    result = transform(input_data)\n    return result\n</code></pre> <p>Additional explanation...</p> <pre><code>### List Processing\nLists are kept together to maintain context:\n\n```markdown\n## Features\n\n- Feature 1\n  - Subfeature 1.1\n  - Subfeature 1.2\n- Feature 2\n- Feature 3\n</code></pre>"},{"location":"markdown_processor/#table-preservation","title":"Table Preservation","text":"<p>Tables are treated as atomic units:</p> <pre><code>| Column 1 | Column 2 | Column 3 |\n|----------|----------|----------|\n| Data 1   | Data 2   | Data 3   |\n| Data 4   | Data 5   | Data 6   |\n</code></pre>"},{"location":"markdown_processor/#configuration-options","title":"Configuration Options","text":"Option Type Default Description <code>chunk_size</code> int 100 Target lines per chunk <code>preserve_headers</code> bool True Keep headers with their content <code>group_sections</code> bool True Group content by header hierarchy <code>preserve_code_blocks</code> bool True Never split code blocks <code>preserve_tables</code> bool True Keep tables intact <code>min_section_size</code> int 5 Minimum lines for separate section <code>max_header_depth</code> int 3 Maximum header depth for grouping"},{"location":"markdown_processor/#advanced-features","title":"Advanced Features","text":""},{"location":"markdown_processor/#front-matter-support","title":"Front Matter Support","text":"<p>YAML or TOML front matter is preserved as a separate chunk:</p> <pre><code>---\ntitle: \"Document Title\"\nauthor: \"Author Name\"\ndate: 2024-01-23\n---\n\n# Main Content\n...\n</code></pre>"},{"location":"markdown_processor/#link-reference-handling","title":"Link Reference Handling","text":"<p>Link references are grouped with their usage context:</p> <pre><code>Check out [my website][website] for more information.\n\n[website]: https://example.com \"Example Website\"\n</code></pre>"},{"location":"markdown_processor/#blockquote-grouping","title":"Blockquote Grouping","text":"<p>Multi-line blockquotes are kept together:</p> <pre><code>&gt; This is a long quote that spans\n&gt; multiple lines and should be\n&gt; kept together as one chunk.\n</code></pre>"},{"location":"markdown_processor/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Header Hierarchy: Use consistent header levels to enable better section grouping.</p> </li> <li> <p>Code Block Languages: Always specify language in code blocks for better processing.</p> </li> <li> <p>List Formatting: Use consistent indentation for nested lists to ensure proper grouping.</p> </li> <li> <p>Table Formatting: Keep tables reasonably sized as they cannot be split.</p> </li> <li> <p>Section Size: Balance section sizes to avoid extremely large or small chunks.</p> </li> </ol>"},{"location":"markdown_processor/#integration-with-phase-11","title":"Integration with Phase 11","text":"<p>The MarkdownProcessor is part of Phase 11's text processing capabilities and integrates with: - Sliding Window Fallback system - Intelligent Fallback Chunker - Token limit handling - Document processing pipeline</p>"},{"location":"markdown_processor/#see-also","title":"See Also","text":"<ul> <li>Intelligent Fallback - Automatic processor selection</li> <li>Config Processor - Configuration file processing</li> <li>Log Processor - Log file processing</li> <li>Token Limits - Token-aware chunking</li> </ul>"},{"location":"metadata-extraction/","title":"Metadata Extraction","text":"<p>The metadata extraction feature enriches code chunks with detailed information about functions, methods, and classes, including signatures, complexity metrics, documentation, and dependencies.</p>"},{"location":"metadata-extraction/#overview","title":"Overview","text":"<p>When enabled, the chunker automatically extracts the following metadata for each chunk:</p> <ul> <li>Function/Method Signatures: Parameter names, types, default values, return types</li> <li>Complexity Metrics: Cyclomatic and cognitive complexity, nesting depth, lines of code</li> <li>Documentation: Docstrings, JSDoc comments, and other documentation</li> <li>Dependencies: External symbols referenced by the chunk</li> <li>Imports/Exports: Module dependencies and exported symbols</li> </ul>"},{"location":"metadata-extraction/#usage","title":"Usage","text":""},{"location":"metadata-extraction/#basic-usage","title":"Basic Usage","text":"<pre><code>from chunker.chunker import chunk_text\n\n# Extract chunks with metadata (default)\nchunks = chunk_text(code, 'python')\n\n# Access metadata\nfor chunk in chunks:\n    print(f\"Function: {chunk.metadata['signature']['name']}\")\n    print(f\"Complexity: {chunk.metadata['complexity']['cyclomatic']}\")\n    print(f\"Docstring: {chunk.metadata['docstring']}\")\n</code></pre>"},{"location":"metadata-extraction/#disabling-metadata-extraction","title":"Disabling Metadata Extraction","text":"<pre><code># Disable metadata extraction for performance\nchunks = chunk_text(code, 'python', extract_metadata=False)\n</code></pre>"},{"location":"metadata-extraction/#metadata-structure","title":"Metadata Structure","text":"<p>Each chunk's metadata dictionary contains:</p> <pre><code>{\n    'signature': {\n        'name': 'function_name',\n        'parameters': [\n            {'name': 'param1', 'type': 'str', 'default': None},\n            {'name': 'param2', 'type': 'int', 'default': '0'}\n        ],\n        'return_type': 'bool',\n        'decorators': ['staticmethod', 'lru_cache'],\n        'modifiers': ['async', 'staticmethod']\n    },\n    'complexity': {\n        'cyclomatic': 5,\n        'cognitive': 8,\n        'nesting_depth': 3,\n        'lines_of_code': 25,\n        'logical_lines': 18\n    },\n    'docstring': 'Function documentation...',\n    'dependencies': ['external_func', 'SomeClass'],\n    'imports': ['import os', 'from typing import List'],\n    'exports': ['exported_function', 'ExportedClass']\n}\n</code></pre>"},{"location":"metadata-extraction/#supported-languages","title":"Supported Languages","text":"<p>Currently, metadata extraction is supported for:</p> <ul> <li>Python: Full support including type annotations, decorators, async/await</li> <li>JavaScript: Functions, arrow functions, async, generators, JSDoc</li> <li>TypeScript: All JavaScript features plus interfaces, type annotations</li> <li>JSX/TSX: Same as JavaScript/TypeScript</li> </ul>"},{"location":"metadata-extraction/#language-specific-features","title":"Language-Specific Features","text":""},{"location":"metadata-extraction/#python","title":"Python","text":"<ul> <li>Type annotations from function signatures</li> <li>Decorators and their parameters</li> <li>Async function detection</li> <li>Docstring extraction (Google, NumPy, and Sphinx styles)</li> <li>Special method modifiers (staticmethod, classmethod)</li> </ul>"},{"location":"metadata-extraction/#javascripttypescript","title":"JavaScript/TypeScript","text":"<ul> <li>JSDoc comment parsing</li> <li>Arrow function support</li> <li>Async/await and generator detection</li> <li>TypeScript type annotations</li> <li>Interface method signatures</li> <li>Method modifiers (static, private, protected)</li> </ul>"},{"location":"metadata-extraction/#complexity-metrics","title":"Complexity Metrics","text":""},{"location":"metadata-extraction/#cyclomatic-complexity","title":"Cyclomatic Complexity","text":"<p>Measures the number of linearly independent paths through the code: - Base complexity: 1 - +1 for each: if, while, for, case, catch, and, or - Higher values indicate more complex control flow</p>"},{"location":"metadata-extraction/#cognitive-complexity","title":"Cognitive Complexity","text":"<p>Measures how difficult code is to understand: - Considers nesting levels - Penalizes deeply nested conditions - Accounts for logical operators and recursion</p>"},{"location":"metadata-extraction/#example","title":"Example","text":"<pre><code>def example(items):         # Cyclomatic: 1 (base)\n    for item in items:      # Cyclomatic: 2, Cognitive: 1\n        if item &gt; 0:        # Cyclomatic: 3, Cognitive: 3 (nesting penalty)\n            process(item)\n</code></pre>"},{"location":"metadata-extraction/#performance-considerations","title":"Performance Considerations","text":"<p>Metadata extraction adds overhead to the chunking process. For large codebases where metadata is not needed, disable it:</p> <pre><code># Faster chunking without metadata\nchunks = chunk_file('large_file.py', 'python', extract_metadata=False)\n</code></pre>"},{"location":"metadata-extraction/#extending-metadata-extraction","title":"Extending Metadata Extraction","text":"<p>To add support for a new language:</p> <ol> <li>Create a new extractor class inheriting from <code>BaseMetadataExtractor</code></li> <li>Create a complexity analyzer inheriting from <code>BaseComplexityAnalyzer</code></li> <li>Register them in <code>MetadataExtractorFactory</code></li> </ol> <p>Example:</p> <pre><code>from chunker.metadata.extractor import BaseMetadataExtractor\n\nclass RubyMetadataExtractor(BaseMetadataExtractor):\n    def extract_signature(self, node, source):\n        # Implement Ruby-specific signature extraction\n        pass\n</code></pre>"},{"location":"metadata-extraction/#api-reference","title":"API Reference","text":""},{"location":"metadata-extraction/#metadataextractorfactory","title":"MetadataExtractorFactory","text":"<pre><code># Create extractors for a language\nextractor = MetadataExtractorFactory.create_extractor('python')\nanalyzer = MetadataExtractorFactory.create_analyzer('python')\n\n# Check language support\nif MetadataExtractorFactory.is_supported('ruby'):\n    # Extract metadata for Ruby\n    pass\n</code></pre>"},{"location":"metadata-extraction/#chunk-methods","title":"Chunk Methods","text":"<pre><code>chunk = chunks[0]\n\n# Access metadata\nsignature = chunk.metadata.get('signature', {})\ncomplexity = chunk.metadata.get('complexity', {})\n\n# Dependencies are also stored in the chunk\ndeps = chunk.dependencies  # List of dependency names\n</code></pre>"},{"location":"overlapping-fallback/","title":"Overlapping Fallback Chunker","text":""},{"location":"overlapping-fallback/#overview","title":"Overview","text":"<p>The overlapping fallback chunker provides sliding window chunking for files that do not have Tree-sitter support. This is useful for maintaining context across chunk boundaries in text files, logs, markdown documents, and other non-code content.</p>"},{"location":"overlapping-fallback/#key-features","title":"Key Features","text":"<ul> <li>Tree-sitter Protection: Automatically detects and rejects files with Tree-sitter support, ensuring code files use proper AST-based chunking</li> <li>Configurable Overlap: Support for fixed size, percentage-based, asymmetric, and dynamic overlap strategies</li> <li>Line or Character Based: Chunk by lines (useful for logs) or characters (useful for prose)</li> <li>Natural Boundary Detection: Can find paragraph breaks, sentence ends, and other natural boundaries for overlap points</li> <li>Clear Warnings: Emits warnings when overlapping fallback is used, making it clear this is not AST-based chunking</li> </ul>"},{"location":"overlapping-fallback/#usage","title":"Usage","text":""},{"location":"overlapping-fallback/#basic-overlapping-chunks","title":"Basic Overlapping Chunks","text":"<pre><code>from chunker.fallback_overlap import OverlappingFallbackChunker, OverlapStrategy\n\nchunker = OverlappingFallbackChunker()\n\n# Chunk a log file with line-based overlap\nchunks = chunker.chunk_with_overlap(\n    content=log_content,\n    file_path=\"app.log\",\n    chunk_size=100,      # 100 lines per chunk\n    overlap_size=10,     # 10 lines overlap\n    strategy=OverlapStrategy.FIXED,\n    unit=\"lines\"\n)\n</code></pre>"},{"location":"overlapping-fallback/#overlap-strategies","title":"Overlap Strategies","text":""},{"location":"overlapping-fallback/#fixed-overlap","title":"Fixed Overlap","text":"<p>The simplest strategy - use a fixed number of lines or characters for overlap:</p> <pre><code>chunks = chunker.chunk_with_overlap(\n    content, \"file.txt\",\n    chunk_size=1000,\n    overlap_size=200,  # Fixed 200 character overlap\n    strategy=OverlapStrategy.FIXED,\n    unit=\"characters\"\n)\n</code></pre>"},{"location":"overlapping-fallback/#percentage-based-overlap","title":"Percentage-Based Overlap","text":"<p>Overlap size as a percentage of chunk size:</p> <pre><code>chunks = chunker.chunk_with_overlap(\n    content, \"file.md\",\n    chunk_size=1000,\n    overlap_size=20,  # 20% overlap (200 chars)\n    strategy=OverlapStrategy.PERCENTAGE,\n    unit=\"characters\"\n)\n</code></pre>"},{"location":"overlapping-fallback/#asymmetric-overlap","title":"Asymmetric Overlap","text":"<p>Different overlap sizes before and after each chunk:</p> <pre><code>chunks = chunker.chunk_with_asymmetric_overlap(\n    content, \"file.log\",\n    chunk_size=100,\n    overlap_before=5,   # 5 lines from previous chunk\n    overlap_after=10,   # 10 lines into next chunk\n    unit=\"lines\"\n)\n</code></pre>"},{"location":"overlapping-fallback/#dynamic-overlap","title":"Dynamic Overlap","text":"<p>Automatically adjust overlap based on natural boundaries:</p> <pre><code>chunks = chunker.chunk_with_dynamic_overlap(\n    content, \"document.txt\",\n    chunk_size=1000,\n    min_overlap=50,\n    max_overlap=200,\n    unit=\"characters\"\n)\n</code></pre>"},{"location":"overlapping-fallback/#tree-sitter-protection","title":"Tree-sitter Protection","text":"<p>The chunker will raise an error if you try to use it on files with Tree-sitter support:</p> <pre><code># This will raise TreeSitterOverlapError\ntry:\n    chunks = chunker.chunk_with_overlap(\n        \"def foo(): pass\",\n        \"script.py\",  # Python has Tree-sitter support!\n        chunk_size=100,\n        overlap_size=20\n    )\nexcept TreeSitterOverlapError as e:\n    print(f\"Error: {e}\")\n    # Use regular Tree-sitter chunking instead\n</code></pre>"},{"location":"overlapping-fallback/#supported-file-types","title":"Supported File Types","text":"<p>The overlapping chunker is designed for: - Log files (<code>.log</code>) - Markdown documents (<code>.md</code>) - Plain text files (<code>.txt</code>) - CSV files (<code>.csv</code>) - Configuration files (<code>.conf</code>, <code>.ini</code>) - Any file without Tree-sitter grammar support</p>"},{"location":"overlapping-fallback/#warnings-and-logging","title":"Warnings and Logging","text":"<p>The chunker emits clear warnings when used:</p> <pre><code>WARNING: Using overlapping fallback chunker for app.log. This file has no Tree-sitter support. Overlap strategy: fixed, size: 10 lines\n</code></pre> <p>This ensures users are aware they're not getting AST-based chunking.</p>"},{"location":"overlapping-fallback/#natural-boundary-detection","title":"Natural Boundary Detection","text":"<p>The chunker can find natural boundaries for overlap placement:</p> <pre><code># Find the best overlap boundary near a desired position\nboundary = chunker.find_natural_overlap_boundary(\n    content,\n    desired_position=500,\n    search_window=100\n)\n</code></pre> <p>Priority order for boundaries: 1. Paragraph breaks (<code>\\n\\n</code>) 2. Line breaks (<code>\\n</code>) 3. Sentence ends (<code>.</code>, <code>!</code>, <code>?</code>) 4. Clause boundaries (<code>,</code>, <code>;</code>, <code>:</code>) 5. Word boundaries (spaces)</p>"},{"location":"overlapping-fallback/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Overlapping chunks increase the total data size (each overlap is duplicated)</li> <li>For large files, consider using larger chunk sizes to reduce the number of chunks</li> <li>Character-based chunking is more expensive than line-based for large files</li> </ul>"},{"location":"overlapping-fallback/#example-processing-server-logs","title":"Example: Processing Server Logs","text":"<pre><code># Read log file\nwith open(\"server.log\", \"r\") as f:\n    log_content = f.read()\n\n# Create overlapping chunks for context-aware analysis\nchunker = OverlappingFallbackChunker()\nchunks = chunker.chunk_with_overlap(\n    log_content,\n    \"server.log\",\n    chunk_size=100,     # 100 lines per chunk\n    overlap_size=10,    # 10 lines overlap for context\n    strategy=OverlapStrategy.FIXED,\n    unit=\"lines\"\n)\n\n# Process chunks with context preserved across boundaries\nfor chunk in chunks:\n    # Each chunk has overlap with neighbors\n    # ensuring log entries aren't cut off mid-context\n    analyze_log_chunk(chunk)\n</code></pre>"},{"location":"overlapping-fallback/#integration-with-existing-fallback-system","title":"Integration with Existing Fallback System","text":"<p>The overlapping chunker extends the base fallback chunker, so it inherits all standard fallback functionality while adding overlap support. It can be used as a drop-in replacement when overlap is needed for non-Tree-sitter files.</p>"},{"location":"packaging/","title":"Packaging and Distribution Guide","text":"<p>This guide covers the packaging and distribution process for treesitter-chunker.</p>"},{"location":"packaging/#overview","title":"Overview","text":"<p>TreeSitter Chunker is distributed through multiple channels: - PyPI (Python Package Index) - Conda/Conda-forge - Homebrew (macOS/Linux) - Docker Hub / GitHub Container Registry - Direct downloads (GitHub Releases)</p>"},{"location":"packaging/#building-packages","title":"Building Packages","text":""},{"location":"packaging/#prerequisites","title":"Prerequisites","text":"<p>Install build dependencies:</p> <pre><code>pip install -r requirements-build.txt\n</code></pre>"},{"location":"packaging/#quick-build","title":"Quick Build","text":"<p>Use the automated packaging script:</p> <pre><code>python scripts/package.py --clean --release\n</code></pre>"},{"location":"packaging/#manual-build-process","title":"Manual Build Process","text":""},{"location":"packaging/#1-build-grammars","title":"1. Build Grammars","text":"<pre><code>python scripts/fetch_grammars.py\npython scripts/build_lib.py\n</code></pre>"},{"location":"packaging/#2-build-source-distribution","title":"2. Build Source Distribution","text":"<pre><code>python -m build --sdist\n</code></pre>"},{"location":"packaging/#3-build-wheels","title":"3. Build Wheels","text":"<p>Standard wheel:</p> <pre><code>python -m build --wheel\n</code></pre> <p>Platform-specific wheels:</p> <pre><code>python scripts/build_wheels.py --platform auto\n</code></pre> <p>Cross-platform wheels using cibuildwheel:</p> <pre><code>cibuildwheel --platform auto\n</code></pre>"},{"location":"packaging/#platform-specific-builds","title":"Platform-Specific Builds","text":""},{"location":"packaging/#windows","title":"Windows","text":"<pre><code>scripts\\build_windows.bat\n</code></pre>"},{"location":"packaging/#macos","title":"macOS","text":"<pre><code>./scripts/build_macos.sh\n</code></pre>"},{"location":"packaging/#linux-manylinux","title":"Linux (manylinux)","text":"<pre><code>python scripts/build_wheels.py --platform manylinux\n</code></pre>"},{"location":"packaging/#publishing","title":"Publishing","text":""},{"location":"packaging/#pypi","title":"PyPI","text":"<p>Test PyPI first:</p> <pre><code>python scripts/package.py --test-upload\n</code></pre> <p>Production PyPI:</p> <pre><code>python scripts/package.py --upload\n</code></pre> <p>Manual upload:</p> <pre><code>twine upload --repository testpypi dist/*\ntwine upload dist/*\n</code></pre>"},{"location":"packaging/#docker","title":"Docker","text":"<p>Build images:</p> <pre><code>docker build -t treesitter-chunker:latest .\ndocker build -f Dockerfile.alpine -t treesitter-chunker:alpine .\n</code></pre> <p>Push to registry:</p> <pre><code>docker push ghcr.io/consiliency/treesitter-chunker:latest\ndocker push ghcr.io/consiliency/treesitter-chunker:alpine\n</code></pre>"},{"location":"packaging/#homebrew","title":"Homebrew","text":"<p>Update formula with new version and SHA256:</p> <pre><code># Calculate SHA256\nshasum -a 256 dist/treesitter-chunker-*.tar.gz\n\n# Update homebrew/treesitter-chunker.rb\n# Submit PR to homebrew tap\n</code></pre>"},{"location":"packaging/#conda","title":"Conda","text":"<p>Update meta.yaml and submit to conda-forge:</p> <pre><code># Update conda/meta.yaml with new version\n# Submit PR to conda-forge/staged-recipes\n</code></pre>"},{"location":"packaging/#release-process","title":"Release Process","text":""},{"location":"packaging/#1-update-version","title":"1. Update Version","text":"<p>Update version in: - <code>pyproject.toml</code> - <code>chunker/__init__.py</code> (if applicable) - <code>conda/meta.yaml</code> - <code>homebrew/treesitter-chunker.rb</code></p>"},{"location":"packaging/#2-update-changelog","title":"2. Update Changelog","text":"<p>Update <code>CHANGELOG.md</code> with release notes.</p>"},{"location":"packaging/#3-create-git-tag","title":"3. Create Git Tag","text":"<pre><code>git tag -a v0.1.0 -m \"Release v0.1.0\"\ngit push origin v0.1.0\n</code></pre>"},{"location":"packaging/#4-github-release","title":"4. GitHub Release","text":"<p>The GitHub Actions workflow will automatically: 1. Build all wheels and distributions 2. Create Docker images 3. Create GitHub release with artifacts 4. Upload to PyPI (if configured)</p>"},{"location":"packaging/#5-post-release","title":"5. Post-Release","text":"<ol> <li>Update Homebrew formula</li> <li>Submit conda-forge PR</li> <li>Update documentation</li> <li>Announce release</li> </ol>"},{"location":"packaging/#testing-packages","title":"Testing Packages","text":""},{"location":"packaging/#local-testing","title":"Local Testing","text":"<pre><code>./scripts/test_packaging.sh\n</code></pre>"},{"location":"packaging/#test-installation","title":"Test Installation","text":"<p>PyPI:</p> <pre><code>pip install -i https://test.pypi.org/simple/ treesitter-chunker\n</code></pre> <p>Docker:</p> <pre><code>docker run --rm treesitter-chunker:latest --version\n</code></pre>"},{"location":"packaging/#verification","title":"Verification","text":"<pre><code># Check imports\npython -c \"import chunker; print(chunker.__version__)\"\n\n# Check CLI\ntreesitter-chunker --version\ntreesitter-chunker list-languages\n\n# Test functionality\necho \"def test(): pass\" | treesitter-chunker chunk - -l python\n</code></pre>"},{"location":"packaging/#troubleshooting","title":"Troubleshooting","text":""},{"location":"packaging/#common-issues","title":"Common Issues","text":"<ol> <li>Grammar compilation fails</li> <li>Ensure C/C++ compiler is installed</li> <li> <p>Check tree-sitter version compatibility</p> </li> <li> <p>Wheel building fails</p> </li> <li>Install Visual Studio Build Tools (Windows)</li> <li>Install Xcode Command Line Tools (macOS)</li> <li> <p>Use manylinux Docker image (Linux)</p> </li> <li> <p>Import errors after installation</p> </li> <li>Verify grammars are included in wheel</li> <li>Check platform compatibility</li> <li>Reinstall with <code>--force-reinstall</code></li> </ol>"},{"location":"packaging/#debug-commands","title":"Debug Commands","text":"<pre><code># Check wheel contents\nunzip -l dist/*.whl | grep -E \"\\.so|\\.dll|\\.dylib\"\n\n# Verify package metadata\npython -m pip show treesitter-chunker\n\n# Test in isolated environment\npython -m venv test_env\nsource test_env/bin/activate\npip install dist/*.whl\npython -m chunker.parser\n</code></pre>"},{"location":"packaging/#maintenance","title":"Maintenance","text":""},{"location":"packaging/#updating-dependencies","title":"Updating Dependencies","text":"<ol> <li>Update <code>pyproject.toml</code></li> <li>Update <code>conda/meta.yaml</code></li> <li>Update <code>homebrew/treesitter-chunker.rb</code></li> <li>Test all installation methods</li> </ol>"},{"location":"packaging/#adding-new-languages","title":"Adding New Languages","text":"<ol> <li>Add grammar to <code>scripts/fetch_grammars.py</code></li> <li>Update language list in documentation</li> <li>Add tests for new language</li> <li>Rebuild all packages</li> </ol>"},{"location":"packaging/#security","title":"Security","text":"<ul> <li>Sign releases with GPG</li> <li>Use 2FA for PyPI account</li> <li>Verify checksums in all distribution methods</li> <li>Regular dependency updates</li> </ul>"},{"location":"packaging/#resources","title":"Resources","text":"<ul> <li>Python Packaging Guide</li> <li>cibuildwheel Documentation</li> <li>Homebrew Formula Cookbook</li> <li>Conda-forge Documentation</li> <li>Docker Best Practices</li> </ul>"},{"location":"performance-guide/","title":"Performance Guide","text":"<p>This guide provides comprehensive strategies and best practices for optimizing Tree-sitter Chunker performance. Learn how to leverage caching, parallel processing, and streaming to handle codebases of any size efficiently.</p>"},{"location":"performance-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Performance Overview</li> <li>AST Caching</li> <li>Parallel Processing</li> <li>Streaming Large Files</li> <li>Memory Management</li> <li>Benchmarking</li> <li>Configuration Tuning</li> <li>Common Bottlenecks</li> <li>Performance Monitoring</li> <li>Best Practices</li> </ol>"},{"location":"performance-guide/#performance-overview","title":"Performance Overview","text":"<p>Tree-sitter Chunker is designed for high performance with several key optimizations:</p> <ul> <li>AST Caching: Up to 11.9x speedup for repeated file processing</li> <li>Parser Pooling: Efficient reuse of parser instances</li> <li>Parallel Processing: Near-linear speedup with CPU cores</li> <li>Streaming Support: Process files larger than available memory</li> <li>Lazy Loading: Languages loaded only when needed</li> </ul>"},{"location":"performance-guide/#performance-metrics","title":"Performance Metrics","text":"Operation Performance Notes Parser Creation ~10-50ms One-time cost per language File Parsing O(n) with file size ~1MB/s typical Cached Parse ~0.1ms 11.9x speedup Chunk Extraction O(n) with AST nodes Linear traversal Memory Usage ~10x source size For AST storage"},{"location":"performance-guide/#ast-caching","title":"AST Caching","text":"<p>The AST cache dramatically improves performance when processing files multiple times.</p>"},{"location":"performance-guide/#basic-usage","title":"Basic Usage","text":"<pre><code>from chunker import chunk_file, ASTCache\n\n# Caching is enabled by default\nchunks1 = chunk_file(\"large_file.py\", \"python\")  # Parses file\nchunks2 = chunk_file(\"large_file.py\", \"python\")  # Uses cache (11.9x faster)\n</code></pre>"},{"location":"performance-guide/#cache-configuration","title":"Cache Configuration","text":"<pre><code>from chunker import ASTCache\n\n# Create cache with custom size\ncache = ASTCache(max_size=500)  # Cache up to 500 ASTs\n\n# Monitor cache performance\nstats = cache.get_stats()\nprint(f\"Hit rate: {stats['hit_rate']:.2%}\")\nprint(f\"Hits: {stats['hits']}, Misses: {stats['misses']}\")\nprint(f\"Current size: {stats['size']}/{stats['max_size']}\")\n\n# Clear cache when needed\ncache.clear()\n</code></pre>"},{"location":"performance-guide/#cache-key-strategy","title":"Cache Key Strategy","text":"<p>The cache uses a composite key based on: - File path (absolute) - File modification time - File size - Language</p> <p>This ensures cache invalidation when files change.</p>"},{"location":"performance-guide/#advanced-caching","title":"Advanced Caching","text":"<pre><code>from chunker import ASTCache\nfrom pathlib import Path\nimport time\n\nclass SmartChunker:\n    def __init__(self, cache_size=200):\n        self.cache = ASTCache(max_size=cache_size)\n\n    def process_with_cache_warmup(self, files, language):\n        \"\"\"Warm up cache for frequently accessed files.\"\"\"\n        # Pre-process popular files\n        popular_files = self.identify_popular_files(files)\n        for file in popular_files[:self.cache.max_size]:\n            chunk_file(file, language)  # Warm cache\n\n        # Process all files\n        results = {}\n        for file in files:\n            results[file] = chunk_file(file, language)\n\n        return results\n\n    def monitor_cache_efficiency(self):\n        \"\"\"Monitor and report cache efficiency.\"\"\"\n        stats = self.cache.get_stats()\n        if stats['hit_rate'] &lt; 0.5 and stats['size'] == stats['max_size']:\n            print(\"Consider increasing cache size for better performance\")\n        return stats\n</code></pre>"},{"location":"performance-guide/#parallel-processing","title":"Parallel Processing","text":"<p>Leverage multiple CPU cores for processing many files simultaneously.</p>"},{"location":"performance-guide/#basic-parallel-processing","title":"Basic Parallel Processing","text":"<pre><code>from chunker import chunk_files_parallel\n\n# Process multiple files in parallel\nfiles = [\"file1.py\", \"file2.py\", \"file3.py\", \"file4.py\"]\nresults = chunk_files_parallel(\n    files,\n    \"python\",\n    max_workers=4,  # Use 4 CPU cores\n    show_progress=True\n)\n\n# Results is a dict mapping file paths to chunks\nfor file_path, chunks in results.items():\n    print(f\"{file_path}: {len(chunks)} chunks\")\n</code></pre>"},{"location":"performance-guide/#directory-processing","title":"Directory Processing","text":"<pre><code>from chunker import chunk_directory_parallel\n\n# Process entire directory tree\nresults = chunk_directory_parallel(\n    \"src/\",\n    \"python\",\n    pattern=\"**/*.py\",  # Glob pattern\n    max_workers=8,\n    show_progress=True\n)\n\nprint(f\"Processed {len(results)} files\")\n</code></pre>"},{"location":"performance-guide/#custom-parallel-implementation","title":"Custom Parallel Implementation","text":"<pre><code>from concurrent.futures import ProcessPoolExecutor, as_completed\nfrom chunker import chunk_file\nimport multiprocessing as mp\n\ndef process_large_codebase(directory, language):\n    \"\"\"Custom parallel processing with fine control.\"\"\"\n    from pathlib import Path\n\n    # Find all files\n    files = list(Path(directory).rglob(f\"*.{language[:2]}\"))\n\n    # Determine optimal worker count\n    cpu_count = mp.cpu_count()\n    worker_count = min(cpu_count, len(files), 32)  # Cap at 32\n\n    results = {}\n    failed = []\n\n    # Process with progress tracking\n    with ProcessPoolExecutor(max_workers=worker_count) as executor:\n        # Submit all tasks\n        future_to_file = {\n            executor.submit(chunk_file, str(f), language): f \n            for f in files\n        }\n\n        # Process completed tasks\n        for future in as_completed(future_to_file):\n            file = future_to_file[future]\n            try:\n                chunks = future.result(timeout=30)\n                results[str(file)] = chunks\n            except Exception as e:\n                print(f\"Failed to process {file}: {e}\")\n                failed.append(file)\n\n    return results, failed\n\n# Use it\nresults, failed = process_large_codebase(\"large_project/\", \"python\")\nprint(f\"Processed: {len(results)}, Failed: {len(failed)}\")\n</code></pre>"},{"location":"performance-guide/#parallel-processing-best-practices","title":"Parallel Processing Best Practices","text":"<ol> <li>Worker Count: Use <code>min(cpu_count, file_count)</code> workers</li> <li>Batch Size: Group small files to reduce overhead</li> <li>Memory Limits: Monitor memory usage with many workers</li> <li>Error Handling: Isolate failures to individual files</li> </ol>"},{"location":"performance-guide/#streaming-large-files","title":"Streaming Large Files","text":"<p>For files too large to fit in memory, use streaming processing.</p>"},{"location":"performance-guide/#basic-streaming","title":"Basic Streaming","text":"<pre><code>from chunker import chunk_file_streaming\n\n# Process a very large file\nfor chunk in chunk_file_streaming(\"huge_codebase.py\", \"python\"):\n    # Each chunk is yielded as it's found\n    print(f\"Found {chunk.node_type} at lines {chunk.start_line}-{chunk.end_line}\")\n\n    # Process immediately without storing all chunks\n    if chunk.node_type == \"function_definition\":\n        analyze_function(chunk)\n</code></pre>"},{"location":"performance-guide/#streaming-with-batching","title":"Streaming with Batching","text":"<pre><code>from chunker import chunk_file_streaming\nfrom itertools import islice\n\ndef process_in_batches(file_path, language, batch_size=100):\n    \"\"\"Process chunks in batches to balance memory and efficiency.\"\"\"\n    stream = chunk_file_streaming(file_path, language)\n\n    while True:\n        # Get next batch\n        batch = list(islice(stream, batch_size))\n        if not batch:\n            break\n\n        # Process batch\n        process_batch(batch)\n\n        # Optional: Clear memory between batches\n        import gc\n        gc.collect()\n\ndef process_batch(chunks):\n    \"\"\"Process a batch of chunks.\"\"\"\n    # Example: Save to database\n    records = [chunk_to_record(chunk) for chunk in chunks]\n    db.insert_many(records)\n</code></pre>"},{"location":"performance-guide/#custom-streaming-implementation","title":"Custom Streaming Implementation","text":"<pre><code>from chunker import StreamingChunker\nimport mmap\n\nclass MemoryEfficientChunker:\n    def __init__(self, language):\n        self.chunker = StreamingChunker(language)\n\n    def process_huge_file(self, file_path):\n        \"\"\"Process files of any size efficiently.\"\"\"\n        with open(file_path, 'rb') as f:\n            # Use memory mapping for efficient access\n            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped:\n                # Process in chunks\n                chunk_size = 1024 * 1024  # 1MB chunks\n                offset = 0\n\n                while offset &lt; len(mmapped):\n                    # Read chunk\n                    end = min(offset + chunk_size, len(mmapped))\n                    data = mmapped[offset:end]\n\n                    # Process chunk\n                    for code_chunk in self.chunker.process_bytes(data, offset):\n                        yield code_chunk\n\n                    offset = end\n</code></pre>"},{"location":"performance-guide/#memory-management","title":"Memory Management","text":"<p>Optimize memory usage for large-scale processing.</p>"},{"location":"performance-guide/#memory-profiling","title":"Memory Profiling","text":"<pre><code>import psutil\nimport os\nfrom chunker import chunk_file\n\ndef profile_memory_usage(file_path, language):\n    \"\"\"Profile memory usage during chunking.\"\"\"\n    process = psutil.Process(os.getpid())\n\n    # Baseline memory\n    baseline = process.memory_info().rss / 1024 / 1024  # MB\n\n    # Process file\n    chunks = chunk_file(file_path, language)\n\n    # Peak memory\n    peak = process.memory_info().rss / 1024 / 1024  # MB\n\n    print(f\"Memory usage: {peak - baseline:.1f} MB\")\n    print(f\"Memory per chunk: {(peak - baseline) / len(chunks):.2f} MB\")\n\n    return chunks\n</code></pre>"},{"location":"performance-guide/#memory-optimization-strategies","title":"Memory Optimization Strategies","text":"<pre><code>from chunker import chunk_files_parallel, ASTCache\nimport gc\n\nclass MemoryOptimizedProcessor:\n    def __init__(self):\n        # Smaller cache for memory-constrained environments\n        self.cache = ASTCache(max_size=50)\n\n    def process_with_memory_limit(self, files, language, memory_limit_mb=1000):\n        \"\"\"Process files while staying within memory limit.\"\"\"\n        import resource\n\n        # Set memory limit (Unix only)\n        if hasattr(resource, 'RLIMIT_AS'):\n            resource.setrlimit(\n                resource.RLIMIT_AS,\n                (memory_limit_mb * 1024 * 1024, -1)\n            )\n\n        # Process in smaller batches\n        batch_size = max(1, memory_limit_mb // 100)  # Rough estimate\n        results = {}\n\n        for i in range(0, len(files), batch_size):\n            batch = files[i:i + batch_size]\n            batch_results = chunk_files_parallel(batch, language, max_workers=2)\n            results.update(batch_results)\n\n            # Force garbage collection between batches\n            gc.collect()\n\n            # Clear cache if memory pressure\n            if self.get_memory_usage() &gt; memory_limit_mb * 0.8:\n                self.cache.clear()\n\n        return results\n\n    def get_memory_usage(self):\n        \"\"\"Get current memory usage in MB.\"\"\"\n        import psutil\n        process = psutil.Process(os.getpid())\n        return process.memory_info().rss / 1024 / 1024\n</code></pre>"},{"location":"performance-guide/#benchmarking","title":"Benchmarking","text":"<p>Measure and compare performance across different scenarios.</p>"},{"location":"performance-guide/#basic-benchmarking","title":"Basic Benchmarking","text":"<pre><code>import time\nfrom chunker import chunk_file, chunk_files_parallel\n\ndef benchmark_single_vs_parallel(files, language):\n    \"\"\"Compare single-threaded vs parallel processing.\"\"\"\n\n    # Single-threaded\n    start = time.time()\n    results_single = {}\n    for file in files:\n        results_single[file] = chunk_file(file, language)\n    single_time = time.time() - start\n\n    # Clear cache for fair comparison\n    from chunker import clear_cache\n    clear_cache()\n\n    # Parallel\n    start = time.time()\n    results_parallel = chunk_files_parallel(files, language)\n    parallel_time = time.time() - start\n\n    print(f\"Single-threaded: {single_time:.2f}s\")\n    print(f\"Parallel: {parallel_time:.2f}s\")\n    print(f\"Speedup: {single_time / parallel_time:.2f}x\")\n\n    return results_parallel\n</code></pre>"},{"location":"performance-guide/#comprehensive-benchmark-suite","title":"Comprehensive Benchmark Suite","text":"<pre><code>import time\nimport statistics\nfrom pathlib import Path\nfrom chunker import chunk_file, ASTCache, chunk_file_streaming\n\nclass ChunkerBenchmark:\n    def __init__(self):\n        self.results = {}\n\n    def benchmark_cache_performance(self, file_path, language, iterations=10):\n        \"\"\"Benchmark cache hit performance.\"\"\"\n        times_cold = []\n        times_hot = []\n\n        for i in range(iterations):\n            # Cold cache\n            cache = ASTCache()\n            cache.clear()\n\n            start = time.perf_counter()\n            chunk_file(file_path, language)\n            times_cold.append(time.perf_counter() - start)\n\n            # Hot cache\n            start = time.perf_counter()\n            chunk_file(file_path, language)\n            times_hot.append(time.perf_counter() - start)\n\n        cold_avg = statistics.mean(times_cold)\n        hot_avg = statistics.mean(times_hot)\n\n        self.results['cache'] = {\n            'cold_avg': cold_avg,\n            'hot_avg': hot_avg,\n            'speedup': cold_avg / hot_avg,\n            'cold_stdev': statistics.stdev(times_cold),\n            'hot_stdev': statistics.stdev(times_hot)\n        }\n\n        print(f\"Cache Performance:\")\n        print(f\"  Cold: {cold_avg*1000:.1f}ms \u00b1 {statistics.stdev(times_cold)*1000:.1f}ms\")\n        print(f\"  Hot:  {hot_avg*1000:.1f}ms \u00b1 {statistics.stdev(times_hot)*1000:.1f}ms\")\n        print(f\"  Speedup: {cold_avg / hot_avg:.1f}x\")\n\n    def benchmark_file_sizes(self, language):\n        \"\"\"Benchmark performance across different file sizes.\"\"\"\n        # Create test files of different sizes\n        test_sizes = [100, 1000, 10000, 100000]  # lines\n\n        for size in test_sizes:\n            content = self.generate_test_file(size, language)\n            file_path = f\"test_{size}.{language[:2]}\"\n\n            with open(file_path, 'w') as f:\n                f.write(content)\n\n            start = time.perf_counter()\n            chunks = chunk_file(file_path, language)\n            elapsed = time.perf_counter() - start\n\n            print(f\"{size} lines: {elapsed*1000:.1f}ms, {len(chunks)} chunks\")\n            print(f\"  Rate: {size/elapsed:.0f} lines/sec\")\n\n            Path(file_path).unlink()  # Clean up\n\n    def generate_test_file(self, lines, language):\n        \"\"\"Generate test file with specified number of lines.\"\"\"\n        if language == \"python\":\n            template = \"\"\"def function_{i}(x, y):\n    \\\"\\\"\\\"Function {i} docstring.\\\"\\\"\\\"\n    result = x + y + {i}\n    return result\n\n\"\"\"\n            functions = lines // 5  # Each function is ~5 lines\n            return '\\n'.join(template.format(i=i) for i in range(functions))\n\n        # Add other languages as needed\n        return '\\n' * lines\n</code></pre>"},{"location":"performance-guide/#configuration-tuning","title":"Configuration Tuning","text":"<p>Optimize configuration for your specific use case.</p>"},{"location":"performance-guide/#cache-size-tuning","title":"Cache Size Tuning","text":"<pre><code>from chunker import ASTCache\nimport psutil\n\ndef determine_optimal_cache_size():\n    \"\"\"Determine optimal cache size based on available memory.\"\"\"\n    # Get available memory\n    available_mb = psutil.virtual_memory().available / 1024 / 1024\n\n    # Use 10% of available memory for cache (rough estimate)\n    # Assume average AST size of 1MB\n    optimal_size = int(available_mb * 0.1)\n\n    # Apply reasonable bounds\n    optimal_size = max(50, min(optimal_size, 1000))\n\n    print(f\"Recommended cache size: {optimal_size} entries\")\n    return optimal_size\n\n# Use it\ncache = ASTCache(max_size=determine_optimal_cache_size())\n</code></pre>"},{"location":"performance-guide/#worker-count-optimization","title":"Worker Count Optimization","text":"<pre><code>import multiprocessing as mp\nfrom chunker import chunk_files_parallel\n\ndef determine_optimal_workers(file_count):\n    \"\"\"Determine optimal number of workers.\"\"\"\n    cpu_count = mp.cpu_count()\n\n    # Rules of thumb:\n    # - Don't exceed CPU count\n    # - Don't create more workers than files\n    # - Account for memory constraints\n    # - Leave some CPUs for system\n\n    if file_count &lt; 10:\n        return min(file_count, 2)\n    elif file_count &lt; 100:\n        return min(file_count, cpu_count // 2)\n    else:\n        return min(32, cpu_count - 1)  # Leave one CPU free\n\n# Use it\nfiles = [\"file1.py\", \"file2.py\", ...]  # Your files\noptimal_workers = determine_optimal_workers(len(files))\nresults = chunk_files_parallel(files, \"python\", max_workers=optimal_workers)\n</code></pre>"},{"location":"performance-guide/#configuration-file-optimization","title":"Configuration File Optimization","text":"<pre><code># chunker.config.toml - Optimized for large codebases\n\n# Performance settings\n[performance]\ncache_size = 500  # Increase for frequently accessed files\nparser_pool_size = 10  # Number of parsers per language\nparallel_workers = 8  # Adjust based on CPU count\n\n# Memory management\n[memory]\nmax_file_size_mb = 50  # Stream files larger than this\nstreaming_chunk_size = 1048576  # 1MB chunks for streaming\ngc_threshold = 100  # Run GC after processing N files\n\n# Language-specific optimizations\n[languages.python]\nenabled = true\n# Only chunk what you need\nchunk_types = [\"function_definition\", \"class_definition\"]\nmin_chunk_size = 5  # Skip tiny functions\nmax_chunk_size = 500  # Split huge functions\n\n[languages.javascript]\nenabled = true\n# Include only important constructs\nchunk_types = [\"function_declaration\", \"class_declaration\", \"arrow_function\"]\n# Skip minified files\nexclude_patterns = [\"*.min.js\", \"*bundle.js\"]\n</code></pre>"},{"location":"performance-guide/#common-bottlenecks","title":"Common Bottlenecks","text":""},{"location":"performance-guide/#1-parser-creation-overhead","title":"1. Parser Creation Overhead","text":"<p>Problem: Creating parsers is expensive (~10-50ms each).</p> <p>Solution: Reuse parsers with pooling.</p> <pre><code>from chunker import get_parser, return_parser\n\n# Bad: Creating new parser each time\ndef process_files_slow(files, language):\n    results = []\n    for file in files:\n        parser = get_parser(language)  # Expensive!\n        # ... use parser ...\n    return results\n\n# Good: Reuse parser\ndef process_files_fast(files, language):\n    parser = get_parser(language)  # Create once\n    try:\n        results = []\n        for file in files:\n            # ... use same parser ...\n            results.append(result)\n        return results\n    finally:\n        return_parser(language, parser)  # Return for reuse\n</code></pre>"},{"location":"performance-guide/#2-memory-exhaustion","title":"2. Memory Exhaustion","text":"<p>Problem: Processing too many large files at once.</p> <p>Solution: Use streaming and batching.</p> <pre><code># Bad: Load everything into memory\ndef process_all_at_once(directory, language):\n    all_chunks = []\n    for file in Path(directory).rglob(\"*.py\"):\n        chunks = chunk_file(file, language)\n        all_chunks.extend(chunks)  # Memory grows unbounded!\n    return all_chunks\n\n# Good: Process and release\ndef process_incrementally(directory, language):\n    for file in Path(directory).rglob(\"*.py\"):\n        chunks = chunk_file(file, language)\n        yield from chunks  # Yield immediately\n        # Chunks are garbage collected after use\n</code></pre>"},{"location":"performance-guide/#3-cache-thrashing","title":"3. Cache Thrashing","text":"<p>Problem: Cache constantly evicting useful entries.</p> <p>Solution: Increase cache size or implement smarter eviction.</p> <pre><code>from chunker import ASTCache\n\nclass SmartCache(ASTCache):\n    def __init__(self, max_size=100):\n        super().__init__(max_size)\n        self.access_counts = {}\n\n    def get(self, file_path, language):\n        result = super().get(file_path, language)\n        if result:\n            # Track access frequency\n            key = (str(file_path), language)\n            self.access_counts[key] = self.access_counts.get(key, 0) + 1\n        return result\n\n    def evict_least_frequently_used(self):\n        \"\"\"Evict based on access frequency instead of recency.\"\"\"\n        if len(self.cache) &gt;= self.max_size:\n            # Find least frequently used\n            lfu_key = min(self.access_counts.items(), key=lambda x: x[1])[0]\n            del self.cache[lfu_key]\n            del self.access_counts[lfu_key]\n</code></pre>"},{"location":"performance-guide/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"performance-guide/#real-time-monitoring","title":"Real-time Monitoring","text":"<pre><code>import time\nimport psutil\nimport threading\nfrom chunker import chunk_files_parallel\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.running = False\n        self.stats = {\n            'files_processed': 0,\n            'chunks_extracted': 0,\n            'processing_time': 0,\n            'peak_memory_mb': 0,\n            'cpu_percent': []\n        }\n\n    def start_monitoring(self):\n        \"\"\"Start background monitoring thread.\"\"\"\n        self.running = True\n        self.monitor_thread = threading.Thread(target=self._monitor)\n        self.monitor_thread.start()\n\n    def stop_monitoring(self):\n        \"\"\"Stop monitoring and return stats.\"\"\"\n        self.running = False\n        self.monitor_thread.join()\n        return self.stats\n\n    def _monitor(self):\n        \"\"\"Background monitoring loop.\"\"\"\n        process = psutil.Process()\n\n        while self.running:\n            # CPU usage\n            cpu = process.cpu_percent(interval=0.1)\n            self.stats['cpu_percent'].append(cpu)\n\n            # Memory usage\n            memory_mb = process.memory_info().rss / 1024 / 1024\n            self.stats['peak_memory_mb'] = max(\n                self.stats['peak_memory_mb'], \n                memory_mb\n            )\n\n            time.sleep(0.1)\n\n    def process_with_monitoring(self, files, language):\n        \"\"\"Process files while monitoring performance.\"\"\"\n        self.start_monitoring()\n        start_time = time.time()\n\n        try:\n            results = chunk_files_parallel(files, language)\n\n            # Update stats\n            self.stats['files_processed'] = len(results)\n            self.stats['chunks_extracted'] = sum(\n                len(chunks) for chunks in results.values()\n            )\n            self.stats['processing_time'] = time.time() - start_time\n\n            return results\n        finally:\n            stats = self.stop_monitoring()\n            self.print_report(stats)\n\n    def print_report(self, stats):\n        \"\"\"Print performance report.\"\"\"\n        print(\"\\nPerformance Report:\")\n        print(f\"Files processed: {stats['files_processed']}\")\n        print(f\"Chunks extracted: {stats['chunks_extracted']}\")\n        print(f\"Processing time: {stats['processing_time']:.2f}s\")\n        print(f\"Peak memory: {stats['peak_memory_mb']:.1f} MB\")\n\n        if stats['cpu_percent']:\n            avg_cpu = sum(stats['cpu_percent']) / len(stats['cpu_percent'])\n            print(f\"Average CPU: {avg_cpu:.1f}%\")\n\n        if stats['processing_time'] &gt; 0:\n            rate = stats['files_processed'] / stats['processing_time']\n            print(f\"Processing rate: {rate:.1f} files/sec\")\n</code></pre>"},{"location":"performance-guide/#logging-performance-metrics","title":"Logging Performance Metrics","text":"<pre><code>import logging\nfrom functools import wraps\nfrom chunker import chunk_file\n\n# Configure performance logger\nperf_logger = logging.getLogger('chunker.performance')\nperf_logger.setLevel(logging.INFO)\n\ndef log_performance(func):\n    \"\"\"Decorator to log function performance.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.perf_counter()\n        result = func(*args, **kwargs)\n        elapsed = time.perf_counter() - start\n\n        perf_logger.info(\n            f\"{func.__name__} completed in {elapsed:.3f}s\"\n        )\n        return result\n    return wrapper\n\n# Use it\n@log_performance\ndef process_codebase(directory, language):\n    # Your processing logic\n    pass\n</code></pre>"},{"location":"performance-guide/#best-practices","title":"Best Practices","text":""},{"location":"performance-guide/#1-profile-before-optimizing","title":"1. Profile Before Optimizing","text":"<p>Always measure before optimizing:</p> <pre><code>import cProfile\nimport pstats\n\ndef profile_chunking(file_path, language):\n    \"\"\"Profile chunking performance.\"\"\"\n    profiler = cProfile.Profile()\n    profiler.enable()\n\n    # Run the code\n    chunks = chunk_file(file_path, language)\n\n    profiler.disable()\n\n    # Print stats\n    stats = pstats.Stats(profiler)\n    stats.sort_stats('cumulative')\n    stats.print_stats(10)  # Top 10 functions\n\n    return chunks\n</code></pre>"},{"location":"performance-guide/#2-choose-the-right-tool","title":"2. Choose the Right Tool","text":"<ul> <li>Small files (&lt;1MB): Use <code>chunk_file()</code></li> <li>Many files: Use <code>chunk_files_parallel()</code></li> <li>Large files (&gt;10MB): Use <code>chunk_file_streaming()</code></li> <li>Entire codebases: Use <code>chunk_directory_parallel()</code></li> </ul>"},{"location":"performance-guide/#3-optimize-for-your-use-case","title":"3. Optimize for Your Use Case","text":"<pre><code># For CI/CD - Speed matters most\nconfig = {\n    'max_workers': mp.cpu_count(),\n    'cache_size': 1000,\n    'show_progress': False\n}\n\n# For development - Memory efficiency matters\nconfig = {\n    'max_workers': 2,\n    'cache_size': 100,\n    'streaming_threshold': 5 * 1024 * 1024  # 5MB\n}\n\n# For analysis - Completeness matters\nconfig = {\n    'max_workers': 4,\n    'timeout': None,  # No timeout\n    'ignore_errors': False\n}\n</code></pre>"},{"location":"performance-guide/#4-monitor-and-adjust","title":"4. Monitor and Adjust","text":"<p>Continuously monitor and adjust based on real-world usage:</p> <pre><code>from chunker import ASTCache\n\n# Periodic cache effectiveness check\ndef check_cache_effectiveness():\n    cache = ASTCache()\n    stats = cache.get_stats()\n\n    if stats['hit_rate'] &lt; 0.3:\n        print(\"Low cache hit rate - consider:\")\n        print(\"- Increasing cache size\")\n        print(\"- Pre-warming cache with common files\")\n        print(\"- Checking if files are being modified\")\n\n    if stats['size'] == stats['max_size'] and stats['misses'] &gt; stats['hits']:\n        print(\"Cache is full but ineffective - increase size\")\n</code></pre>"},{"location":"performance-guide/#5-error-recovery","title":"5. Error Recovery","text":"<p>Build resilient systems that handle failures gracefully:</p> <pre><code>def robust_processing(files, language, max_retries=3):\n    \"\"\"Process files with retry logic and error handling.\"\"\"\n    results = {}\n    failed = []\n\n    for file in files:\n        for attempt in range(max_retries):\n            try:\n                chunks = chunk_file(file, language)\n                results[file] = chunks\n                break\n            except Exception as e:\n                if attempt == max_retries - 1:\n                    print(f\"Failed to process {file} after {max_retries} attempts: {e}\")\n                    failed.append((file, str(e)))\n                else:\n                    time.sleep(0.1 * (attempt + 1))  # Exponential backoff\n\n    return results, failed\n</code></pre>"},{"location":"performance-guide/#see-also","title":"See Also","text":"<ul> <li>API Reference - Complete API documentation</li> <li>User Guide - General usage guide</li> <li>Export Formats - Output format optimization</li> <li>Configuration - Performance-related settings</li> </ul>"},{"location":"plugin-development/","title":"Plugin Development Guide","text":"<p>This guide covers how to create custom language plugins for Tree-sitter Chunker. The plugin architecture allows you to add support for new languages or customize the chunking behavior for existing languages.</p>"},{"location":"plugin-development/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Quick Start</li> <li>Plugin Architecture</li> <li>Creating a Language Plugin</li> <li>Plugin Configuration</li> <li>Advanced Features</li> <li>Testing Your Plugin</li> <li>Distributing Plugins</li> <li>Built-in Plugin Examples</li> <li>Best Practices</li> </ol>"},{"location":"plugin-development/#overview","title":"Overview","text":"<p>The Tree-sitter Chunker plugin system provides:</p> <ul> <li>Extensibility: Add support for new languages without modifying core code</li> <li>Customization: Override default chunking behavior for specific languages</li> <li>Configuration: Fine-tune plugin behavior through configuration files</li> <li>Discovery: Automatic plugin discovery from directories</li> <li>Hot Loading: Load plugins at runtime</li> </ul>"},{"location":"plugin-development/#quick-start","title":"Quick Start","text":"<p>Here's a minimal example of a custom language plugin:</p> <pre><code>from chunker.languages.plugin_base import LanguagePlugin\nfrom typing import Set, Optional\nfrom tree_sitter import Node\n\nclass GoPlugin(LanguagePlugin):\n    \"\"\"Plugin for Go language support.\"\"\"\n\n    @property\n    def language_name(self) -&gt; str:\n        return \"go\"\n\n    @property\n    def supported_extensions(self) -&gt; Set[str]:\n        return {\".go\"}\n\n    @property\n    def default_chunk_types(self) -&gt; Set[str]:\n        return {\n            \"function_declaration\",\n            \"method_declaration\",\n            \"type_declaration\",\n            \"interface_declaration\"\n        }\n\n    def get_node_name(self, node: Node, source: bytes) -&gt; Optional[str]:\n        \"\"\"Extract the name from a Go node.\"\"\"\n        for child in node.children:\n            if child.type == \"identifier\":\n                return source[child.start_byte:child.end_byte].decode('utf-8')\n        return None\n</code></pre> <p>To use this plugin:</p> <pre><code>from chunker import get_plugin_manager\n\nmanager = get_plugin_manager()\nmanager.register_plugin(GoPlugin)\n\n# Now you can chunk Go files\nchunks = chunk_file(\"main.go\", \"go\")\n</code></pre>"},{"location":"plugin-development/#plugin-architecture","title":"Plugin Architecture","text":""},{"location":"plugin-development/#core-components","title":"Core Components","text":"<ol> <li>LanguagePlugin: Abstract base class that all plugins must inherit from</li> <li>PluginManager: Manages plugin discovery, loading, and lifecycle</li> <li>PluginRegistry: Internal registry of available plugins</li> <li>PluginConfig: Configuration for individual plugins</li> </ol>"},{"location":"plugin-development/#plugin-lifecycle","title":"Plugin Lifecycle","text":"<pre><code>graph TD\n    A[Plugin Class Definition] --&gt; B[Plugin Discovery]\n    B --&gt; C[Plugin Registration]\n    C --&gt; D[Plugin Instantiation]\n    D --&gt; E[Plugin Configuration]\n    E --&gt; F[Plugin Usage]\n    F --&gt; G[Chunk Processing]\n</code></pre>"},{"location":"plugin-development/#key-interfaces","title":"Key Interfaces","text":"<pre><code>class LanguagePlugin(ABC):\n    \"\"\"Base class for all language plugins.\"\"\"\n\n    @property\n    @abstractmethod\n    def language_name(self) -&gt; str:\n        \"\"\"Return the language identifier (e.g., 'python', 'rust').\"\"\"\n\n    @property\n    @abstractmethod\n    def supported_extensions(self) -&gt; Set[str]:\n        \"\"\"Return set of file extensions this plugin handles.\"\"\"\n\n    @property\n    @abstractmethod\n    def default_chunk_types(self) -&gt; Set[str]:\n        \"\"\"Return default set of node types to chunk.\"\"\"\n\n    @abstractmethod\n    def get_node_name(self, node: Node, source: bytes) -&gt; Optional[str]:\n        \"\"\"Extract a human-readable name from a node.\"\"\"\n\n    # Optional methods to override\n    def should_include_chunk(self, chunk: CodeChunk) -&gt; bool:\n        \"\"\"Filter chunks based on custom criteria.\"\"\"\n\n    def process_node(self, node: Node, source: bytes, \n                    file_path: str, parent_context: Optional[str] = None) -&gt; Optional[CodeChunk]:\n        \"\"\"Process a node and return a chunk if appropriate.\"\"\"\n\n    def get_context_for_children(self, node: Node, chunk: CodeChunk) -&gt; str:\n        \"\"\"Build context string for nested definitions.\"\"\"\n</code></pre>"},{"location":"plugin-development/#creating-a-language-plugin","title":"Creating a Language Plugin","text":""},{"location":"plugin-development/#step-1-define-your-plugin-class","title":"Step 1: Define Your Plugin Class","text":"<pre><code>from chunker.languages.plugin_base import LanguagePlugin\nfrom chunker.languages.base import PluginConfig\nfrom typing import Set, Optional, Dict, Any\nfrom tree_sitter import Node\nimport re\n\nclass SwiftPlugin(LanguagePlugin):\n    \"\"\"Plugin for Swift language support.\"\"\"\n\n    def __init__(self, config: Optional[PluginConfig] = None):\n        super().__init__(config)\n        self._method_pattern = re.compile(r'func\\s+(\\w+)')\n        self._class_pattern = re.compile(r'class\\s+(\\w+)')\n\n    @property\n    def language_name(self) -&gt; str:\n        return \"swift\"\n\n    @property\n    def supported_extensions(self) -&gt; Set[str]:\n        return {\".swift\"}\n\n    @property\n    def default_chunk_types(self) -&gt; Set[str]:\n        return {\n            \"function_declaration\",\n            \"init_declaration\",\n            \"class_declaration\",\n            \"struct_declaration\",\n            \"enum_declaration\",\n            \"protocol_declaration\",\n            \"extension_declaration\"\n        }\n\n    @property\n    def plugin_metadata(self) -&gt; Dict[str, Any]:\n        \"\"\"Return plugin metadata.\"\"\"\n        return {\n            \"name\": \"Swift Language Plugin\",\n            \"version\": \"1.0.0\",\n            \"author\": \"Your Name\",\n            \"description\": \"Provides Swift language support for chunking\"\n        }\n</code></pre>"},{"location":"plugin-development/#step-2-implement-name-extraction","title":"Step 2: Implement Name Extraction","text":"<pre><code>def get_node_name(self, node: Node, source: bytes) -&gt; Optional[str]:\n    \"\"\"Extract the name from a Swift node.\"\"\"\n    # Different node types have different structures\n    if node.type == \"function_declaration\":\n        # Look for the identifier after 'func' keyword\n        for child in node.children:\n            if child.type == \"simple_identifier\":\n                return source[child.start_byte:child.end_byte].decode('utf-8')\n\n    elif node.type in [\"class_declaration\", \"struct_declaration\"]:\n        # Look for type identifier\n        for child in node.children:\n            if child.type == \"type_identifier\":\n                return source[child.start_byte:child.end_byte].decode('utf-8')\n\n    # Fallback to regex pattern matching\n    content = source[node.start_byte:node.end_byte].decode('utf-8')\n    first_line = content.split('\\n')[0]\n\n    if \"func\" in first_line:\n        match = self._method_pattern.search(first_line)\n        if match:\n            return match.group(1)\n    elif \"class\" in first_line:\n        match = self._class_pattern.search(first_line)\n        if match:\n            return match.group(1)\n\n    return None\n</code></pre>"},{"location":"plugin-development/#step-3-implement-custom-chunk-processing","title":"Step 3: Implement Custom Chunk Processing","text":"<pre><code>def process_node(self, node: Node, source: bytes, \n                file_path: str, parent_context: Optional[str] = None) -&gt; Optional[CodeChunk]:\n    \"\"\"Process Swift nodes with special handling.\"\"\"\n\n    # Skip private methods if configured\n    if self.config.custom_options.get(\"skip_private\", False):\n        content = source[node.start_byte:node.end_byte].decode('utf-8')\n        if content.strip().startswith(\"private \"):\n            return None\n\n    # Handle computed properties specially\n    if node.type == \"computed_property\":\n        # Create a custom chunk for computed properties\n        chunk = self.create_chunk(node, source, file_path, parent_context)\n        if chunk:\n            chunk.node_type = \"computed_property\"\n            # Add metadata about getter/setter\n            has_getter = \"get {\" in chunk.content\n            has_setter = \"set {\" in chunk.content\n            chunk.metadata = {\n                \"has_getter\": has_getter,\n                \"has_setter\": has_setter\n            }\n        return chunk\n\n    # Default processing for other nodes\n    return super().process_node(node, source, file_path, parent_context)\n</code></pre>"},{"location":"plugin-development/#step-4-add-custom-filtering","title":"Step 4: Add Custom Filtering","text":"<pre><code>def should_include_chunk(self, chunk: CodeChunk) -&gt; bool:\n    \"\"\"Filter chunks based on Swift-specific criteria.\"\"\"\n    # Always include if no size constraints\n    if not self.config:\n        return True\n\n    # Apply size constraints\n    chunk_lines = chunk.end_line - chunk.start_line + 1\n    if chunk_lines &lt; self.config.min_chunk_size:\n        return False\n    if chunk_lines &gt; self.config.max_chunk_size:\n        return False\n\n    # Skip test methods if configured\n    if self.config.custom_options.get(\"skip_tests\", False):\n        if chunk.file_path.endswith(\"Tests.swift\") or \"test\" in chunk.content.lower():\n            return False\n\n    # Skip generated code\n    if \"// Generated code - do not modify\" in chunk.content:\n        return False\n\n    return True\n</code></pre>"},{"location":"plugin-development/#step-5-context-building","title":"Step 5: Context Building","text":"<pre><code>def get_context_for_children(self, node: Node, chunk: CodeChunk) -&gt; str:\n    \"\"\"Build context string for nested Swift definitions.\"\"\"\n    name = self.get_node_name(node, chunk.content.encode('utf-8'))\n\n    if not name:\n        return chunk.parent_context\n\n    # Build hierarchical context\n    context_parts = []\n    if chunk.parent_context:\n        context_parts.append(chunk.parent_context)\n\n    # Add type information to context\n    if node.type == \"class_declaration\":\n        context_parts.append(f\"class:{name}\")\n    elif node.type == \"struct_declaration\":\n        context_parts.append(f\"struct:{name}\")\n    elif node.type == \"enum_declaration\":\n        context_parts.append(f\"enum:{name}\")\n    elif node.type == \"protocol_declaration\":\n        context_parts.append(f\"protocol:{name}\")\n    elif node.type == \"extension_declaration\":\n        context_parts.append(f\"extension:{name}\")\n    else:\n        context_parts.append(name)\n\n    return \".\".join(context_parts)\n</code></pre>"},{"location":"plugin-development/#plugin-configuration","title":"Plugin Configuration","text":""},{"location":"plugin-development/#configuration-options","title":"Configuration Options","text":"<p>Plugins can be configured through:</p> <ol> <li>PluginConfig object:</li> </ol> <pre><code>from chunker.languages.base import PluginConfig\n\nconfig = PluginConfig(\n    enabled=True,\n    chunk_types={\"function_declaration\", \"class_declaration\"},\n    min_chunk_size=3,\n    max_chunk_size=200,\n    custom_options={\n        \"skip_private\": True,\n        \"skip_tests\": False,\n        \"include_comments\": True\n    }\n)\n\nplugin = SwiftPlugin(config)\n</code></pre> <ol> <li>Configuration files (TOML/YAML/JSON):</li> </ol> <pre><code># chunker.config.toml\n[plugins.swift]\nenabled = true\nchunk_types = [\"function_declaration\", \"class_declaration\", \"struct_declaration\"]\nmin_chunk_size = 3\nmax_chunk_size = 300\n\n[plugins.swift.custom_options]\nskip_private = true\nskip_tests = false\ninclude_comments = true\n</code></pre> <ol> <li>Environment variables:</li> </ol> <pre><code>export CHUNKER_SWIFT_ENABLED=true\nexport CHUNKER_SWIFT_MIN_SIZE=5\n</code></pre>"},{"location":"plugin-development/#using-configuration-in-plugins","title":"Using Configuration in Plugins","text":"<pre><code>def __init__(self, config: Optional[PluginConfig] = None):\n    super().__init__(config)\n\n    # Access configuration\n    if self.config:\n        self.skip_private = self.config.custom_options.get(\"skip_private\", False)\n        self.include_comments = self.config.custom_options.get(\"include_comments\", True)\n    else:\n        # Default values\n        self.skip_private = False\n        self.include_comments = True\n</code></pre>"},{"location":"plugin-development/#advanced-features","title":"Advanced Features","text":""},{"location":"plugin-development/#1-language-detection","title":"1. Language Detection","text":"<p>Implement custom language detection based on file content:</p> <pre><code>def detect_language(self, file_path: str, content: bytes) -&gt; bool:\n    \"\"\"Detect if file is Swift based on content.\"\"\"\n    # Check extension first\n    if any(file_path.endswith(ext) for ext in self.supported_extensions):\n        return True\n\n    # Check shebang\n    if content.startswith(b\"#!/usr/bin/swift\"):\n        return True\n\n    # Check for Swift-specific keywords\n    content_str = content[:1000].decode('utf-8', errors='ignore')\n    swift_keywords = ['import Foundation', 'import UIKit', 'func ', 'var ', 'let ']\n    return any(keyword in content_str for keyword in swift_keywords)\n</code></pre>"},{"location":"plugin-development/#2-custom-node-types","title":"2. Custom Node Types","text":"<p>Define language-specific node types:</p> <pre><code>@property\ndef custom_node_mappings(self) -&gt; Dict[str, str]:\n    \"\"\"Map tree-sitter node types to semantic types.\"\"\"\n    return {\n        \"computed_property\": \"property\",\n        \"init_declaration\": \"constructor\",\n        \"deinit_declaration\": \"destructor\",\n        \"operator_declaration\": \"operator\"\n    }\n</code></pre>"},{"location":"plugin-development/#3-metadata-extraction","title":"3. Metadata Extraction","text":"<p>Extract additional metadata from chunks:</p> <pre><code>def extract_metadata(self, chunk: CodeChunk) -&gt; Dict[str, Any]:\n    \"\"\"Extract Swift-specific metadata.\"\"\"\n    metadata = {}\n\n    # Extract access level\n    access_levels = [\"public\", \"internal\", \"fileprivate\", \"private\", \"open\"]\n    for level in access_levels:\n        if chunk.content.strip().startswith(f\"{level} \"):\n            metadata[\"access_level\"] = level\n            break\n\n    # Extract attributes\n    import re\n    attributes = re.findall(r'@(\\w+)', chunk.content.split('\\n')[0])\n    if attributes:\n        metadata[\"attributes\"] = attributes\n\n    # Check if async\n    if \"async\" in chunk.content.split('\\n')[0]:\n        metadata[\"is_async\"] = True\n\n    # Check if throws\n    if \"throws\" in chunk.content.split('\\n')[0]:\n        metadata[\"throws\"] = True\n\n    return metadata\n</code></pre>"},{"location":"plugin-development/#4-relationship-detection","title":"4. Relationship Detection","text":"<p>Detect relationships between chunks:</p> <pre><code>def detect_relationships(self, chunks: List[CodeChunk]) -&gt; Dict[str, List[str]]:\n    \"\"\"Detect relationships between Swift code chunks.\"\"\"\n    relationships = {}\n\n    for chunk in chunks:\n        chunk_id = chunk.chunk_id\n        relationships[chunk_id] = []\n\n        # Find protocol conformances\n        if chunk.node_type == \"class_declaration\":\n            # Extract protocols from class declaration\n            match = re.search(r'class\\s+\\w+\\s*:\\s*([\\w\\s,]+)', chunk.content)\n            if match:\n                protocols = [p.strip() for p in match.group(1).split(',')]\n                for protocol in protocols:\n                    # Find protocol chunks\n                    protocol_chunks = [c for c in chunks \n                                     if c.node_type == \"protocol_declaration\" \n                                     and protocol in c.content]\n                    for pc in protocol_chunks:\n                        relationships[chunk_id].append(pc.chunk_id)\n\n        # Find method calls\n        method_calls = re.findall(r'(\\w+)\\(', chunk.content)\n        for method_name in method_calls:\n            # Find method definitions\n            method_chunks = [c for c in chunks \n                           if c.node_type == \"function_declaration\"\n                           and method_name in c.content]\n            for mc in method_chunks:\n                if mc.chunk_id != chunk_id:\n                    relationships[chunk_id].append(mc.chunk_id)\n\n    return relationships\n</code></pre>"},{"location":"plugin-development/#testing-your-plugin","title":"Testing Your Plugin","text":""},{"location":"plugin-development/#unit-tests","title":"Unit Tests","text":"<pre><code>import pytest\nfrom chunker import chunk_file, get_plugin_manager\nfrom your_plugin import SwiftPlugin\n\ndef test_swift_plugin_registration():\n    \"\"\"Test plugin registration.\"\"\"\n    manager = get_plugin_manager()\n    manager.register_plugin(SwiftPlugin)\n\n    assert \"swift\" in manager.list_plugins()\n    plugin = manager.get_plugin(\"swift\")\n    assert isinstance(plugin, SwiftPlugin)\n\ndef test_swift_chunking():\n    \"\"\"Test Swift file chunking.\"\"\"\n    manager = get_plugin_manager()\n    manager.register_plugin(SwiftPlugin)\n\n    # Create test file\n    swift_code = '''\n    class MyClass {\n        func myMethod() {\n            print(\"Hello\")\n        }\n\n        private func privateMethod() {\n            // This should be skipped if configured\n        }\n    }\n    '''\n\n    with open(\"test.swift\", \"w\") as f:\n        f.write(swift_code)\n\n    chunks = chunk_file(\"test.swift\", \"swift\")\n    assert len(chunks) &gt;= 2  # Class and at least one method\n\n    # Check chunk types\n    chunk_types = {chunk.node_type for chunk in chunks}\n    assert \"class_declaration\" in chunk_types\n    assert \"function_declaration\" in chunk_types\n\ndef test_plugin_configuration():\n    \"\"\"Test plugin configuration.\"\"\"\n    config = PluginConfig(\n        custom_options={\"skip_private\": True}\n    )\n    plugin = SwiftPlugin(config)\n\n    # Test that configuration is applied\n    assert plugin.config.custom_options[\"skip_private\"] is True\n</code></pre>"},{"location":"plugin-development/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_swift_plugin_with_real_files():\n    \"\"\"Test with real Swift files.\"\"\"\n    import os\n    import tempfile\n\n    manager = get_plugin_manager()\n    manager.register_plugin(SwiftPlugin)\n\n    # Create a more complex Swift file\n    swift_content = '''\n    import Foundation\n\n    @objc protocol DataSource {\n        func numberOfItems() -&gt; Int\n        func itemAtIndex(_ index: Int) -&gt; String\n    }\n\n    class ViewController: UIViewController, DataSource {\n        private var items: [String] = []\n\n        override func viewDidLoad() {\n            super.viewDidLoad()\n            setupUI()\n        }\n\n        private func setupUI() {\n            // UI setup code\n        }\n\n        // MARK: - DataSource\n\n        func numberOfItems() -&gt; Int {\n            return items.count\n        }\n\n        func itemAtIndex(_ index: Int) -&gt; String {\n            return items[index]\n        }\n    }\n    '''\n\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.swift', delete=False) as f:\n        f.write(swift_content)\n        temp_path = f.name\n\n    try:\n        chunks = chunk_file(temp_path, \"swift\")\n\n        # Verify chunks\n        assert len(chunks) &gt; 0\n\n        # Check for protocol\n        protocol_chunks = [c for c in chunks if c.node_type == \"protocol_declaration\"]\n        assert len(protocol_chunks) == 1\n\n        # Check for class\n        class_chunks = [c for c in chunks if c.node_type == \"class_declaration\"]\n        assert len(class_chunks) == 1\n\n        # Check context\n        method_chunks = [c for c in chunks if c.node_type == \"function_declaration\"]\n        class_methods = [c for c in method_chunks if \"class:ViewController\" in c.parent_context]\n        assert len(class_methods) &gt; 0\n\n    finally:\n        os.unlink(temp_path)\n</code></pre>"},{"location":"plugin-development/#distributing-plugins","title":"Distributing Plugins","text":""},{"location":"plugin-development/#1-package-structure","title":"1. Package Structure","text":"<pre><code>swift-chunker-plugin/\n\u251c\u2500\u2500 swift_chunker/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 plugin.py\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_plugin.py\n\u251c\u2500\u2500 examples/\n\u2502   \u2514\u2500\u2500 example.swift\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 pyproject.toml\n</code></pre>"},{"location":"plugin-development/#2-entry-points","title":"2. Entry Points","text":"<p>Use setuptools entry points for automatic discovery:</p> <pre><code># pyproject.toml\n[project.entry-points.\"chunker.plugins\"]\nswift = \"swift_chunker.plugin:SwiftPlugin\"\n</code></pre>"},{"location":"plugin-development/#3-installation","title":"3. Installation","text":"<p>Users can install your plugin:</p> <pre><code>pip install swift-chunker-plugin\n\n# The plugin is automatically discovered\nfrom chunker import get_plugin_manager\n\nmanager = get_plugin_manager()\nmanager.discover_plugins()  # Finds entry point plugins\n</code></pre>"},{"location":"plugin-development/#4-plugin-directory","title":"4. Plugin Directory","text":"<p>Alternatively, users can place plugins in a directory:</p> <pre><code>from pathlib import Path\nfrom chunker import get_plugin_manager\n\nmanager = get_plugin_manager()\nmanager.load_plugin_directory(Path(\"~/.chunker/plugins\"))\n</code></pre>"},{"location":"plugin-development/#built-in-plugin-examples","title":"Built-in Plugin Examples","text":"<p>Study the built-in plugins for best practices:</p>"},{"location":"plugin-development/#pythonplugin","title":"PythonPlugin","text":"<pre><code>class PythonPlugin(LanguagePlugin):\n    \"\"\"Shows how to handle decorators and async functions.\"\"\"\n\n    def process_node(self, node: Node, source: bytes, \n                    file_path: str, parent_context: Optional[str] = None):\n        # Handle decorated definitions specially\n        if node.type == \"decorated_definition\":\n            # Extract the actual function/class\n            for child in node.children:\n                if child.type in {\"function_definition\", \"class_definition\"}:\n                    chunk = self.create_chunk(node, source, file_path, parent_context)\n                    if chunk:\n                        chunk.node_type = f\"decorated_{child.type}\"\n                    return chunk\n\n        return super().process_node(node, source, file_path, parent_context)\n</code></pre>"},{"location":"plugin-development/#javascriptplugin","title":"JavaScriptPlugin","text":"<pre><code>class JavaScriptPlugin(LanguagePlugin):\n    \"\"\"Shows how to handle different function types.\"\"\"\n\n    @property\n    def default_chunk_types(self) -&gt; Set[str]:\n        return {\n            \"function_declaration\",   # function foo() {}\n            \"function_expression\",     # const foo = function() {}\n            \"arrow_function\",         # const foo = () =&gt; {}\n            \"method_definition\",      # class methods\n            \"class_declaration\",\n            \"class_expression\"\n        }\n</code></pre>"},{"location":"plugin-development/#rustplugin","title":"RustPlugin","text":"<pre><code>class RustPlugin(LanguagePlugin):\n    \"\"\"Shows how to handle impl blocks and traits.\"\"\"\n\n    def get_context_for_children(self, node: Node, chunk: CodeChunk) -&gt; str:\n        if node.type == \"impl_item\":\n            # Extract what is being implemented\n            impl_text = chunk.content.split('\\n')[0]\n            if \" for \" in impl_text:\n                # impl Trait for Type\n                parts = impl_text.split(\" for \")\n                type_name = parts[1].split()[0].strip()\n                trait_name = parts[0].split()[-1].strip()\n                return f\"impl:{trait_name}:for:{type_name}\"\n            else:\n                # impl Type\n                type_name = impl_text.split()[-1].strip()\n                return f\"impl:{type_name}\"\n\n        return super().get_context_for_children(node, chunk)\n</code></pre>"},{"location":"plugin-development/#best-practices","title":"Best Practices","text":""},{"location":"plugin-development/#1-error-handling","title":"1. Error Handling","text":"<p>Always handle parsing errors gracefully:</p> <pre><code>def get_node_name(self, node: Node, source: bytes) -&gt; Optional[str]:\n    try:\n        # Your extraction logic\n        return extracted_name\n    except Exception as e:\n        logger.warning(f\"Failed to extract name from {node.type}: {e}\")\n        return None\n</code></pre>"},{"location":"plugin-development/#2-performance","title":"2. Performance","text":"<p>Cache expensive operations:</p> <pre><code>def __init__(self, config: Optional[PluginConfig] = None):\n    super().__init__(config)\n    self._name_cache = {}\n\ndef get_node_name(self, node: Node, source: bytes) -&gt; Optional[str]:\n    cache_key = (node.start_byte, node.end_byte)\n    if cache_key in self._name_cache:\n        return self._name_cache[cache_key]\n\n    name = self._extract_name(node, source)\n    self._name_cache[cache_key] = name\n    return name\n</code></pre>"},{"location":"plugin-development/#3-compatibility","title":"3. Compatibility","text":"<p>Support multiple tree-sitter grammar versions:</p> <pre><code>def get_node_name(self, node: Node, source: bytes) -&gt; Optional[str]:\n    # Try new grammar structure first\n    name_node = node.child_by_field_name(\"name\")\n    if name_node:\n        return source[name_node.start_byte:name_node.end_byte].decode('utf-8')\n\n    # Fall back to old structure\n    for child in node.children:\n        if child.type == \"identifier\":\n            return source[child.start_byte:child.end_byte].decode('utf-8')\n\n    return None\n</code></pre>"},{"location":"plugin-development/#4-documentation","title":"4. Documentation","text":"<p>Document your plugin thoroughly:</p> <pre><code>class MyPlugin(LanguagePlugin):\n    \"\"\"\n    Plugin for MyLanguage support.\n\n    This plugin provides chunking support for MyLanguage files (.ml).\n    It extracts functions, classes, and modules as separate chunks.\n\n    Configuration Options:\n        - skip_private (bool): Skip private functions (default: False)\n        - min_chunk_size (int): Minimum lines per chunk (default: 3)\n        - include_docstrings (bool): Include docstrings in chunks (default: True)\n\n    Example:\n        &gt;&gt;&gt; from chunker import get_plugin_manager\n        &gt;&gt;&gt; manager = get_plugin_manager()\n        &gt;&gt;&gt; manager.register_plugin(MyPlugin)\n        &gt;&gt;&gt; chunks = chunk_file(\"example.ml\", \"mylang\")\n    \"\"\"\n</code></pre>"},{"location":"plugin-development/#5-testing","title":"5. Testing","text":"<p>Write comprehensive tests:</p> <ul> <li>Unit tests for each method</li> <li>Integration tests with real files</li> <li>Edge case tests (empty files, malformed code)</li> <li>Performance tests for large files</li> <li>Configuration tests</li> </ul>"},{"location":"plugin-development/#conclusion","title":"Conclusion","text":"<p>The plugin architecture makes Tree-sitter Chunker highly extensible. By following this guide, you can:</p> <ul> <li>Add support for new languages</li> <li>Customize chunking behavior</li> <li>Share plugins with the community</li> <li>Integrate with existing tools</li> </ul> <p>For more examples and the latest plugin API, see the API Reference and explore the built-in plugins in the <code>chunker/languages/</code> directory.</p>"},{"location":"requested-changes-dfrom-chunger-lib/","title":"Requested changes dfrom chunger lib","text":"<p>You are enhancing Tree-sitter Chunker to serve an agentic programming platform.</p> <p>Goals:   1) Add stable, byte-accurate spans and node IDs      - For each chunk: file_id, symbol_id, start_byte, end_byte, start_line, end_line      - node_id = sha1(path + language + ast_route + text_hash16)   2) Expose chunk hierarchy + xref graph      - Parent route (list of ancestor node_types)      - Edges: defines, calls, imports, inherits, references (src_id, dst_id, type)   3) Token awareness and packing hints      - token_count(model=\"claude-3.5\") per chunk      - pack_hint: priority score for context packing (size/importance)   4) Incremental re-index      - Watch mode; only changed files; update nodes/edges/spans   5) Postgres exporter      - Tables: nodes(id, file, lang, symbol, kind, attrs jsonb)                edges(src, dst, type, weight)                spans(file_id, symbol_id, start_byte, end_byte)      - Upsert by (id) with change_version   6) GraphCut endpoint      - Input: seeds[], radius, budget, rank weights (distance/publicness/hotspots)      - Output: node_ids[], edges[] (minimal cut)   7) Nearest-tests helper      - For a set of symbols, return candidate test files + rationale</p> <p>API contracts (JSON):   - POST /chunk/file { path, language? } -&gt; { chunks: [CodeChunk] }   - POST /export/postgres { repo_root, config } -&gt; { rows_written }   - POST /graph/xref { paths[] } -&gt; { nodes[], edges[] }   - POST /graph/cut { seeds[], params } -&gt; { nodes[], edges[] }   - POST /nearest-tests { symbols[] } -&gt; { tests: [{path, score, symbols[]}] }</p> <p>Metrics:   - timings per stage, cache hit rate, files/min, memory high-water</p> <p>Tests:   - Span round-trip tests (apply slice == original text)   - Deterministic node_id stability across runs   - Large repo incremental update correctness</p>"},{"location":"testing-methodology-complete/","title":"\ud83e\uddea Treesitter-Chunker Testing Methodology","text":""},{"location":"testing-methodology-complete/#overview","title":"Overview","text":"<p>This document outlines the comprehensive testing methodology used to validate the treesitter-chunker's production readiness across multiple programming languages and features.</p>"},{"location":"testing-methodology-complete/#testing-environment-setup","title":"Testing Environment Setup","text":""},{"location":"testing-methodology-complete/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8-3.12</li> <li>UV package manager</li> <li>Docker (for containerized testing)</li> <li>8GB+ RAM for performance tests</li> <li>All language grammars compiled</li> </ul>"},{"location":"testing-methodology-complete/#cli-access","title":"CLI Access","text":"<pre><code>python -m cli.main --help\n</code></pre>"},{"location":"testing-methodology-complete/#testing-framework","title":"Testing Framework","text":"<pre><code>python -m pytest -xvs\npython -m pytest --cov=chunker --cov-report=html\n</code></pre>"},{"location":"testing-methodology-complete/#1-language-coverage-testing","title":"1. Language Coverage Testing","text":"<p>Test each supported programming language with real-world repositories:</p> Language Repository Test File Expected Chunks Python pallets/click src/click/core.py 176+ chunks JavaScript lodash/lodash lodash.js 1,865+ chunks Go gin-gonic/gin gin.go 67+ chunks Rust serde-rs/serde src/de/size_hint.rs 3+ chunks C++ google/googletest src/gtest_main.cc 2+ chunks Java google/guava TraverserRewrite.java 19+ chunks Ruby ruby/ruby array.rb 12+ chunks C git/git apply.c 45+ chunks TypeScript microsoft/TypeScript src/compiler/parser.ts 230+ chunks TSX facebook/react packages/react/src/React.tsx 15+ chunks PHP laravel/framework src/Illuminate/Foundation/Application.php 89+ chunks Kotlin JetBrains/kotlin compiler/frontend/src/org/jetbrains/kotlin/resolve/BindingContext.kt 34+ chunks C# dotnet/roslyn src/Compilers/CSharp/Portable/Parser/LanguageParser.cs 156+ chunks Swift apple/swift stdlib/public/core/Array.swift 78+ chunks"},{"location":"testing-methodology-complete/#2-feature-testing-matrix","title":"2. Feature Testing Matrix","text":""},{"location":"testing-methodology-complete/#a-single-file-chunking","title":"A. Single File Chunking","text":"<pre><code>python -m cli.main chunk &lt;file&gt; --lang &lt;language&gt;\n</code></pre>"},{"location":"testing-methodology-complete/#b-ast-visualization","title":"B. AST Visualization","text":"<pre><code>python -m cli.main debug ast &lt;file&gt; --lang &lt;language&gt; --format tree\npython -m cli.main debug ast &lt;file&gt; --lang &lt;language&gt; --format dot --output ast.svg\n</code></pre>"},{"location":"testing-methodology-complete/#c-chunk-analysis","title":"C. Chunk Analysis","text":"<pre><code># Test chunking decisions and coverage\npython -m cli.main debug chunks &lt;file&gt; --lang &lt;language&gt;\n\n# Expected: Detailed analysis with chunked vs non-chunked nodes\n</code></pre>"},{"location":"testing-methodology-complete/#d-query-debugging","title":"D. Query Debugging","text":"<pre><code>python -m cli.main debug query &lt;file&gt; --lang &lt;language&gt; --query \"(function_definition) @func\"\n</code></pre>"},{"location":"testing-methodology-complete/#e-batch-processing","title":"E. Batch Processing","text":"<pre><code># Test multiple files\npython -m cli.main chunk *.py --lang python --output results.jsonl --format jsonl\n</code></pre>"},{"location":"testing-methodology-complete/#f-repository-processing","title":"F. Repository Processing","text":"<pre><code># Test full repository analysis\npython -m cli.main repo process &lt;repo&gt; --file-pattern \"src/**/*.py\" --output results.jsonl\n\n# Expected: Comprehensive repository analysis\n</code></pre>"},{"location":"testing-methodology-complete/#3-security-testing","title":"3. Security Testing","text":""},{"location":"testing-methodology-complete/#a-input-validation","title":"A. Input Validation","text":"<pre><code># Test malicious file paths\npython -m cli.main chunk \"../../../../../etc/passwd\" --lang python  # Should fail safely\npython -m cli.main chunk \"file://malicious.py\" --lang python  # Should fail safely\npython -m cli.main chunk \"; rm -rf /\" --lang python  # Should fail safely\n</code></pre>"},{"location":"testing-methodology-complete/#b-resource-limits","title":"B. Resource Limits","text":"<pre><code># Test memory limits\npython -c \"\nfrom chunker import chunk_file\nimport resource\n# Set 1GB memory limit\nresource.setrlimit(resource.RLIMIT_AS, (1024*1024*1024, 1024*1024*1024))\ntry:\n    chunks = chunk_file('massive_file.py', 'python')\nexcept MemoryError:\n    print('Memory limit enforced correctly')\n\"\n</code></pre>"},{"location":"testing-methodology-complete/#c-configuration-injection","title":"C. Configuration Injection","text":"<pre><code># Test config file security\necho 'malicious_config = \"__import__(\\\"os\\\").system(\\\"echo pwned\\\")\"' &gt; bad_config.py\npython -m cli.main chunk test.py --config bad_config.py  # Should not execute\n</code></pre>"},{"location":"testing-methodology-complete/#d-dependency-scanning","title":"D. Dependency Scanning","text":"<pre><code># Check for known vulnerabilities\npip-audit\nsafety check\n</code></pre>"},{"location":"testing-methodology-complete/#4-performance-scalability-testing","title":"4. Performance &amp; Scalability Testing","text":""},{"location":"testing-methodology-complete/#a-large-file-handling","title":"A. Large File Handling","text":"<pre><code># Test with progressively larger files\nfor size in [1, 10, 100, 1000]:  # MB\n    create_test_file(f\"test_{size}mb.py\", size)\n    start = time.time()\n    chunks = chunk_file(f\"test_{size}mb.py\", \"python\")\n    print(f\"{size}MB: {time.time()-start:.2f}s, {len(chunks)} chunks\")\n</code></pre>"},{"location":"testing-methodology-complete/#b-concurrent-processing","title":"B. Concurrent Processing","text":"<pre><code># Test parallel processing limits\npython -m cli.main chunk **/*.py --lang python --workers 1 --output single.jsonl\npython -m cli.main chunk **/*.py --lang python --workers 4 --output parallel4.jsonl\npython -m cli.main chunk **/*.py --lang python --workers 16 --output parallel16.jsonl\n\n# Compare performance and correctness\n</code></pre>"},{"location":"testing-methodology-complete/#c-memory-usage-profiling","title":"C. Memory Usage Profiling","text":"<pre><code># Profile memory usage\nmprof run python -m cli.main repo process large_repo/\nmprof plot\n</code></pre>"},{"location":"testing-methodology-complete/#d-cache-efficiency","title":"D. Cache Efficiency","text":"<pre><code># Test cache hit rates\nfrom chunker.cache import get_cache_stats\nchunk_file(\"test.py\", \"python\")  # First run\nstats1 = get_cache_stats()\nchunk_file(\"test.py\", \"python\")  # Cached run\nstats2 = get_cache_stats()\nassert stats2.hits &gt; stats1.hits\n</code></pre>"},{"location":"testing-methodology-complete/#5-reliability-stability-testing","title":"5. Reliability &amp; Stability Testing","text":""},{"location":"testing-methodology-complete/#a-long-running-tests","title":"A. Long-Running Tests","text":"<pre><code># 24-hour stability test\npython scripts/stability_test.py --duration 86400 --interval 60\n</code></pre>"},{"location":"testing-methodology-complete/#b-error-recovery","title":"B. Error Recovery","text":"<pre><code># Test graceful degradation\ndef test_error_recovery():\n    # Corrupt AST scenario\n    with mock.patch('tree_sitter.Parser.parse', side_effect=Exception):\n        chunks = chunk_file(\"test.py\", \"python\", fallback=True)\n        assert len(chunks) &gt; 0  # Should use fallback chunker\n</code></pre>"},{"location":"testing-methodology-complete/#c-thread-safety","title":"C. Thread Safety","text":"<pre><code># Test concurrent access\nimport threading\ndef worker(file_path, results):\n    chunks = chunk_file(file_path, \"python\")\n    results.append(len(chunks))\n\nresults = []\nthreads = [threading.Thread(target=worker, args=(\"test.py\", results)) \n           for _ in range(100)]\nfor t in threads: t.start()\nfor t in threads: t.join()\nassert all(r == results[0] for r in results)  # All results should be identical\n</code></pre>"},{"location":"testing-methodology-complete/#d-memory-leak-detection","title":"D. Memory Leak Detection","text":"<pre><code># Run with memory leak detection\nvalgrind --leak-check=full python -m cli.main chunk large_file.py --lang python\n</code></pre>"},{"location":"testing-methodology-complete/#6-data-integrity-testing","title":"6. Data Integrity Testing","text":""},{"location":"testing-methodology-complete/#a-chunk-boundary-validation","title":"A. Chunk Boundary Validation","text":"<pre><code>def test_chunk_boundaries():\n    chunks = chunk_file(\"test.py\", \"python\")\n    source = open(\"test.py\").read()\n\n    # Verify no overlaps\n    for i in range(len(chunks)-1):\n        assert chunks[i].end_line &lt; chunks[i+1].start_line\n\n    # Verify complete coverage\n    reconstructed = \"\".join(chunk.content for chunk in chunks)\n    assert reconstructed == source\n</code></pre>"},{"location":"testing-methodology-complete/#b-unicode-handling","title":"B. Unicode Handling","text":"<pre><code># Test various encodings\ntest_files = [\n    (\"utf8_emoji.py\", \"utf-8\", \"\ud83d\ude80 def rocket(): pass\"),\n    (\"utf16.py\", \"utf-16\", \"def test(): return '\u6d4b\u8bd5'\"),\n    (\"latin1.py\", \"latin-1\", \"def caf\u00e9(): pass\"),\n]\n\nfor filename, encoding, content in test_files:\n    with open(filename, \"w\", encoding=encoding) as f:\n        f.write(content)\n    chunks = chunk_file(filename, \"python\")\n    assert len(chunks) &gt; 0\n</code></pre>"},{"location":"testing-methodology-complete/#c-cross-language-consistency","title":"C. Cross-Language Consistency","text":"<pre><code># Test mixed-language files\necho '&lt;?php echo \"&lt;script&gt;console.log(\\\"test\\\")&lt;/script&gt;\"; ?&gt;' &gt; mixed.php\npython -m cli.main chunk mixed.php --lang php\n# Should handle embedded JavaScript correctly\n</code></pre>"},{"location":"testing-methodology-complete/#7-integration-testing","title":"7. Integration Testing","text":""},{"location":"testing-methodology-complete/#a-cicd-pipeline-integration","title":"A. CI/CD Pipeline Integration","text":"<pre><code># .github/workflows/test.yml\nname: Test\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        python: ['3.8', '3.9', '3.10', '3.11', '3.12']\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python }}\n      - run: |\n          pip install uv\n          uv pip install -e \".[dev]\"\n          python -m pytest\n</code></pre>"},{"location":"testing-methodology-complete/#b-docker-testing","title":"B. Docker Testing","text":"<pre><code># Test in containerized environment\nFROM python:3.11-slim\nCOPY . /app\nWORKDIR /app\nRUN pip install uv &amp;&amp; uv pip install -e .\nRUN python -m pytest\n</code></pre>"},{"location":"testing-methodology-complete/#c-ide-plugin-testing","title":"C. IDE Plugin Testing","text":"<pre><code># Test VS Code integration\ndef test_vscode_integration():\n    # Test MCP protocol compatibility\n    from chunker.contracts.debug_contract import DebugContract\n    debug = DebugContract()\n    diagnostics = debug.get_diagnostics(\"test.py\")\n    assert isinstance(diagnostics, list)\n</code></pre>"},{"location":"testing-methodology-complete/#8-operational-testing","title":"8. Operational Testing","text":""},{"location":"testing-methodology-complete/#a-installation-testing","title":"A. Installation Testing","text":"<pre><code># Test various installation methods\npip install treesitter-chunker\nconda install -c conda-forge treesitter-chunker\nbrew install treesitter-chunker\ndocker pull treesitter/chunker:latest\n</code></pre>"},{"location":"testing-methodology-complete/#b-upgrade-testing","title":"B. Upgrade Testing","text":"<pre><code># Test upgrade paths\npip install treesitter-chunker==0.1.0\npython -m cli.main chunk test.py --lang python &gt; v1_output.json\npip install --upgrade treesitter-chunker\npython -m cli.main chunk test.py --lang python &gt; v2_output.json\n# Verify compatibility\n</code></pre>"},{"location":"testing-methodology-complete/#c-configuration-migration","title":"C. Configuration Migration","text":"<pre><code># Test config format migrations\ndef test_config_migration():\n    old_config = {\"chunk_size\": 100}\n    new_config = migrate_config(old_config)\n    assert \"max_chunk_size\" in new_config\n</code></pre>"},{"location":"testing-methodology-complete/#d-monitoring-telemetry","title":"D. Monitoring &amp; Telemetry","text":"<pre><code># Test OpenTelemetry integration\nfrom opentelemetry import trace\ntracer = trace.get_tracer(__name__)\n\nwith tracer.start_as_current_span(\"chunk_file\"):\n    chunks = chunk_file(\"test.py\", \"python\")\n\n# Verify span was recorded\n</code></pre>"},{"location":"testing-methodology-complete/#success-criteria","title":"Success Criteria","text":""},{"location":"testing-methodology-complete/#functional-requirements","title":"Functional Requirements","text":"<ul> <li>\u2705 All 14 languages produce valid chunks</li> <li>\u2705 AST parsing succeeds for all languages</li> <li>\u2705 Chunks are meaningful code units</li> <li>\u2705 Coverage &gt;90% for typical files</li> <li>\u2705 Process 1000+ files without errors</li> </ul>"},{"location":"testing-methodology-complete/#performance-requirements","title":"Performance Requirements","text":"<ul> <li>\u2705 Process 100 files/second (average size)</li> <li>\u2705 Handle files up to 10MB</li> <li>\u2705 Memory usage &lt;2GB for typical workload</li> <li>\u2705 Cache hit rate &gt;80% for repeated files</li> </ul>"},{"location":"testing-methodology-complete/#quality-requirements","title":"Quality Requirements","text":"<ul> <li>\u2705 Chunk size: 3-200 lines (configurable)</li> <li>\u2705 No data loss or corruption</li> <li>\u2705 Graceful error handling</li> <li>\u2705 Clear error messages</li> </ul>"},{"location":"testing-methodology-complete/#security-requirements","title":"Security Requirements","text":"<ul> <li>\u2705 No arbitrary code execution</li> <li>\u2705 Path traversal protection</li> <li>\u2705 Resource limit enforcement</li> <li>\u2705 Safe configuration parsing</li> </ul>"},{"location":"testing-methodology-complete/#debugging-methodology","title":"Debugging Methodology","text":""},{"location":"testing-methodology-complete/#when-tests-fail","title":"When Tests Fail","text":"<ol> <li>Check language grammar installation</li> <li>Verify file encoding</li> <li>Review chunk configuration</li> <li>Enable debug logging</li> <li>Use visualization tools</li> </ol>"},{"location":"testing-methodology-complete/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":"Issue Solution No chunks produced Verify language grammar, check AST parsing Language not recognized Run <code>scripts/fetch_grammars.py</code> and rebuild Memory errors Increase limits or use streaming mode Performance degradation Check cache configuration, use parallel mode"},{"location":"testing-methodology-complete/#test-automation","title":"Test Automation","text":""},{"location":"testing-methodology-complete/#continuous-testing","title":"Continuous Testing","text":"<pre><code># Run all tests continuously\nwhile true; do\n    python -m pytest\n    python benchmarks/run_benchmarks.py\n    sleep 300  # 5 minutes\ndone\n</code></pre>"},{"location":"testing-methodology-complete/#regression-testing","title":"Regression Testing","text":"<pre><code># Compare against baseline\npython benchmarks/regression_tracker.py --baseline v1.0.0\n</code></pre>"},{"location":"testing-methodology-complete/#reporting","title":"Reporting","text":""},{"location":"testing-methodology-complete/#test-coverage-report","title":"Test Coverage Report","text":"<pre><code>python -m pytest --cov=chunker --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"testing-methodology-complete/#performance-report","title":"Performance Report","text":"<pre><code>python benchmarks/comprehensive_suite.py --output perf_report.html\n</code></pre>"},{"location":"testing-methodology-complete/#security-report","title":"Security Report","text":"<pre><code>bandit -r chunker/\nsafety check --json &gt; security_report.json\n</code></pre>"},{"location":"testing-methodology-complete/#conclusion","title":"Conclusion","text":"<p>This comprehensive testing methodology ensures the treesitter-chunker is production-ready by validating:</p> <ol> <li>Functionality across all 14 supported languages</li> <li>Performance at scale with real-world codebases</li> <li>Security against common vulnerabilities</li> <li>Reliability under various conditions</li> <li>Compatibility across platforms and integrations</li> </ol> <p>Regular execution of these tests provides confidence in the system's stability and readiness for production deployment.</p>"},{"location":"token_limits/","title":"Token Limit Handling","text":"<p>The treesitter-chunker now includes built-in support for respecting token limits when chunking code. This is essential when preparing code for LLMs that have specific context window limitations.</p>"},{"location":"token_limits/#quick-start","title":"Quick Start","text":"<pre><code>from chunker import chunk_file_with_token_limit\n\n# Chunk a file ensuring no chunk exceeds 1000 tokens\nchunks = chunk_file_with_token_limit(\n    \"example.py\", \n    language=\"python\", \n    max_tokens=1000,\n    model=\"gpt-4\"\n)\n\nfor chunk in chunks:\n    print(f\"{chunk.node_type}: {chunk.metadata['token_count']} tokens\")\n</code></pre>"},{"location":"token_limits/#features","title":"Features","text":""},{"location":"token_limits/#1-token-aware-chunking","title":"1. Token-Aware Chunking","text":"<p>The chunker automatically adds token count information to each chunk's metadata:</p> <pre><code>from chunker import chunk_file\n\nchunks = chunk_file(\"example.py\", \"python\")\nfor chunk in chunks:\n    # Token info is automatically added\n    print(f\"Tokens: {chunk.metadata.get('token_count', 'N/A')}\")\n</code></pre>"},{"location":"token_limits/#2-automatic-chunk-splitting","title":"2. Automatic Chunk Splitting","text":"<p>When a chunk exceeds the specified token limit, it's automatically split while preserving code structure:</p> <pre><code># Large functions/classes are split intelligently\nchunks = chunk_file_with_token_limit(\"large_file.py\", \"python\", max_tokens=500)\n</code></pre>"},{"location":"token_limits/#3-multiple-tokenizer-models","title":"3. Multiple Tokenizer Models","text":"<p>Support for different LLM tokenizers:</p> <pre><code># GPT-4 (default)\nchunks = chunk_file_with_token_limit(\"file.py\", \"python\", max_tokens=1000, model=\"gpt-4\")\n\n# Claude\nchunks = chunk_file_with_token_limit(\"file.py\", \"python\", max_tokens=1000, model=\"claude\")\n\n# GPT-3.5\nchunks = chunk_file_with_token_limit(\"file.py\", \"python\", max_tokens=1000, model=\"gpt-3.5-turbo\")\n</code></pre>"},{"location":"token_limits/#api-reference","title":"API Reference","text":""},{"location":"token_limits/#functions","title":"Functions","text":""},{"location":"token_limits/#chunk_text_with_token_limit","title":"<code>chunk_text_with_token_limit()</code>","text":"<pre><code>def chunk_text_with_token_limit(\n    text: str, \n    language: str, \n    max_tokens: int, \n    file_path: str = \"\", \n    model: str = \"gpt-4\",\n    extract_metadata: bool = True\n) -&gt; list[CodeChunk]\n</code></pre> <p>Chunks text ensuring no chunk exceeds the token limit.</p>"},{"location":"token_limits/#chunk_file_with_token_limit","title":"<code>chunk_file_with_token_limit()</code>","text":"<pre><code>def chunk_file_with_token_limit(\n    path: str | Path, \n    language: str, \n    max_tokens: int,\n    model: str = \"gpt-4\", \n    extract_metadata: bool = True\n) -&gt; list[CodeChunk]\n</code></pre> <p>Chunks a file ensuring no chunk exceeds the token limit.</p>"},{"location":"token_limits/#count_chunk_tokens","title":"<code>count_chunk_tokens()</code>","text":"<pre><code>def count_chunk_tokens(chunk: CodeChunk, model: str = \"gpt-4\") -&gt; int\n</code></pre> <p>Counts tokens in an existing chunk.</p>"},{"location":"token_limits/#classes","title":"Classes","text":""},{"location":"token_limits/#treesittertokenawarechunker","title":"<code>TreeSitterTokenAwareChunker</code>","text":"<p>For advanced use cases, you can use the token-aware chunker directly:</p> <pre><code>from chunker import TreeSitterTokenAwareChunker\n\nchunker = TreeSitterTokenAwareChunker()\n\n# Add token info to existing chunks\nchunks_with_tokens = chunker.add_token_info(chunks, model=\"gpt-4\")\n\n# Chunk with token limits\nlimited_chunks = chunker.chunk_with_token_limit(\n    \"file.py\", \"python\", max_tokens=1000\n)\n</code></pre>"},{"location":"token_limits/#chunk-metadata","title":"Chunk Metadata","text":"<p>When using token-aware chunking, chunks include additional metadata:</p> <pre><code>{\n    \"token_count\": 156,           # Number of tokens in the chunk\n    \"tokenizer_model\": \"gpt-4\",   # Model used for tokenization\n    \"chars_per_token\": 4.2,       # Average characters per token\n    \"is_split\": True,             # Whether chunk was split from larger chunk\n    \"split_index\": 1,             # Index if split (1, 2, 3...)\n    \"original_chunk_id\": \"abc123\" # ID of original chunk before splitting\n}\n</code></pre>"},{"location":"token_limits/#splitting-strategies","title":"Splitting Strategies","text":""},{"location":"token_limits/#class-splitting","title":"Class Splitting","text":"<p>Classes are intelligently split by methods when they exceed token limits:</p> <pre><code>class LargeClass:\n    def __init__(self):\n        # ...\n\n    def method1(self):\n        # ...\n\n    def method2(self):\n        # ...\n</code></pre> <p>If this class exceeds the token limit, it will be split into: - Chunk 1: Class header + <code>__init__</code> + <code>method1</code> - Chunk 2: Class header + <code>method2</code></p>"},{"location":"token_limits/#function-splitting","title":"Function Splitting","text":"<p>Large functions are split by logical line groups while preserving context:</p> <pre><code>def large_function():\n    # Setup code\n    # ...\n\n    # Main logic\n    # ...\n\n    # Cleanup\n    # ...\n</code></pre>"},{"location":"token_limits/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Choose Appropriate Limits: Consider the LLM's context window and leave room for prompts:    <code>python    # For GPT-4 (8k context), leave room for prompts    chunks = chunk_file_with_token_limit(\"file.py\", \"python\", max_tokens=6000)</code></p> </li> <li> <p>Model-Specific Tokenization: Use the same model for tokenization as you'll use for processing:    <code>python    # If using Claude for processing    chunks = chunk_file_with_token_limit(\"file.py\", \"python\",                                         max_tokens=5000, model=\"claude\")</code></p> </li> <li> <p>Preserve Metadata: Token-aware chunking preserves all metadata extraction:    <code>python    chunks = chunk_file_with_token_limit(\"file.py\", \"python\",                                         max_tokens=1000, extract_metadata=True)    # Chunks still include signatures, docstrings, complexity metrics, etc.</code></p> </li> </ol>"},{"location":"token_limits/#integration-with-fallback-strategies","title":"Integration with Fallback Strategies","text":"<p>Token limits work seamlessly with the fallback chunking system. When tree-sitter chunks are too large, the sliding window fallback can be used:</p> <pre><code>from chunker.fallback import SlidingWindowFallback\n\nfallback = SlidingWindowFallback()\n# Automatically uses sliding window for files that can't be parsed\n# or produce chunks exceeding token limits\n</code></pre>"},{"location":"token_limits/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Token counting is cached per encoding type</li> <li>Splitting only occurs when necessary</li> <li>Original chunk structure is preserved when possible</li> <li>Metadata extraction happens before splitting for efficiency</li> </ul>"},{"location":"troubleshooting/","title":"Tree-sitter Chunker Troubleshooting Guide","text":""},{"location":"troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#abi-version-mismatch","title":"ABI Version Mismatch","text":"<p>Error: <code>RuntimeError: Cannot create language version 15, expected 13-14</code></p> <p>Solution: Install py-tree-sitter from GitHub to get ABI 15 support:</p> <pre><code>uv pip install git+https://github.com/tree-sitter/py-tree-sitter.git\n</code></pre>"},{"location":"troubleshooting/#grammar-compilation-failed","title":"Grammar Compilation Failed","text":"<p>Error: <code>Failed to compile grammars</code></p> <p>Solution: 1. Ensure you have a C compiler installed (gcc/clang) 2. Run the build scripts in order:    <code>bash    python scripts/fetch_grammars.py    python scripts/build_lib.py</code></p>"},{"location":"troubleshooting/#import-errors","title":"Import Errors","text":""},{"location":"troubleshooting/#module-import-errors","title":"Module Import Errors","text":"<p>Error: <code>ImportError: cannot import name 'chunk_file' from 'chunker'</code></p> <p>Solution: Use the correct module path:</p> <pre><code># Old (incorrect)\nfrom chunker import chunk_file\n\n# New (correct)\nfrom chunker.core import chunk_file\n</code></pre> <p>Common import corrections: - <code>from chunker.core import chunk_file</code> - <code>from chunker.parallel import chunk_files_parallel</code> - <code>from chunker.streaming import chunk_file_streaming</code> - <code>from chunker.plugin_manager import get_plugin_manager</code> - <code>from chunker.cache import ASTCache</code> - <code>from chunker.export.json_export import JSONExporter, JSONLExporter</code> - <code>from chunker.export.formatters import SchemaType</code></p>"},{"location":"troubleshooting/#circular-import-errors","title":"Circular Import Errors","text":"<p>Error: <code>ImportError: cannot import name '_walk' from partially initialized module</code></p> <p>Solution: This has been fixed in the latest version. Ensure you're using the latest code where circular dependencies have been resolved by moving shared functions to <code>chunker.core</code>.</p>"},{"location":"troubleshooting/#runtime-issues","title":"Runtime Issues","text":""},{"location":"troubleshooting/#no-chunks-returned","title":"No Chunks Returned","text":"<p>Problem: <code>chunk_file()</code> returns empty list</p> <p>Possible causes: 1. File too small: Default <code>min_chunk_size</code> is 3 lines. Adjust if needed:    <code>python    from chunker.chunker_config import ChunkerConfig    config = ChunkerConfig(min_chunk_size=1)</code></p> <ol> <li> <p>Language not supported: Check available languages:    <code>python    from chunker.parser import list_languages    print(list_languages())</code></p> </li> <li> <p>File excluded by pattern: When using batch processing, files with \"test\" in the name are excluded by default:    <code>bash    python cli/main.py batch src/ --exclude \"*.tmp\" --include \"*.py\"</code></p> </li> </ol>"},{"location":"troubleshooting/#parser-not-available","title":"Parser Not Available","text":"<p>Error: <code>LanguageNotFoundError: Language 'xyz' not found</code></p> <p>Solution: 1. Check if language is supported:    <code>python    from chunker.parser import list_languages    print(list_languages())</code></p> <ol> <li>For universal language support, use ZeroConfigAPI:    <code>python    from chunker.auto import ZeroConfigAPI    api = ZeroConfigAPI()    result = api.auto_chunk_file(\"file.xyz\")  # Auto-downloads grammar if available</code></li> </ol>"},{"location":"troubleshooting/#cli-issues","title":"CLI Issues","text":""},{"location":"troubleshooting/#json-parse-errors-in-tests","title":"JSON Parse Errors in Tests","text":"<p>Error: <code>json.decoder.JSONDecodeError: Invalid control character</code></p> <p>Solution: This can occur when test output contains ANSI escape codes. The latest version includes fallback parsing to handle this.</p>"},{"location":"troubleshooting/#batch-command-not-finding-files","title":"Batch Command Not Finding Files","text":"<p>Problem: No files processed when running batch command</p> <p>Common issues: 1. Default exclude pattern filters out test files 2. Wrong file extension pattern 3. Incorrect path</p> <p>Solution:</p> <pre><code># Override default excludes\npython cli/main.py batch src/ --exclude \"\" --include \"*.py\"\n\n# Be explicit about patterns\npython cli/main.py batch src/ --pattern \"**/*.py\"\n</code></pre>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-processing","title":"Slow Processing","text":"<p>Problem: Chunking takes too long</p> <p>Solutions: 1. Enable caching:    <code>python    from chunker.cache import ASTCache    cache = ASTCache()</code></p> <ol> <li> <p>Use parallel processing:    <code>python    from chunker.parallel import chunk_files_parallel    results = chunk_files_parallel(files, \"python\", max_workers=4)</code></p> </li> <li> <p>Use streaming for large files:    <code>python    from chunker.streaming import chunk_file_streaming    chunks = list(chunk_file_streaming(\"large_file.py\", \"python\"))</code></p> </li> </ol>"},{"location":"troubleshooting/#export-issues","title":"Export Issues","text":""},{"location":"troubleshooting/#memory-issues-with-large-exports","title":"Memory Issues with Large Exports","text":"<p>Problem: Out of memory when exporting large datasets</p> <p>Solution: Use streaming export:</p> <pre><code>from chunker.export.json_export import JSONLExporter\nfrom chunker.streaming import chunk_file_streaming\n\nexporter = JSONLExporter()\nexporter.stream_export(\n    chunk_file_streaming(\"large_file.py\", \"python\"),\n    \"output.jsonl\"\n)\n</code></pre>"},{"location":"troubleshooting/#testing-issues","title":"Testing Issues","text":""},{"location":"troubleshooting/#skipped-tests","title":"Skipped Tests","text":"<p>Notice: Some tests are skipped with \"ABI version mismatch\"</p> <p>Explanation: This is expected when grammars were compiled with different ABI versions. The skip markers prevent false failures. To run these tests, recompile grammars with matching ABI version.</p>"},{"location":"troubleshooting/#coverage-module-issues","title":"Coverage Module Issues","text":"<p>Error: Circular import errors from coverage module</p> <p>Solution: Run tests without coverage:</p> <pre><code>python -m pytest -p no:cov\n</code></pre>"},{"location":"troubleshooting/#language-specific-issues","title":"Language-Specific Issues","text":""},{"location":"troubleshooting/#language-plugin-not-found","title":"Language Plugin Not Found","text":"<p>Error: <code>Plugin for language 'xyz' not found</code></p> <p>Solution: 1. Load built-in plugins:    <code>python    from chunker.plugin_manager import get_plugin_manager    manager = get_plugin_manager()    manager.load_built_in_plugins()</code></p> <ol> <li>Check available plugins:    <code>python    print(manager.list_plugins())</code></li> </ol>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check the GitHub Issues</li> <li>Review the API Reference</li> <li>See the User Guide for detailed examples</li> <li>File a new issue with:</li> <li>Python version</li> <li>Tree-sitter chunker version</li> <li>Minimal code to reproduce</li> <li>Full error traceback</li> </ol>"},{"location":"user-guide/","title":"Tree-sitter Chunker User Guide","text":""},{"location":"user-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Installation</li> <li>Core Concepts</li> <li>Basic Usage</li> <li>Supported Languages</li> <li>Working with Chunks</li> <li>Advanced Features</li> <li>Integration Patterns</li> <li>Performance Best Practices</li> <li>Configuration</li> <li>Troubleshooting</li> </ol>"},{"location":"user-guide/#introduction","title":"Introduction","text":"<p>Tree-sitter Chunker is a powerful library for semantically chunking source code using Tree-sitter parsers. It provides intelligent code splitting that understands syntax and structure, making it ideal for code analysis, documentation generation, and AI/LLM applications.</p>"},{"location":"user-guide/#key-features","title":"Key Features","text":"<ul> <li>Dynamic Language Discovery: Automatically discovers available languages from compiled grammars</li> <li>Efficient Parser Management: LRU caching and pooling for optimal performance</li> <li>Thread-Safe Operation: Designed for concurrent processing</li> <li>Rich Error Handling: Comprehensive exception hierarchy with recovery suggestions</li> <li>Semantic Understanding: Extracts meaningful code units (functions, classes, methods)</li> <li>Context Preservation: Maintains parent-child relationships for nested structures</li> </ul>"},{"location":"user-guide/#when-to-use-tree-sitter-chunker","title":"When to Use Tree-sitter Chunker","text":"<p>Tree-sitter Chunker is ideal for:</p> <ul> <li>Code Embedding Generation: Create embeddings for semantic code search</li> <li>LLM Context Windows: Split code intelligently for language model processing</li> <li>Documentation Generation: Extract functions and classes with metadata</li> <li>Code Analysis: Analyze code structure, complexity, and patterns</li> <li>Code Navigation: Build code maps and understand project structure</li> <li>Refactoring Tools: Identify and process code units programmatically</li> </ul>"},{"location":"user-guide/#installation","title":"Installation","text":""},{"location":"user-guide/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>uv package manager (recommended) or pip</li> <li>C compiler (for building tree-sitter grammars)</li> <li>Git (for fetching grammar repositories)</li> </ul>"},{"location":"user-guide/#install-with-uv-recommended","title":"Install with uv (Recommended)","text":"<pre><code># Create virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install package\nuv pip install -e \".[dev]\"\n\n# Install py-tree-sitter with ABI 15 support\nuv pip install git+https://github.com/tree-sitter/py-tree-sitter.git\n</code></pre>"},{"location":"user-guide/#build-language-grammars","title":"Build Language Grammars","text":"<pre><code># Fetch grammar repositories\npython scripts/fetch_grammars.py\n\n# Compile grammars into shared library\npython scripts/build_lib.py\n</code></pre>"},{"location":"user-guide/#verify-installation","title":"Verify Installation","text":"<pre><code>from chunker.parser import list_languages, get_language_info\n\n# Check available languages\nlanguages = list_languages()\nprint(f\"Available languages: {languages}\")\n\n# Get language details\ninfo = get_language_info(\"python\")\nprint(f\"Python ABI version: {info.version}\")\nprint(f\"Node types: {info.node_types_count}\")\n</code></pre>"},{"location":"user-guide/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/#languages-and-grammars","title":"Languages and Grammars","text":"<p>Tree-sitter uses grammar files to understand language syntax. Each language has: - A grammar definition that describes syntax rules - A parser that builds Abstract Syntax Trees (AST) - Node types that represent different code constructs</p>"},{"location":"user-guide/#code-chunks","title":"Code Chunks","text":"<p>A chunk is a semantic unit of code with: - Content: The actual source code - Metadata: Location, type, and context information - Relationships: Parent-child relationships for nested structures</p>"},{"location":"user-guide/#parser-management","title":"Parser Management","text":"<p>The library uses several optimization strategies: - Caching: Recently used parsers are kept in memory - Pooling: Multiple parsers per language for concurrent use - Lazy Loading: Languages are loaded only when needed</p>"},{"location":"user-guide/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/#command-line-interface","title":"Command Line Interface","text":"<pre><code># Basic chunking\npython cli/main.py chunk example.py -l python\n\n# Output as JSON\npython cli/main.py chunk src/main.rs -l rust --json\n\n# Process JavaScript file\npython cli/main.py chunk app.js -l javascript\n</code></pre>"},{"location":"user-guide/#python-api-simple","title":"Python API - Simple","text":"<pre><code>from chunker.chunker import chunk_file\n\n# Chunk a Python file\nchunks = chunk_file(\"example.py\", \"python\")\n\n# Process chunks\nfor chunk in chunks:\n    print(f\"{chunk.node_type} at lines {chunk.start_line}-{chunk.end_line}\")\n    if chunk.parent_context:\n        print(f\"  Parent: {chunk.parent_context}\")\n    print(f\"  Preview: {chunk.content.split(chr(10))[0]}...\")\n</code></pre>"},{"location":"user-guide/#python-api-advanced","title":"Python API - Advanced","text":"<pre><code>from chunker.parser import get_parser, return_parser\nfrom chunker.exceptions import LanguageNotFoundError\n\n# Manual parser management for better control\ntry:\n    parser = get_parser(\"python\")\n\n    with open(\"example.py\", \"rb\") as f:\n        tree = parser.parse(f.read())\n\n    # Process the AST\n    root = tree.root_node\n    print(f\"Root type: {root.type}\")\n    print(f\"Children: {root.child_count}\")\n\nfinally:\n    # Return parser for reuse\n    return_parser(\"python\", parser)\n</code></pre>"},{"location":"user-guide/#supported-languages","title":"Supported Languages","text":""},{"location":"user-guide/#python","title":"Python","text":"<pre><code># Extracted node types:\n# - function_definition (functions)\n# - class_definition (classes)\n# - method_definition (methods within classes)\n\nclass Calculator:  # class_definition\n    def add(self, a, b):  # method_definition\n        return a + b\n\ndef main():  # function_definition\n    calc = Calculator()\n</code></pre>"},{"location":"user-guide/#javascript","title":"JavaScript","text":"<pre><code>// Extracted node types:\n// - function_declaration\n// - class_declaration\n// - method_definition\n// - arrow_function\n\nclass Component {  // class_declaration\n    render() {  // method_definition\n        return null;\n    }\n}\n\nconst handler = () =&gt; {  // arrow_function\n    console.log(\"clicked\");\n};\n</code></pre>"},{"location":"user-guide/#rust","title":"Rust","text":"<pre><code>// Extracted node types:\n// - function_item\n// - impl_item\n// - struct_item\n// - trait_item\n\nstruct Data {  // struct_item\n    value: i32,\n}\n\nimpl Data {  // impl_item\n    fn new() -&gt; Self {  // function_item\n        Data { value: 0 }\n    }\n}\n</code></pre>"},{"location":"user-guide/#cc","title":"C/C++","text":"<pre><code>// C extracts: function_definition\n// C++ adds: class_specifier, method_declaration\n\nclass Widget {  // class_specifier (C++ only)\npublic:\n    void update();  // method_declaration\n};\n\nint process_data(int* data) {  // function_definition\n    return data[0];\n}\n</code></pre>"},{"location":"user-guide/#working-with-chunks","title":"Working with Chunks","text":""},{"location":"user-guide/#understanding-codechunk","title":"Understanding CodeChunk","text":"<pre><code>from chunker.chunker import CodeChunk\n\n# Example chunk structure\nchunk = CodeChunk(\n    language=\"python\",\n    file_path=\"/path/to/file.py\",\n    node_type=\"function_definition\",\n    start_line=10,\n    end_line=15,\n    byte_start=234,\n    byte_end=456,\n    parent_context=\"class:Calculator\",\n    content=\"def add(self, a, b):\\n    return a + b\"\n)\n\n# Access properties\nprint(f\"Function spans {chunk.end_line - chunk.start_line + 1} lines\")\nprint(f\"Belongs to: {chunk.parent_context}\")\nprint(f\"Size: {chunk.byte_end - chunk.byte_start} bytes\")\n</code></pre>"},{"location":"user-guide/#filtering-and-grouping","title":"Filtering and Grouping","text":"<pre><code>from collections import defaultdict\n\nchunks = chunk_file(\"project.py\", \"python\")\n\n# Group by type\nby_type = defaultdict(list)\nfor chunk in chunks:\n    by_type[chunk.node_type].append(chunk)\n\nprint(f\"Functions: {len(by_type['function_definition'])}\")\nprint(f\"Classes: {len(by_type['class_definition'])}\")\nprint(f\"Methods: {len(by_type['method_definition'])}\")\n\n# Find large functions\nlarge_functions = [\n    c for c in chunks \n    if c.node_type == \"function_definition\" \n    and (c.end_line - c.start_line) &gt; 50\n]\n\n# Get methods of a specific class\nclass_methods = [\n    c for c in chunks \n    if c.parent_context == \"class:MyClass\"\n]\n</code></pre>"},{"location":"user-guide/#analyzing-code-structure","title":"Analyzing Code Structure","text":"<pre><code>def analyze_file_structure(file_path, language):\n    \"\"\"Analyze the structure of a code file.\"\"\"\n    chunks = chunk_file(file_path, language)\n\n    # Build hierarchy\n    top_level = [c for c in chunks if not c.parent_context]\n    nested = [c for c in chunks if c.parent_context]\n\n    print(f\"File: {file_path}\")\n    print(f\"Total chunks: {len(chunks)}\")\n    print(f\"Top-level: {len(top_level)}\")\n    print(f\"Nested: {len(nested)}\")\n\n    # Complexity metrics\n    sizes = [(c.end_line - c.start_line + 1) for c in chunks]\n    if sizes:\n        print(f\"Average size: {sum(sizes) / len(sizes):.1f} lines\")\n        print(f\"Largest: {max(sizes)} lines\")\n        print(f\"Smallest: {min(sizes)} lines\")\n\n    return chunks\n</code></pre>"},{"location":"user-guide/#advanced-features","title":"Advanced Features","text":""},{"location":"user-guide/#working-with-the-language-registry","title":"Working with the Language Registry","text":"<pre><code>from chunker.registry import LanguageRegistry\nfrom pathlib import Path\n\n# Access the registry directly\nregistry = LanguageRegistry(Path(\"build/my-languages.so\"))\n\n# Discover all languages\nlanguages = registry.discover_languages()\nfor name, metadata in languages.items():\n    print(f\"{name}:\")\n    print(f\"  Version: {metadata.version}\")\n    print(f\"  Node types: {metadata.node_types_count}\")\n    print(f\"  Has scanner: {metadata.has_scanner}\")\n\n# Check specific language\nif registry.has_language(\"python\"):\n    lang = registry.get_language(\"python\")\n    # Use with tree-sitter parser\n</code></pre>"},{"location":"user-guide/#custom-parser-configuration","title":"Custom Parser Configuration","text":"<pre><code>from chunker.parser import get_parser\nfrom chunker.factory import ParserConfig\n\n# Configure parser with timeout\nconfig = ParserConfig(\n    timeout_ms=5000,  # 5 second timeout\n    included_ranges=[(0, 1000), (2000, 3000)]  # Parse only specific byte ranges\n)\n\nparser = get_parser(\"python\", config)\n# Parser will only parse specified ranges and timeout after 5s\n</code></pre>"},{"location":"user-guide/#concurrent-processing","title":"Concurrent Processing","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\n\ndef process_directory(directory, language, max_workers=4):\n    \"\"\"Process all files in directory concurrently.\"\"\"\n    files = list(Path(directory).rglob(f\"*.{language[:2]}\"))\n\n    def process_file(file_path):\n        try:\n            return chunk_file(file_path, language)\n        except Exception as e:\n            print(f\"Error in {file_path}: {e}\")\n            return []\n\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all files\n        futures = {executor.submit(process_file, f): f for f in files}\n\n        # Collect results\n        all_chunks = []\n        for future in futures:\n            chunks = future.result()\n            all_chunks.extend(chunks)\n\n    return all_chunks\n\n# Process entire codebase\nchunks = process_directory(\"src\", \"python\", max_workers=8)\n</code></pre>"},{"location":"user-guide/#cache-management","title":"Cache Management","text":"<pre><code>from chunker.parser import clear_cache, _factory\n\n# Monitor cache performance\ndef process_with_stats(files, language):\n    # Clear cache for fresh start\n    clear_cache()\n\n    # Process files\n    for file in files:\n        chunk_file(file, language)\n\n    # Get statistics\n    stats = _factory.get_stats()\n    print(f\"Cache Performance:\")\n    print(f\"  Hits: {stats['cache_hits']}\")\n    print(f\"  Misses: {stats['cache_misses']}\")\n    print(f\"  Hit rate: {stats['hit_rate']:.2%}\")\n    print(f\"  Current cache size: {stats['cache_size']}\")\n\n    return stats\n</code></pre>"},{"location":"user-guide/#integration-patterns","title":"Integration Patterns","text":""},{"location":"user-guide/#integration-with-embedding-systems","title":"Integration with Embedding Systems","text":"<pre><code>from chunker.chunker import chunk_file\nimport numpy as np\n\ndef create_code_embeddings(file_path, language, embedding_model):\n    \"\"\"Generate embeddings for code chunks.\"\"\"\n    chunks = chunk_file(file_path, language)\n\n    embeddings = []\n    for chunk in chunks:\n        # Prepare text for embedding\n        context = f\"File: {chunk.file_path}\\n\"\n        context += f\"Type: {chunk.node_type}\\n\"\n        if chunk.parent_context:\n            context += f\"Parent: {chunk.parent_context}\\n\"\n        context += f\"Lines: {chunk.start_line}-{chunk.end_line}\\n\\n\"\n        context += chunk.content\n\n        # Generate embedding (example with sentence-transformers)\n        embedding = embedding_model.encode(context)\n\n        embeddings.append({\n            \"chunk_id\": f\"{chunk.file_path}:{chunk.start_line}\",\n            \"metadata\": {\n                \"type\": chunk.node_type,\n                \"parent\": chunk.parent_context,\n                \"lines\": (chunk.start_line, chunk.end_line)\n            },\n            \"embedding\": embedding,\n            \"content\": chunk.content\n        })\n\n    return embeddings\n\n# Search function\ndef semantic_search(query, embeddings, embedding_model, top_k=5):\n    \"\"\"Search for relevant code chunks.\"\"\"\n    query_embedding = embedding_model.encode(query)\n\n    # Calculate similarities\n    similarities = []\n    for item in embeddings:\n        similarity = np.dot(query_embedding, item[\"embedding\"])\n        similarities.append((similarity, item))\n\n    # Return top results\n    similarities.sort(key=lambda x: x[0], reverse=True)\n    return [item for _, item in similarities[:top_k]]\n</code></pre>"},{"location":"user-guide/#documentation-generation","title":"Documentation Generation","text":"<pre><code>import ast\nfrom chunker.chunker import chunk_file\n\ndef extract_docstrings(project_path, output_file):\n    \"\"\"Extract all docstrings from Python project.\"\"\"\n    from pathlib import Path\n\n    documentation = []\n\n    for py_file in Path(project_path).rglob(\"*.py\"):\n        chunks = chunk_file(py_file, \"python\")\n\n        for chunk in chunks:\n            if chunk.node_type in [\"function_definition\", \"class_definition\"]:\n                try:\n                    # Parse chunk to extract docstring\n                    tree = ast.parse(chunk.content)\n                    if tree.body:\n                        node = tree.body[0]\n                        docstring = ast.get_docstring(node)\n\n                        if docstring:\n                            documentation.append({\n                                \"type\": chunk.node_type,\n                                \"name\": node.name,\n                                \"file\": str(py_file),\n                                \"line\": chunk.start_line,\n                                \"docstring\": docstring,\n                                \"signature\": chunk.content.split('\\n')[0]\n                            })\n                except:\n                    continue\n\n    # Generate markdown documentation\n    with open(output_file, \"w\") as f:\n        f.write(\"# API Documentation\\n\\n\")\n\n        # Group by file\n        by_file = {}\n        for doc in documentation:\n            by_file.setdefault(doc[\"file\"], []).append(doc)\n\n        for file, docs in sorted(by_file.items()):\n            f.write(f\"## {file}\\n\\n\")\n            for doc in docs:\n                f.write(f\"### {doc['name']}\\n\")\n                f.write(f\"*{doc['type']} at line {doc['line']}*\\n\\n\")\n                f.write(f\"```python\\n{doc['signature']}\\n```\\n\\n\")\n                f.write(f\"{doc['docstring']}\\n\\n\")\n</code></pre>"},{"location":"user-guide/#code-quality-analysis","title":"Code Quality Analysis","text":"<pre><code>from chunker.chunker import chunk_file\nimport re\n\ndef analyze_code_quality(file_path, language):\n    \"\"\"Analyze code quality metrics.\"\"\"\n    chunks = chunk_file(file_path, language)\n\n    issues = []\n\n    for chunk in chunks:\n        # Check function length\n        lines = chunk.end_line - chunk.start_line + 1\n        if chunk.node_type == \"function_definition\" and lines &gt; 50:\n            issues.append({\n                \"type\": \"long_function\",\n                \"severity\": \"warning\" if lines &lt; 100 else \"error\",\n                \"location\": f\"{chunk.file_path}:{chunk.start_line}\",\n                \"message\": f\"Function is {lines} lines long (recommended: &lt;50)\",\n                \"chunk\": chunk\n            })\n\n        # Check complexity (simple heuristic based on nesting)\n        max_indent = 0\n        for line in chunk.content.split('\\n'):\n            if line.strip():\n                indent = len(line) - len(line.lstrip())\n                max_indent = max(max_indent, indent)\n\n        if max_indent &gt; 20:  # 5 levels of nesting (4 spaces each)\n            issues.append({\n                \"type\": \"high_complexity\",\n                \"severity\": \"warning\",\n                \"location\": f\"{chunk.file_path}:{chunk.start_line}\",\n                \"message\": f\"High nesting level detected\",\n                \"chunk\": chunk\n            })\n\n        # Check naming conventions (Python example)\n        if language == \"python\" and chunk.node_type == \"function_definition\":\n            # Extract function name\n            match = re.match(r'def\\s+(\\w+)', chunk.content)\n            if match:\n                func_name = match.group(1)\n                if not re.match(r'^[a-z_][a-z0-9_]*$', func_name):\n                    issues.append({\n                        \"type\": \"naming_convention\",\n                        \"severity\": \"info\",\n                        \"location\": f\"{chunk.file_path}:{chunk.start_line}\",\n                        \"message\": f\"Function '{func_name}' doesn't follow snake_case\",\n                        \"chunk\": chunk\n                    })\n\n    return issues\n\n# Generate report\ndef generate_quality_report(directory, language):\n    \"\"\"Generate code quality report for directory.\"\"\"\n    from pathlib import Path\n\n    all_issues = []\n    for file in Path(directory).rglob(f\"*.{language[:2]}\"):\n        issues = analyze_code_quality(file, language)\n        all_issues.extend(issues)\n\n    # Summary\n    by_type = {}\n    by_severity = {}\n    for issue in all_issues:\n        by_type[issue[\"type\"]] = by_type.get(issue[\"type\"], 0) + 1\n        by_severity[issue[\"severity\"]] = by_severity.get(issue[\"severity\"], 0) + 1\n\n    print(\"Code Quality Report\")\n    print(\"=\" * 50)\n    print(f\"Total issues: {len(all_issues)}\")\n    print(\"\\nBy type:\")\n    for type, count in by_type.items():\n        print(f\"  {type}: {count}\")\n    print(\"\\nBy severity:\")\n    for severity, count in by_severity.items():\n        print(f\"  {severity}: {count}\")\n\n    return all_issues\n</code></pre>"},{"location":"user-guide/#plugin-system","title":"Plugin System","text":""},{"location":"user-guide/#using-built-in-plugins","title":"Using Built-in Plugins","text":"<p>Tree-sitter Chunker comes with built-in plugins for Python, JavaScript, Rust, C, and C++:</p> <pre><code>from chunker.plugin_manager import get_plugin_manager\n\n# Load built-in plugins\nmanager = get_plugin_manager()\nmanager.load_built_in_plugins()\n\n# List available plugins\nprint(manager.list_plugins())\n# Output: ['python', 'javascript', 'rust', 'c', 'cpp']\n\n# Chunk files using plugins\nchunks = chunk_file(\"example.py\", \"python\")\n</code></pre>"},{"location":"user-guide/#plugin-configuration","title":"Plugin Configuration","text":"<p>Configure plugins through configuration files or programmatically:</p> <pre><code>from chunker.chunker_config import ChunkerConfig\nfrom chunker.plugin_manager import PluginConfig\n\n# Load configuration from file\nconfig = ChunkerConfig(\"chunker.config.toml\")\n\n# Or configure programmatically\nconfig = ChunkerConfig()\nconfig.set_plugin_config(\"python\", PluginConfig(\n    enabled=True,\n    chunk_types={\"function_definition\", \"class_definition\"},\n    min_chunk_size=5,\n    max_chunk_size=300,\n    custom_options={\n        \"include_docstrings\": True,\n        \"skip_private\": False\n    }\n))\n</code></pre>"},{"location":"user-guide/#loading-custom-plugins","title":"Loading Custom Plugins","text":"<pre><code>from pathlib import Path\nfrom chunker.plugin_manager import get_plugin_manager\n\n# Load plugins from a directory\nmanager = get_plugin_manager()\nmanager.load_plugin_directory(Path(\"~/.chunker/plugins\"))\n\n# Or register a plugin class directly\nfrom my_plugin import SwiftPlugin\nmanager.register_plugin(SwiftPlugin)\n</code></pre>"},{"location":"user-guide/#performance-features","title":"Performance Features","text":""},{"location":"user-guide/#ast-caching","title":"AST Caching","text":"<p>The AST cache provides up to 11.9x speedup for repeated file processing:</p> <pre><code>from chunker.core import chunk_file\nfrom chunker.cache import ASTCache\n\n# Caching is enabled by default\nchunks1 = chunk_file(\"large_file.py\", \"python\")  # First run: parses\nchunks2 = chunk_file(\"large_file.py\", \"python\")  # Second run: uses cache (11.9x faster)\n\n# Monitor cache performance\ncache = ASTCache(max_size=200)\nstats = cache.get_stats()\nprint(f\"Cache hit rate: {stats['hit_rate']:.2%}\")\nprint(f\"Cache size: {stats['size']}/{stats['max_size']}\")\n</code></pre>"},{"location":"user-guide/#parallel-file-processing","title":"Parallel File Processing","text":"<p>Process multiple files concurrently for maximum performance:</p> <pre><code>from chunker.parallel import chunk_files_parallel, chunk_directory_parallel\n\n# Process specific files\nfiles = [\"src/main.py\", \"src/utils.py\", \"src/models.py\"]\nresults = chunk_files_parallel(\n    files, \n    \"python\", \n    max_workers=8,\n    show_progress=True\n)\n\n# Process entire directory\nresults = chunk_directory_parallel(\n    \"src/\",\n    \"python\",\n    pattern=\"**/*.py\",\n    max_workers=8\n)\n\nfor file_path, chunks in results.items():\n    print(f\"{file_path}: {len(chunks)} chunks\")\n</code></pre>"},{"location":"user-guide/#streaming-large-files","title":"Streaming Large Files","text":"<p>For very large files, use streaming to avoid loading everything into memory:</p> <pre><code>from chunker.streaming import chunk_file_streaming\n\n# Process a huge file incrementally\nfor chunk in chunk_file_streaming(\"massive_codebase.py\", \"python\"):\n    # Process each chunk as it's found\n    print(f\"Found {chunk.node_type} at lines {chunk.start_line}-{chunk.end_line}\")\n    # Save to database, send to API, etc.\n    process_chunk(chunk)\n</code></pre>"},{"location":"user-guide/#performance-tips","title":"Performance Tips","text":"<ol> <li>Enable Caching: Always use caching for files that are processed multiple times</li> <li>Use Parallel Processing: Take advantage of multiple CPU cores</li> <li>Stream Large Files: Use streaming for files over 10MB</li> <li>Configure Cache Size: Adjust based on available memory</li> <li>Batch Operations: Process files in batches rather than one at a time</li> </ol>"},{"location":"user-guide/#export-formats","title":"Export Formats","text":""},{"location":"user-guide/#json-export","title":"JSON Export","text":"<p>Export chunks to JSON with different schema types:</p> <pre><code>from chunker.core import chunk_file\nfrom chunker.export.json_export import JSONExporter\nfrom chunker.export.formatters import SchemaType\n\n# Get chunks\nchunks = chunk_file(\"example.py\", \"python\")\n\n# Export with flat schema (default)\nexporter = JSONExporter(schema_type=SchemaType.FLAT)\nexporter.export(chunks, \"output.json\", indent=2)\n\n# Export with nested schema (preserves hierarchy)\nexporter = JSONExporter(schema_type=SchemaType.NESTED)\nexporter.export(chunks, \"output_nested.json\")\n\n# Export with compression\nexporter.export(chunks, \"output.json.gz\", compress=True)\n</code></pre>"},{"location":"user-guide/#jsonl-export","title":"JSONL Export","text":"<p>JSON Lines format is ideal for streaming and large datasets:</p> <pre><code>from chunker.export.json_export import JSONLExporter\n\n# Export to JSONL\nexporter = JSONLExporter()\nexporter.export(chunks, \"output.jsonl\")\n\n# Stream export for large datasets\nfrom chunker.streaming import chunk_file_streaming\n\ndef chunk_generator():\n    for chunk in chunk_file_streaming(\"huge_file.py\", \"python\"):\n        yield chunk\n\nexporter.export_streaming(chunk_generator(), \"large_output.jsonl\")\n</code></pre>"},{"location":"user-guide/#parquet-export","title":"Parquet Export","text":"<p>Parquet format is excellent for analytics and data science workflows:</p> <pre><code>from chunker.exporters import ParquetExporter\n\n# Basic export\nexporter = ParquetExporter()\nexporter.export(chunks, \"output.parquet\")\n\n# Export with custom columns and compression\nexporter = ParquetExporter(\n    columns=[\"language\", \"file_path\", \"node_type\", \"content\", \"start_line\", \"end_line\"],\n    compression=\"snappy\"  # Options: snappy, gzip, brotli, lz4, zstd\n)\nexporter.export(chunks, \"output.parquet\")\n\n# Export with partitioning for large datasets\nexporter.export_partitioned(\n    chunks,\n    \"output_dir/\",\n    partition_cols=[\"language\", \"node_type\"]\n)\n\n# Stream export for memory efficiency\nfrom chunker.parallel import chunk_directory_parallel\n\ndef process_directory(directory):\n    results = chunk_directory_parallel(directory, \"python\")\n    for file_path, file_chunks in results.items():\n        for chunk in file_chunks:\n            yield chunk\n\nexporter.export_streaming(\n    process_directory(\"large_codebase/\"),\n    \"streaming_output.parquet\",\n    batch_size=1000\n)\n</code></pre>"},{"location":"user-guide/#export-format-comparison","title":"Export Format Comparison","text":"Format Best For Compression Streaming Schema Support JSON Human-readable, small datasets Optional No Flexible JSONL Streaming, logs, APIs Optional Yes Per-line Parquet Analytics, big data Built-in Yes Typed"},{"location":"user-guide/#custom-export-example","title":"Custom Export Example","text":"<pre><code># Export chunks with filtering and transformation\nfrom chunker.parallel import chunk_directory_parallel\nfrom chunker.exporters import ParquetExporter\n\n# Process a project\nresults = chunk_directory_parallel(\"myproject/\", \"python\")\n\n# Filter and transform chunks\nprocessed_chunks = []\nfor file_path, chunks in results.items():\n    for chunk in chunks:\n        # Only export functions and classes\n        if chunk.node_type in [\"function_definition\", \"class_definition\"]:\n            # Add custom metadata\n            chunk.metadata = {\n                \"project\": \"myproject\",\n                \"version\": \"1.0.0\",\n                \"extracted_at\": datetime.now().isoformat()\n            }\n            processed_chunks.append(chunk)\n\n# Export to Parquet with partitioning\nexporter = ParquetExporter(compression=\"zstd\")\nexporter.export_partitioned(\n    processed_chunks,\n    \"exports/myproject/\",\n    partition_cols=[\"node_type\"]\n)\n</code></pre>"},{"location":"user-guide/#performance-best-practices","title":"Performance Best Practices","text":""},{"location":"user-guide/#1-reuse-parsers","title":"1. Reuse Parsers","text":"<pre><code># Good - parser reused automatically\nfor file in files:\n    chunks = chunk_file(file, \"python\")\n\n# Also good - manual control\nparser = get_parser(\"python\")\ntry:\n    for file in files:\n        # Use same parser for multiple files\n        with open(file, 'rb') as f:\n            tree = parser.parse(f.read())\nfinally:\n    return_parser(\"python\", parser)\n</code></pre>"},{"location":"user-guide/#2-process-in-parallel","title":"2. Process in Parallel","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\n\n# Process multiple files concurrently\nwith ThreadPoolExecutor(max_workers=8) as executor:\n    futures = [executor.submit(chunk_file, f, \"python\") for f in files]\n    results = [f.result() for f in futures]\n</code></pre>"},{"location":"user-guide/#3-configure-cache-size","title":"3. Configure Cache Size","text":"<pre><code>import os\n\n# Set via environment variables\nos.environ['CHUNKER_CACHE_SIZE'] = '20'\nos.environ['CHUNKER_POOL_SIZE'] = '10'\n\n# Or configure factory directly\nfrom chunker.factory import ParserFactory\nfrom chunker.registry import LanguageRegistry\n\nregistry = LanguageRegistry(Path(\"build/my-languages.so\"))\nfactory = ParserFactory(registry, cache_size=20, pool_size=10)\n</code></pre>"},{"location":"user-guide/#4-handle-large-files","title":"4. Handle Large Files","text":"<pre><code>def chunk_large_file(file_path, language, max_size_mb=10):\n    \"\"\"Handle large files efficiently.\"\"\"\n    from pathlib import Path\n\n    file_size = Path(file_path).stat().st_size / (1024 * 1024)\n\n    if file_size &gt; max_size_mb:\n        # Process in chunks using included_ranges\n        from chunker.factory import ParserConfig\n\n        chunk_size = int(max_size_mb * 1024 * 1024)\n        ranges = []\n\n        with open(file_path, 'rb') as f:\n            data = f.read()\n            for i in range(0, len(data), chunk_size):\n                ranges.append((i, min(i + chunk_size, len(data))))\n\n        all_chunks = []\n        for start, end in ranges:\n            config = ParserConfig(included_ranges=[(start, end)])\n            parser = get_parser(language, config)\n            # Process range...\n\n        return all_chunks\n    else:\n        return chunk_file(file_path, language)\n</code></pre>"},{"location":"user-guide/#configuration","title":"Configuration","text":""},{"location":"user-guide/#environment-variables","title":"Environment Variables","text":"<pre><code># Set logging level\nexport CHUNKER_LOG_LEVEL=DEBUG\n\n# Configure cache sizes\nexport CHUNKER_CACHE_SIZE=20\nexport CHUNKER_POOL_SIZE=10\n\n# Run with custom configuration\npython cli/main.py chunk file.py -l python\n</code></pre>"},{"location":"user-guide/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>import logging\nfrom chunker.factory import ParserConfig\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Configure parser\nconfig = ParserConfig(\n    timeout_ms=10000,  # 10 second timeout\n    logger=logging.getLogger(\"parser\")\n)\n</code></pre>"},{"location":"user-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"user-guide/#language-not-found","title":"Language Not Found","text":"<pre><code>from chunker.parser import list_languages\nfrom chunker.exceptions import LanguageNotFoundError\n\ntry:\n    chunks = chunk_file(\"file.xyz\", \"xyz\")\nexcept LanguageNotFoundError as e:\n    print(f\"Error: {e}\")\n    available = list_languages()\n    print(f\"Available languages: {', '.join(available)}\")\n    # Suggestion: Check if language is compiled into .so file\n</code></pre>"},{"location":"user-guide/#library-not-found","title":"Library Not Found","text":"<pre><code># Error: LibraryNotFoundError: Shared library not found: build/my-languages.so\n\n# Solution: Build the library\npython scripts/fetch_grammars.py\npython scripts/build_lib.py\n</code></pre>"},{"location":"user-guide/#parser-version-mismatch","title":"Parser Version Mismatch","text":"<pre><code># Error: Language 'python' ABI version 14 doesn't match parser version 13\n\n# Solution: Update py-tree-sitter\nuv pip install git+https://github.com/tree-sitter/py-tree-sitter.git\n</code></pre>"},{"location":"user-guide/#empty-results","title":"Empty Results","text":"<pre><code># Debug why no chunks are returned\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\nchunks = chunk_file(\"file.py\", \"python\")\nif not chunks:\n    # Check if file has supported node types\n    parser = get_parser(\"python\")\n    with open(\"file.py\", \"rb\") as f:\n        tree = parser.parse(f.read())\n\n    # Inspect AST\n    def print_tree(node, indent=0):\n        print(\"  \" * indent + node.type)\n        for child in node.children:\n            print_tree(child, indent + 1)\n\n    print_tree(tree.root_node)\n</code></pre>"},{"location":"user-guide/#debug-information","title":"Debug Information","text":"<pre><code>from chunker.parser import _factory, _registry\n\n# Check loaded languages\nprint(\"Loaded languages:\", _registry.list_languages())\n\n# Check cache statistics\nstats = _factory.get_stats()\nprint(\"Cache stats:\", stats)\n\n# Enable detailed logging\nimport logging\nlogging.getLogger(\"chunker\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"user-guide/#getting-help","title":"Getting Help","text":"<p>When reporting issues:</p> <ol> <li> <p>Version Information:    <code>python    import sys    import tree_sitter    print(f\"Python: {sys.version}\")    print(f\"Tree-sitter: {tree_sitter.__version__}\")</code></p> </li> <li> <p>Minimal Example:    <code>python    from chunker.chunker import chunk_file    chunks = chunk_file(\"problem_file.py\", \"python\")</code></p> </li> <li> <p>Error Traceback: Include the full error message</p> </li> <li> <p>Environment: OS, Python version, installation method</p> </li> </ol>"},{"location":"user-guide/#see-also","title":"See Also","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Plugin Development - Creating custom language plugins</li> <li>Configuration - Configuration file reference</li> <li>Performance Guide - Optimization strategies</li> <li>Export Formats - Detailed export documentation</li> <li>Getting Started - Quick start tutorial  </li> <li>Architecture - System design details</li> <li>Cookbook - Practical recipes and examples</li> </ul>"},{"location":"zero_config_api/","title":"Zero-Configuration API","text":"<p>The Zero-Config API provides the simplest way to use treesitter-chunker with automatic language detection, grammar management, and intelligent fallbacks.</p>"},{"location":"zero_config_api/#overview","title":"Overview","text":"<p>The <code>ZeroConfigAPI</code> class provides a high-level interface that: - Automatically detects programming languages from file extensions and content - Downloads and sets up tree-sitter grammars as needed - Falls back to intelligent text chunking when tree-sitter is unavailable - Supports batch operations and preloading for offline use</p>"},{"location":"zero_config_api/#basic-usage","title":"Basic Usage","text":"<pre><code>from chunker import ZeroConfigAPI\nfrom chunker.contracts.registry_stub import UniversalRegistryStub\n\n# Create API instance with a registry\nregistry = UniversalRegistryStub()  # Or use real UniversalLanguageRegistry\napi = ZeroConfigAPI(registry)\n\n# Chunk a file - language is auto-detected\nresult = api.auto_chunk_file(\"example.py\")\n\n# Access chunks\nfor chunk in result.chunks:\n    print(f\"Type: {chunk['type']}, Lines: {chunk['start_line']}-{chunk['end_line']}\")\n    print(chunk['content'])\n</code></pre>"},{"location":"zero_config_api/#key-features","title":"Key Features","text":""},{"location":"zero_config_api/#1-automatic-language-detection","title":"1. Automatic Language Detection","text":"<pre><code># Detects Python from .py extension\nlanguage = api.detect_language(\"script.py\")  # Returns \"python\"\n\n# Detects from shebang\nlanguage = api.detect_language(\"script\")  # Checks #!/usr/bin/env python\n\n# Special file names\nlanguage = api.detect_language(\"Makefile\")  # Returns \"makefile\"\nlanguage = api.detect_language(\"Dockerfile\")  # Returns \"dockerfile\"\n</code></pre>"},{"location":"zero_config_api/#2-zero-configuration-file-chunking","title":"2. Zero-Configuration File Chunking","text":"<pre><code># Automatically detects language and chunks appropriately\nresult = api.auto_chunk_file(\"main.go\")\n\n# Check if grammar was downloaded\nif result.grammar_downloaded:\n    print(\"Grammar was automatically downloaded\")\n\n# Check if fallback was used\nif result.fallback_used:\n    print(\"Used text-based chunking (tree-sitter unavailable)\")\n</code></pre>"},{"location":"zero_config_api/#3-direct-text-chunking","title":"3. Direct Text Chunking","text":"<pre><code># Chunk text content directly\ncode = \"\"\"\ndef hello(name):\n    return f\"Hello, {name}!\"\n\"\"\"\n\nresult = api.chunk_text(code, \"python\")\n</code></pre>"},{"location":"zero_config_api/#4-token-limited-chunking","title":"4. Token-Limited Chunking","text":"<pre><code># Limit chunks to specific token count\nresult = api.auto_chunk_file(\"large_file.py\", token_limit=1000)\n</code></pre>"},{"location":"zero_config_api/#5-language-preloading","title":"5. Language Preloading","text":"<pre><code># Preload multiple languages for offline use\nlanguages = [\"python\", \"javascript\", \"go\", \"rust\"]\nresults = api.preload_languages(languages)\n\nfor lang, success in results.items():\n    print(f\"{lang}: {'\u2713' if success else '\u2717'}\")\n</code></pre>"},{"location":"zero_config_api/#6-ensure-language-availability","title":"6. Ensure Language Availability","text":"<pre><code># Ensure a specific language is ready\nif api.ensure_language(\"java\"):\n    print(\"Java is ready to use\")\n\n# Ensure specific version\nif api.ensure_language(\"python\", \"0.20.0\"):\n    print(\"Python 0.20.0 is ready\")\n</code></pre>"},{"location":"zero_config_api/#7-get-language-specific-chunker","title":"7. Get Language-Specific Chunker","text":"<pre><code># Get a configured chunker for a specific language\nchunker = api.get_chunker_for_language(\"rust\")\n\n# Use it directly\nchunks = chunker.chunk_file(\"main.rs\")\n</code></pre>"},{"location":"zero_config_api/#8-list-supported-extensions","title":"8. List Supported Extensions","text":"<pre><code># Get all supported file extensions\nextensions = api.list_supported_extensions()\n\n# Example output:\n# {\n#     \"python\": [\".py\"],\n#     \"javascript\": [\".js\", \".jsx\"],\n#     \"typescript\": [\".ts\", \".tsx\"],\n#     ...\n# }\n</code></pre>"},{"location":"zero_config_api/#result-structure","title":"Result Structure","text":"<p>The <code>AutoChunkResult</code> object contains:</p> <pre><code>@dataclass\nclass AutoChunkResult:\n    chunks: list[dict[str, Any]]  # List of chunk dictionaries\n    language: str                  # Detected or specified language\n    grammar_downloaded: bool       # Whether grammar was downloaded\n    fallback_used: bool           # Whether fallback chunking was used\n    metadata: dict[str, Any]      # Additional metadata\n</code></pre> <p>Each chunk dictionary contains: - <code>content</code>: The actual code/text content - <code>type</code>: Node type (e.g., \"function_definition\", \"class\", \"text\") - <code>start_line</code>: Starting line number - <code>end_line</code>: Ending line number - <code>metadata</code>: Optional additional metadata</p>"},{"location":"zero_config_api/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = api.auto_chunk_file(\"nonexistent.py\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")  # File not found\n\n# Invalid language handling\nsuccess = api.ensure_language(\"not-a-real-language\")\nassert success is False\n</code></pre>"},{"location":"zero_config_api/#integration-with-registry","title":"Integration with Registry","text":"<p>The Zero-Config API works with any implementation of <code>UniversalRegistryContract</code>:</p> <pre><code>from chunker.contracts.registry_contract import UniversalRegistryContract\n\nclass MyCustomRegistry(UniversalRegistryContract):\n    # Implement required methods\n    pass\n\napi = ZeroConfigAPI(MyCustomRegistry())\n</code></pre>"},{"location":"zero_config_api/#advanced-usage","title":"Advanced Usage","text":""},{"location":"zero_config_api/#custom-language-override","title":"Custom Language Override","text":"<pre><code># Force specific language detection\nresult = api.auto_chunk_file(\"script.txt\", language=\"python\")\n</code></pre>"},{"location":"zero_config_api/#fallback-behavior","title":"Fallback Behavior","text":"<p>When tree-sitter is unavailable, the API automatically falls back to intelligent text chunking:</p> <pre><code># If grammar download fails or language unsupported\nresult = api.auto_chunk_file(\"data.csv\")\nassert result.fallback_used is True\nassert result.language == \"unknown\"  # Or detected type\n</code></pre>"},{"location":"zero_config_api/#batch-processing","title":"Batch Processing","text":"<pre><code>import os\n\n# Process all Python files in a directory\nfor root, dirs, files in os.walk(\"src\"):\n    for file in files:\n        if file.endswith(\".py\"):\n            path = os.path.join(root, file)\n            result = api.auto_chunk_file(path)\n            print(f\"Processed {path}: {len(result.chunks)} chunks\")\n</code></pre>"},{"location":"zero_config_api/#best-practices","title":"Best Practices","text":"<ol> <li>Preload for Production: Use <code>preload_languages()</code> to ensure grammars are available before processing</li> <li>Check Results: Always check <code>fallback_used</code> to know if tree-sitter chunking was successful</li> <li>Handle Unknown Languages: Implement fallback logic for files with unknown languages</li> <li>Cache Registry: Use a persistent registry implementation to avoid re-downloading grammars</li> </ol>"},{"location":"zero_config_api/#example-complete-workflow","title":"Example: Complete Workflow","text":"<pre><code>from chunker import ZeroConfigAPI\nfrom chunker.contracts.registry_stub import UniversalRegistryStub\n\n# Initialize\nregistry = UniversalRegistryStub()\napi = ZeroConfigAPI(registry)\n\n# Preload common languages\nlanguages = [\"python\", \"javascript\", \"typescript\", \"go\"]\npreload_results = api.preload_languages(languages)\nprint(f\"Preloaded: {sum(preload_results.values())} of {len(languages)} languages\")\n\n# Process a mixed codebase\ndef process_file(file_path):\n    try:\n        result = api.auto_chunk_file(file_path)\n\n        print(f\"\\nFile: {file_path}\")\n        print(f\"Language: {result.language}\")\n        print(f\"Chunks: {len(result.chunks)}\")\n        print(f\"Grammar downloaded: {result.grammar_downloaded}\")\n        print(f\"Fallback used: {result.fallback_used}\")\n\n        # Process chunks\n        for i, chunk in enumerate(result.chunks):\n            print(f\"  Chunk {i+1}: {chunk['type']} (lines {chunk['start_line']}-{chunk['end_line']})\")\n\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n\n# Process various file types\nprocess_file(\"main.py\")\nprocess_file(\"app.js\")\nprocess_file(\"server.go\")\nprocess_file(\"README.md\")\nprocess_file(\"Makefile\")\n</code></pre>"},{"location":"zero_config_api/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Grammar Download: First use of a language may be slower due to grammar download</li> <li>Caching: The registry should cache parsers for better performance</li> <li>Parallel Processing: Consider using parallel processing for large codebases</li> <li>Memory Usage: Large files may consume significant memory during parsing</li> </ol>"},{"location":"zero_config_api/#troubleshooting","title":"Troubleshooting","text":""},{"location":"zero_config_api/#grammar-download-failures","title":"Grammar Download Failures","text":"<ul> <li>Check internet connection</li> <li>Verify language name is correct</li> <li>Check if grammar is available in tree-sitter ecosystem</li> </ul>"},{"location":"zero_config_api/#fallback-warnings","title":"Fallback Warnings","text":"<ul> <li>Normal for non-code files (markdown, config files)</li> <li>Check file extension mapping if unexpected</li> <li>Verify tree-sitter installation</li> </ul>"},{"location":"zero_config_api/#performance-issues","title":"Performance Issues","text":"<ul> <li>Preload languages before batch processing</li> <li>Use token limits for very large files</li> <li>Consider chunking strategy configuration</li> </ul>"}]}